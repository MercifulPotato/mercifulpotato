Item(by='1vuio0pswjnm7', descendants=None, kids=[25748502], score=None, time=1610324115, title=None, item_type='comment', url=None, parent=25716952, text='Why not use both?<p>Putting aside what happens if one allows Javascript and uses &quot;modern&quot; browsers, Startpage generally does not seem to require any more data from users than DDG.  A small shell script can be used to search Startpage or DDG (or almost any other search engine) from the commandline without sending any unecessary data, like unecessary headers, cookies or hidden form variables.  The best part is by not using the &quot;modern&quot; browser to send the search, one can easily automate editing the results page before viewing it in a browser, discarding all the cruft.  I like to just return the URLs. (I notice that Startpage also (a) supports HTTP&#x2F;1.1 pipelining, e.g., multiple page requests over a single TCP connection; Google does not and (b) allows bans to be overcome by solving an easily read captcha and this seems to prevent further bans; Google imposes automatic temporary bans that cannot be overcome by solving a captcha.)<p>The biggest problem I see with the major search engines and these minor search engines that repackage results from the major ones is that they are too often limiting the number of results returned.  For example, Google limits to something like 200-300.  In the early days of the web, search engines used to brag about how many pages were searched, and they proved their claim by how many results they returned.  Today search engines want to localise and limit the results.  Not to mention promoting their own websites.  I also notice repeated searches where one is collecting the total results not simply the first page yield different results.<p>Not every query is a question and not every user is interested in an instantaneous &quot;answer&quot; or the most popular website.  That type of quick searching certainly has its place but it is not &quot;research&quot; and will not lead users to learn much about what actually exists on the web, or how to think critically about the web&#x27;s content.  Some users may want to search for pages and then evaluate the pages themselves.  Exploration and discovery.  Those users are treated as &quot;bots&quot; in order to justify what can only be anti-competitive practices.  The sad consequence of this &quot;limiting&quot; behaviour is to keep curious users from ever learning what actually exists on the web (versus what a &quot;search engine&quot; decides to promote, or demote).<p>I have been playing around with Common Crawl data and it seems woefully circumspect in its scope.  A web index should be public information but these search engines sure as heck do not treat it as such.')