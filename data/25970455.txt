Item(by='jhokanson', descendants=None, kids=None, score=None, time=1612024626, title=None, item_type='comment', url=None, parent=25970084, text='Wow, those are big performance differences (660 MB&#x2F;s for fast-double vs 1042 MB&#x2F;s for the &#x27;newer&#x27; fast-float), although most of the numbers (for the different libraries being tested) are all over the place, and even &#x27;strtod&#x27; more than doubled in speed between the two tests (70 MB&#x2F;s fast-double vs 190 fast-float MB&#x2F;s). It wouldn&#x27;t surprise me if those two code bases are essentially the same.<p>That highlights the complexity of benchmarking in general and the importance of comparing within the same benchmark. I haven&#x27;t looked at this in a while but I thought some of the newer JSON parsers were standards compliant (maybe not?).<p>Anyway, that other blog post answers my question as it looks like the big insight is that you use the fast approach (that everyone uses) when you can, and fall back to slow if you really have to. From that blog link:<p>&quot;The full idea requires a whole blog post to explain, but the gist of it is that we can attempt to compute the answer, optimistically using a fast algorithm, and fall back on something else (like the standard library) as needed. It turns out that for the kind of numbers we find in JSON documents, we can parse 99% of them using a simple approach. All we have to do is correctly detect the error cases and bail out.&quot;<p>Again, I swear I&#x27;ve seen this in one of the other JSON parsers but maybe I&#x27;m misremembering. And again, good for them for breaking it out into a header library for others to use.')