Item(by='magusdei', descendants=None, kids=[25440783], score=None, time=1608110058, title=None, item_type='comment', url=None, parent=25440179, text='Wouldn&#x27;t the empirical success of GPT-3 in simple programming tasks itself be evidence against this interpretation?<p>Furthermore, GPT-3 is only a language model because it is trained on textual data. Transformer architectures simply map sequences to other sequences. It doesn&#x27;t particularly matter what those sequences represent. GPT-2 has been used to complete images, for example: <a href="https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;image-gpt&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;image-gpt&#x2F;</a>')