Item(by='jpau', descendants=None, kids=[25996766], score=None, time=1612216218, title=None, item_type='comment', url=None, parent=25994411, text='They use DeltaLake + Spark 3.0, and are mostly careful to partition well.<p>Their datasets are small. Most tables are ~50GB, the odd table up to ~2TB. The clusters typically are nothing shabby for this size, defaults to ~[4-12]x32GB.<p>The queries that I have seen are typically not written well. Think view-on-view-on-view (there&#x27;s a BigCo policy against them materialising data..), and where the filter is applied in the last step. The stuff of horrors, but something I&#x27;ve seen in more-than-one-BigCo.<p>But we have compared some of those same queries on BigQuery vs. Databricks, and, I don&#x27;t know if BigQuery&#x27;s execution optimiser is better? Or if the BigQuery storage is better organising the data? Or if BigQuery is simply throwing more resource their way?')