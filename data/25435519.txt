Item(by='yongjik', descendants=None, kids=[25435632], score=None, time=1608067969, title=None, item_type='comment', url=None, parent=25435028, text='&gt; with dynamically generated graphs, the computational graph is never actually defined anywhere: the computation is traced out on the fly and behind the scene. You can no longer do anything interesting with the computational graph: for example, if the computation is slow, you canâ€™t reason about what parts of the graph are slow.<p>Hmm, my experience is the opposite.  When I used Tensorflow, there was no way I could figure out why something is slow, or require huge memory.  All I have is a gigantic black box.<p>Meanwhile, in PyTorch, all I have to do is run it with CUDA_LAUNCH_BLOCKING=1, and it will give me an accurate picture of exactly how much milliseconds each line is taking!  (Just print the current time before&#x2F;after the line.)  With nvprof it will even tell you which CUDA kernels are executing.<p>* Disclaimer: Haven&#x27;t dabbled in ML for ~a year, so my view might be outdated now.')