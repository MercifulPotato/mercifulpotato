Item(by='dragontamer', descendants=None, kids=None, score=None, time=1611270019, title=None, item_type='comment', url=None, parent=25865381, text='Okay, lets say page-faults are off the table for some reason. Lets think about what can happen even if everything is in RAM still.<p>* MMAP is still on the table: different processes can share RAM at different addresses. (Process#1 thinks the data is at memory location 0x90000000, Process#2 thinks the data is at 0x70000000, but in both cases, the data is at physical location 0x42).<p>* Physical location 0x42 is a far-read on a far-away NUMA node. Which means the CPU#0 now needs to send a message to a CPU#1 very far away to get a copy of that RAM. This message traverses Intel Ultrapath Interconnect or AMD Infinity fabric (proprietary details), but its a remote message that happens nonetheless.<p>* Turns out CPU#1 has modified location 0x42. Now CPU#1 must push the most recent copy out of L1 cache, into L2 cache... then into L3 cache, and then send it back to CPU#0. CPU#0 has to wait until this process is done. If CPU#1 wants to modify the data again (or even read it), it may require messages from CPU#0 (who is now the owner of the data, according to simple MESI models).<p>Modern computers work very hard to hold the illusion of a singular memory space. Eventually, these details are turned into a consistent memory model and become well-ordered sequential operations. The CPU is a good place for that.<p>---------------<p>That&#x27;s how stuff works _today_. If you wanted to make a new programming model that&#x27;s incompatible, that&#x27;s fine. (CUDA does it: GPUs don&#x27;t have as many virtual-memory features as a CPU. And __shared__ memory has a different model than L1 cache.)<p>But if you invent a new memory model that does things differently, it means that it won&#x27;t work for the vast majority of code. Which means you need to bootstrap a new programming environment (much like how CUDA bootstrapped a new community from scratch).')