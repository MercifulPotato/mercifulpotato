Item(by='jandrewrogers', descendants=None, kids=[25916423, 25916930, 25916538, 25916428, 25916820, 25919066, 25916508, 25916344, 25916376, 25920646, 25916352, 25916312], score=None, time=1611672115, title=None, item_type='comment', url=None, parent=25915023, text='The assertion that PostgreSQL can handle dozens of TB of data needs to be qualified, as this is definitely not the case in some surprising and unexpected cases that are rarely talked about.<p>PostgreSQL&#x27;s statistics collection, which is used by the query planner, <i>doesn&#x27;t scale with storage size</i>. For some ordinary data distributions at scale, the statistical model won&#x27;t reflect any kind of reality and therefore can produce pathological query plans. It is quite difficult to get around this scaling behavior. Consequently, I&#x27;ve moved away from using PostgreSQL for data models with distributions such that these query planner limitations will occur. These pathologies occur well before the &quot;dozens of TB&quot; range.<p>The statistics collector is not a scalable design generally, but that is another matter. In its current state it does not degrade gracefully with scale.')