Item(by='adrian_b', descendants=None, kids=[25845591], score=None, time=1611144534, title=None, item_type='comment', url=None, parent=25844849, text='&quot;1st&quot; is not a &quot;natural&quot; choice for the starting point of ordinal numbers.<p>If anything, it is an artificial choice, because in programming it is derived from a property of most, probably of all, human languages that have ordinal numerals.<p>In most, probably in all, human languages the ordinal numerals are derived from the cardinal numerals through the intermediate of the counting sequence 1, 2, 3, ...<p>When cardinal numbers appeared, their initial use was only to communicate how many elements are in a set, which was established by counting 1, 2, 3, ...<p>Later people realized that they can refer to an element of a sequence by using the number reached at that element when counting the elements of the sequence, so the ordinal numerals appeared, being derived from the cardinals by applying some modifier.<p>So any discussion about whether 1 is more natural than 0 as the starting index goes back to whether 1 is more natural as the starting point of the counting sequence.<p>All human languages have words for expressing zero as the number of elements of a set, but the speakers of ancient languages did not consider 0 as a number, mainly because it was not obtained by counting.<p>There was no need to count the elements of an empty set, you just looked at it and it was obvious that the number was 0.<p>Counting with the traditional sequence can be interpreted as looking at the sequence in front of you, pointing at the <i>right</i> of an element and saying how many elements are at the left of your hand, then moving your hand one position to the right.<p>It is equally possible to count by looking at the sequence in front of you, pointing at the <i>left</i> of an element and saying how many elements are at the left of your hand, then moving your hand one position to the right.<p>In the second variant, the counting sequence becomes 0, 1, 2, 3, ...<p>The human languages do not use the second variant for 2 reasons, one reason is that zero was not perceived as having the same nature as the other cardinal numbers and the other reason is that the second variant has 1 extra step, which is not needed, because when looking at a set with 1 element, it is obvious that the number of elements is 1, without counting.<p>So neither 0 or 1 is a more &quot;natural&quot; choice for starting counting, but 1 is more economical when the counting is done by humans.<p>When the counting is done by machines, 0 as the starting point is slightly more economical, because it can be simpler to initialize all the bits or digits of an electronic or mechanical counter to the same value for 0, than initializing them to different values, for 1.<p>While 1 was a more economical choice for humans counting sheep, 0 is a choice that is always slightly simpler, both for hardware and software implementations and for programmers, who are less likely to do &quot;off by one&quot; errors when using 0-based indices, because many index-computing formulas, especially in multi-dimensional arrays, become a little simpler.<p>In conclusion, the choice between 0 and 1 never has anything to do with &quot;naturalness&quot;, but it should always be based on efficiency and simplicity.<p>I prefer 0, even if I have programmed for many years in 1-based languages, like Fortran &amp; Basic, before first using the 0-based C and the other languages influenced by it.')