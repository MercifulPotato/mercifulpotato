Item(by='mjb', descendants=None, kids=[25346714], score=None, time=1607439667, title=None, item_type='comment', url=None, parent=25345586, text='&gt; Which is directly at odds with needing TLA+, because if you do need it, it means the complexity of the algorithms is so great, that you won&#x27;t be able to keep them in your head and understand every aspect of their performance to make something work well.<p>I don&#x27;t think this is correct.<p>What TLA+ allows us to do is be more creative in our design and choice of algorithms, while allowing the computer to help us reason about whether the choices we&#x27;re making still result in a system that is correct. &quot;Correct&quot; in in this context means two things: &quot;safe&quot; as in it doesn&#x27;t lose or corrupt data, and &quot;live&quot; as in it eventually makes progress without deadlock or other blockers. That doesn&#x27;t capture &quot;meets the SLA&quot; or &quot;fast enough for real use&quot; or even &quot;tolerates gray failures&quot;. All of those are critical properties indeed - but unless you have fundamental safety and liveness you&#x27;re never going to get those properties anyway. You might think you have them, but then you&#x27;ll have a bad time eventually.<p>So TLA+ (and similar tools) aren&#x27;t a complete solution to the problem, but they are an exceptionally useful one. Fundamentally, they&#x27;re useful because distributed and concurrent protocols, even very simple ones like 2PC, are wickedly difficult to reason about clearly. Computers can help us reason, and specification languages can help us communicate clearly about our reasoning.')