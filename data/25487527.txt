Item(by='api', descendants=None, kids=[25492883], score=None, time=1608482818, title=None, item_type='comment', url=None, parent=25484560, text='I don&#x27;t buy it. I think there is in fact one &quot;trick,&quot; which is shedding the X86 decode bottleneck.<p>People always make the point that the X86 decoder is only ~5% of the die. Sure, that&#x27;s true, but keep two things in mind:<p>(1) While it&#x27;s only 5% of the die, it runs <i>constantly</i> at full utilization. The ALU is also only a small percentage of the die (5-10%). How hot does your CPU get when you&#x27;re running the ALU full blast? Now consider that there is a roughly ALU-sized piece always running full blast no matter what the CPU is doing because X86 instructions are so complex to decode. Not only does this give X86 a higher power use &quot;floor,&quot; but it means there&#x27;s always more heat being dissipated. This extra heat limits thermal throttling and thus sustained clock speed unless you have really good cooling, which is why the super high performance X86 chips need beefy heatsinks or water cooling.<p>(2) It apparently takes exponentially more silicon to decode X86 instructions with parallelism beyond 4 instructions at once. This limits instruction level parallelism unless you&#x27;re willing to add heat dissipation and power, which is a show stopper for phones and laptops and undesirable even for servers and desktops.<p>People make the point that ARM64 (and even RISC-V) are not really &quot;RISC&quot; in the classic &quot;reduced&quot; sense as they have a lot of instructions, but that&#x27;s not really relevant. The complexity in X86 decoding does not come from the number of instructions or even the number of legacy modes and instructions but from the variable length of these instructions and the complexity of determining that length during pipelined decode.<p>M1 leverages the ARM64 instruction set&#x27;s relative decode simplicity to do 8X parallel decode and keep a really deep reorder buffer full, permitting a lot of reordering and instruction level parallelism for a very low cost in power and complexity. That&#x27;s a huge win. Moreover there is nothing stopping them from going to 12X, 16X, 24X, and so on if it&#x27;s profitable to do so.<p>The second big win is probably weaker memory ordering requirements in multiprocessor ARM, which allows more reordering.<p>There are other wins in M1 like shared memory between CPU, GPU, and I&#x2F;O, but those are smaller wins compared to the big decoder win.<p>So yes this does foreshadow the rise of RISC-V as RISC-V also has a simple-to-decode instruction set. It would be much easier to &quot;pull an M1&quot; with RISC-V than with X86. Apple could have gone RISC-V, but they already had a huge investment in ARM64 due to the iPhone and iPad.<p>X86 isn&#x27;t quite on its death bed, but it&#x27;s been delivered a fatal prognosis. It&#x27;ll be around for a long long time due to legacy demand but it won&#x27;t be where the action is.')