Item(by='wtallis', descendants=None, kids=None, score=None, time=1604989741, title=None, item_type='comment', url=None, parent=25041899, text='The exact difference, as far as I&#x27;m aware, has never been publicly documented by Microsoft, and I have not attempted to properly and thoroughly reverse engineer that particular mess.<p>But the end result is that in the default state (buffer flushing enabled), most or all disk benchmarking tools show drastically lower write performance on consumer NVMe drives than should be expected compared to their theoretical advantage over SATA SSDs.<p>I&#x27;m not intimately familiar with the semantics of how flush&#x2F;sync commands get generated and passed through the Windows IO stack. Empirically, when an application tries to issue a write that is not to be buffered by the OS, it appears that Windows translates that into NVMe commands that constrain or entirely prohibit the SSD from doing its own write buffering, but for SATA SSDs writes are still by default issued in a manner that permits the drive to buffer freely. Figuring out exactly what&#x27;s happening without Microsoft&#x27;s help might require bus analyzers I don&#x27;t have. In effect, it seems that Windows is less willing to trust a NVMe SSD than a SATA SSD, and that doesn&#x27;t strike me as justified.<p>I&#x27;ve also noticed that recent builds of Windows 10 have changed their behavior when running one of our old benchmarking tools that plays back IO traces. On current Windows 10, the OS outright rejects any attempt by the trace playback application to issue a flush command to a NVMe SSD, whether or not buffer flushing is enabled at the driver level. This is definitely a change from earlier builds of Windows 10, and I think it is a change that only affects NVMe drives, not SATA drives. (I&#x27;m not testing very many SATA drives these days, and have to design my testing procedures entirely around the needs of testing NVMe drives.)')