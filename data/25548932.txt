Item(by='lmm', descendants=None, kids=None, score=None, time=1609044439, title=None, item_type='comment', url=None, parent=25548080, text='&gt; You&#x27;re not being quite honest here: of course just knowing if something compiles at all, whilst nice, is not that massive a difference (because you can fairly easily script an automated bisect that will skip non-compiling commits by itself). But that&#x27;s much weaker than what I actually said: knowing that the commit has passed your (1.5 h) CI, and making it so that all your merge commits on master will have this property.<p>In the (somewhat exceptional) case I&#x27;m talking about it was 1.5h for a standard build; IIRC straight compilation was more than half of it. In any case, there&#x27;s no real difference between compilation and &quot;CI&quot; here; you make your bisect script run whatever your CI test is, and skip if it fails, before running the part you&#x27;re actually testing.<p>&gt; Since you apparently haven&#x27;t tried it, maybe you should be a bit more careful about dismissing the utility out of hand?<p>I&#x27;ve tried workflows that made it easy to do a bisect that tests only commits from the history of master; unless you can explain how what you&#x27;re suggesting achieves something better than that, I don&#x27;t think not having used your precise script invalidates my views.<p>In fact it seems to me that your workflow is distinctly worsened by using rebase; if you used merges then some intermediate commits from feature branches would have been successfully built by CI (pre-merge builds, builds of &quot;early review&quot; PRs that were reworked before merging to master, builds that the developer deliberately ran on CI for whatever reason), whereas by using rebase you guarantee that only the post-merge states of master are available to you for bisection.<p>&gt; if you hit a commit that&#x27;s broken in a more subtle fashion than &quot;does not compile&quot; but that CI would have found it&#x27;s not always cheap to work out if you hit the regression or some unrelated temporary breakage.<p>What are you doing on CI that&#x27;s so different to what you&#x27;re doing during local development? If breakages don&#x27;t show up until you make the PR that you want to merge that&#x27;s bad for everyone; running the unit tests that pertain to the code you&#x27;re working on, if not the whole suite, before committing is just common sense. Of course it&#x27;s possible for something to work locally and break on CI, but that&#x27;s a very rare case (much rarer than generally-flaky tests, IME).<p>&gt; You are also making a several implicit assumptions which are unlikely to hold: it is not true that typically none of the utility of bisect manifests before you have identified the precise lines of code that caused an issue and understood them. If you have some acute problem in production, being able to reliably and quickly locate the feature branch that introduced it is very valuable (for getting the right people to look at the problem or even shipping some work around before the problem is fully diagnosed). Often certainly a recent deployment is to blame. But sometimes a deployment can exercise something that regressed much earlier and trigger a data corruption that is not noticed immediately.<p>Narrowing it down to a branch really isn&#x27;t much quicker than narrowing it down to a commit - you&#x27;ve already come up with the test case&#x2F;script, so it&#x27;s just a case of letting it run for maybe 5 more cases (if we assume maybe 30 commits on the feature branch). If you have a very small team then I guess narrowing it down to a specific branch might be much quicker than narrowing it down to a commit within that branch - but in that case the bisect is going to find the same thing, you can see when it&#x27;s got to the stage of testing commits from the same branch and start investigating there. And if you really want to bisect just via the history of master, you can always do that (admittedly with a little scripting, but you don&#x27;t seem to be shy of that).<p>&gt; Also, of course once you found some problematic merge there is no reason you wouldn&#x27;t then continue to use bisect to find the precise commit in the feature branch!<p>But you can&#x27;t do that if you&#x27;ve rebased the branch, because most of the branch history is (often) broken. If you were squash-merging you could dig out the &quot;original&quot; version of the branch (assuming it&#x27;s not been gced) and bisect there (assuming the problem is solely due to a change on that branch and not an interaction between that branch and a concurrent change on master), but if you&#x27;re rebasing you can&#x27;t even do that, because if developers are in the habit of rebasing then the &quot;original&quot; branch was probably rebased and force-pushed as well, so is likely to have old commits that don&#x27;t compile.<p>&gt; Assuming each developer lands something on master every two days on average, you&#x27;d have to deal with 250 merges a day. Say you can run about 5 sequential CI runs per work day, you&#x27;d need to test pretty large batches of ~50PRs per batch, which is more than I have experience with. Since you will only batch stuff that passed branch CI, and disregarding flakes for the moment, a batch should only fail because of an incompatibility of PRs within the same batch or against some very recent addition to master. So you can probably get the failure rate into the low single digit percentages, at which you&#x27;d need to do run enough alternative batches in parallel to cope with one or two bad PRs per batch. That still seems feasible if you put batches together intelligently<p>Yeah, that kind of approach definitely seems viable - we had a design like that sketched out, and may even have prototyped it at one point. It was decided that it wasn&#x27;t worthwhile, because we would still have needed our process for catching when flaky tests were introduced, and that process caught conflicts like this &quot;for free&quot;. So you are probably right that it is achievable to avoid ever merging non-compiling code to master except when meta problems happen, but I don&#x27;t think that actually takes you significantly closer to &quot;master is <i>never</i> broken&quot;.<p>&gt; Sure. But minimizing those bad commits (and probably even marking them after the discovery, e.g. via git-notes) pays off.<p>Yes and no; as long as they&#x27;re isolated, I don&#x27;t think pushing the rate of bad commits from 1% down to 0.5% or even 0.1% is really a game-changer. Obviously there&#x27;s a threshold where you have enough bad commits that it significantly disrupts being able to bisect at all, but unless you can manage to completely eliminate the possibility of a broken commit, there&#x27;s a wide spread of bad-commit rates where your workflow has to be pretty much the same. Just like how buying higher quality hardware to reduce defect rates is generally not worthwhile unless you go all the way to buying a mainframe with fully redundant everything and guaranteed uptime; you still need to be fault-tolerant if you buy the pro component with 99% uptime, so you might as well buy the consumer version with 98% uptime if it&#x27;s even 5% cheaper.')