Item(by='the_optimist', descendants=None, kids=[25962641, 25963634], score=None, time=1611952330, title=None, item_type='comment', url=None, parent=25955579, text='Compiling from high-level lang to GPU is a huge problem, and we greatly appreciate efforts to solve it.<p>If I understand correctly, this (CM) allows for C-style fine-level control over a GPU device as though it were a CPU.<p>However, it does not appear to address data transit (critical for performance). Compilation and operator fusing to minimize transit is possibly more important. See Graphcore Poplar, Tensorflow XLA, Arrayfire, Pytorch Glow, etc.<p>Further, this obviously only applies to Intel GPUs, so investing time in utilizing low-level control is possibly a hardware dead-end.<p>Dream world for programmers is one where data transit and hardware architecture are taken into account without living inside a proprietary DSL Conversely, it is obviously against hardware manufacturers&#x27; interests to create this.<p>Is MLIR &#x2F; LLVM going to solve this? This list has been interesting to consider:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;merrymercy&#x2F;awesome-tensor-compilers" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;merrymercy&#x2F;awesome-tensor-compilers</a>')