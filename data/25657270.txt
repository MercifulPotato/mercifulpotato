Item(by='calebkaiser', descendants=None, kids=None, score=None, time=1609937579, title=None, item_type='comment', url=None, parent=25654551, text='The price of GPU inference can be brutal, but there&#x27;s a lot you can do on the infra side to improve it:<p>- Spot instances<p>- Aggressive autoscaling<p>- Micro batching<p>Can reduce inference compute spend by huge amounts (90% is not uncommon). ML, especially anything involving realtime inference, is an area where effective platform engineering makes a ridiculous difference even in the earliest days.<p>Source: I help maintain open source ML infra for GPU inference and think about compute spend way too much <a href="https:&#x2F;&#x2F;github.com&#x2F;cortexlabs&#x2F;cortex" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;cortexlabs&#x2F;cortex</a>')