Item(by='jerf', descendants=None, kids=None, score=None, time=1601914842, title=None, item_type='comment', url=None, parent=24682111, text='Terminology&#x27;s job is to help us think, and I am distinctly underwhelmed by the track record of the &quot;concurrency vs. parallelism&quot; terminology. It sets up an absolution distinction for something that is almost always on a gradient, and it seems to abstract away some of the most important details to concentrate on less important details. In particular, the problem itself often tends to pick whether you&#x27;re &quot;concurrent&quot; or &quot;parallel&quot;, which means it&#x27;s not a useful metric to analyze possible solutions with since it doesn&#x27;t change based on your potential solution because it doesn&#x27;t provide a gradient to descend.<p>I find it much more productive to consider &quot;what resources do I have&quot; vs &quot;what resources is my code consuming&quot;. I think this focuses on the real questions, which is not &quot;is this more about parallelism or more about concurrency?&quot; but, do I have enough CPU cores for what I need? Am I using GPU resources? Can I use one or the other more effectively for this problem? Is this algorithm disk or RAM limited? etc. etc. In addition to simply subsuming all the considerations about &quot;parallel vs. concurrent&quot;, this mindset raises questions like that last one that isn&#x27;t even related to that question, but more directly gets at the <i>real</i> issues with making code run well on the real machines you have. This <i>does</i> provide a gradient to descend; &quot;oh, I&#x27;m trashing memory, I can fix it with this... I can move this CPU task to the GPU... I can trade this CPU expense to reduce memory load...&quot;, etc.')