Item(by='dragontamer', descendants=None, kids=None, score=None, time=1604604968, title=None, item_type='comment', url=None, parent=25001180, text='For most &quot;deep learning&quot; algorithms, the typical approach is to &quot;gradient ascent&quot;, following the slope of the hypergraph to find incrementally better-and-better results.<p>Genetic Algorithms however, do not necessarily need a gradient to find a better solution, though there&#x27;s similarities between GAs and hill-climbing. This blogpost explores a conceptual similarity between traditional gradient ascent &#x2F; backpropagation (currently a favored technique for neural nets), and how it relates to genetic algorithms.')