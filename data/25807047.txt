Item(by='h0l0cube', descendants=None, kids=None, score=None, time=1610844751, title=None, item_type='comment', url=None, parent=25801630, text='&gt; This is just like how (some) GPU vendors don&#x27;t document their hardware&#x27;s data structures and instruction set. You&#x27;re living dangerously if you choose to rely on their exact behaviour. The vendors really only expect you to use their API (Vulkan&#x2F;Metal&#x2F;Direct3D&#x2F;etc) because that abstraction layer allows them to radically and incompatibly change their hardware on a regular basis to improve performance or functionality.<p>GPUs are external processors, that require DMA transport, and can perform useful jobs in parallel with the CPU, and are produced by multiple vendors.  This almost certainly requires a HAL, and there&#x27;s little impact of making dynamic libraries calls compared to the benefit received.<p>Compare this to a co-processor, that basically can be called directly from CPU instructions, without DMA and a remote instruction queue.  Something like this gains great benefits from a compiler intrinsic, as a function call, particularly as a dynamic library is quite expensive.  This compiler intrinsic can just map to bitcode, which can then map to <i>whatever</i> at compile-time on the app store deployment side.<p>Why would people not want this option?  Why should this only be available to Apple&#x27;s internal products?  Why should Photoshop&#x2F;Lightroom&#x2F;Blender&#x2F;physics simulation&#x2F;video encoding&#x2F;AAA Games etc. run slower?<p>There&#x27;s no excuse for Apple not providing a way to use these instructions, either via bitcode, or providing a mechanism to detect relevant CPU metadata.  At a minimum, I expect hackers to take advantage of this to speed up their own applications, as well as open source forks.')