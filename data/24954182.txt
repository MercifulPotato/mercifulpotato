Item(by='spekcular', descendants=None, kids=[24955345, 24954849, 24956001, 24954607, 24954737], score=None, time=1604170389, title=None, item_type='comment', url=None, parent=24953440, text='&gt; There&#x27;s a saying in mathematics that when any field matures it turns into algebra. Life crossed a threshold from chemistry to biology on Earth, and developed exponentially from there. Order in mathematics crosses a similar threshold, as it becomes sufficiently structured to support algebraic reasoning. The subjective experience is like ice melting into a churning liquid, or a land-locked creature learning to fly. Once one has this experience, one cannot imagine thinking any other way.<p>Hi, this seems wrong to me.<p>First, as far as I can tell, such a saying does not exist. I&#x27;ve never heard it, despite having a fair amount of experience with mathematics (though not as much as you), and I can&#x27;t find it through Googling.<p>Second, and more substantively, it is not the case that mathematical fields inevitably turn into algebra. One only has to look at PDE, probability, and analytic number theory to see this is the case (to give just a few examples). All are highly mature fields and essentially non-algebraic. This is not to say that ideas from algebra are not occasionally useful, just that it should be obvious to anyone who opens an introductory graduate text in PDE that the subject has not &quot;turn[ed] into algebra.&quot;<p>&gt; The killer app for Haskell is parallelism. To this day it has the best implementation of parallelism; one can achieve a 7x speedup on 8 cores by adding a handful of lines to a program. This ease is a consequence of the functional model, and of considerable effort by Haskell&#x27;s developers.<p>I don&#x27;t think this statement is entirely wrong, but I don&#x27;t think it&#x27;s entirely right either. There are plenty of languages where one can get parallelism with just a few extra lines of code; Julia comes to mind immediately. Easy parallelism is hardly a Haskell-specific, or even FP-specific, feature. I would of course agree that designing things without mutable state makes writing parallel code easier, but this can be done in many high-level languages these days.<p>But that is a nitpick. A more substantive objection is that the sole purpose of parallelism is performance, and idiomatic Haskell is generally speaking slower, sometimes a lot slower, than the same algorithm written in C++ or similar languages. (The adjective <i>idiomatic</i> is important here â€“ I&#x27;m aware that one can with enough contortions write C-like code in Haskell, but this response undercuts of the point of highlighting Haskell-specific features like immutability, laziness, etc.) In particular, a few years ago it seemed like the people implementing parallel Haskell features didn&#x27;t understand or care about fundamental things like cache locality. Maybe this has changed?<p>There is also an amusing story about the &quot;ease&quot; of writing parallel quicksort in Haskell [1].<p>&gt; The real point of lazy evaluation is that it best supports this algebraic reasoning, as carbon best supports life.<p>It&#x27;s worth noting that Simon Peyton Jones is on record as saying that any &quot;Haskell 2&quot; should not be lazy [2].<p>[1] <a href="https:&#x2F;&#x2F;sudonull.com&#x2F;post&#x2F;75314-Parallel-quick-sort-on-Haskell-and-how-hard-it-was-to-write" rel="nofollow">https:&#x2F;&#x2F;sudonull.com&#x2F;post&#x2F;75314-Parallel-quick-sort-on-Haske...</a><p>[2] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=1924061" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=1924061</a>')