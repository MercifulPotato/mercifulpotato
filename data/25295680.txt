Item(by='nullc', descendants=None, kids=None, score=None, time=1607032360, title=None, item_type='comment', url=None, parent=25295295, text='&gt; I think the approaches based on invertible bloom filters are generally more efficient.<p>More computationally efficient for sure, but their communications efficiency is only good asymptotically.<p>For &quot;small&quot; cases such as where the set difference has a few hundred or thousand entries and especially where the set members are small (e.g. &lt;=64-bit hashes) there is a pretty large constant factor needed to get a low failure rate from invertible bloom filters.<p>E.g. for a set difference of size 128 we measured that you need 32x communications overhead to get a 1 in 10,000 failure rate.  8x overhead for 2048 items.  The issue is that iblt decodability depends on the non-existence of cycles in a random graph and this only holds for sufficiently large graphs.<p>For applications (such as Bitcoin transaction relay or -- I think -- SCM commits) which frequently synchronize a small number of items these communications overheads take that approach entirely out of the running.  Fortunately, at these sizes the quadratic computation of communications efficient approaches is not a big deal.<p>Interestingly, it is completely unambiguous that IBLT and dense code based reconciliation can be combined (sort of the dual to Raptor codes, which combine sparse fountain codes and dense RS-like codes for block erasure recovery)-- but it&#x27;s an open question if the combination can capture the advantages of both approaches (linear time decode and concrete communications efficiency (as opposed to asymptotic)) for set reconciliation as is the case for block erasure codes.')