Item(by='ritter2a', descendants=None, kids=None, score=None, time=1611839501, title=None, item_type='comment', url=None, parent=25934439, text='&gt; Right, but GPT-2 was the name of the particular ML architecture they were studying the properties of; not the name of any specific model trained on that architecture.<p>That sounds like it would have been a reasonable choice for naming their research, but isn&#x27;t the abbreviation &quot;GPT&quot; short for &quot;Generative Pre-trained Transformer&quot;? Seems like they very specifically refer to the pre-trained model, which I would also take from the GPT-2 paper&#x27;s abstract: &quot;Our largest model, GPT-2, is a 1.5B parameter Transformer[...]&quot; [1]<p>---\n[1] <a href="https:&#x2F;&#x2F;cdn.openai.com&#x2F;better-language-models&#x2F;language_models_are_unsupervised_multitask_learners.pdf" rel="nofollow">https:&#x2F;&#x2F;cdn.openai.com&#x2F;better-language-models&#x2F;language_model...</a>')