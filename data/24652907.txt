Item(by='liuliu', descendants=None, kids=None, score=None, time=1601569856, title=None, item_type='comment', url=None, parent=24652656, text='The training data is around 40GiB: <a href="https:&#x2F;&#x2F;d4mucfpksywv.cloudfront.net&#x2F;better-language-models&#x2F;language_models_are_unsupervised_multitask_learners.pdf" rel="nofollow">https:&#x2F;&#x2F;d4mucfpksywv.cloudfront.net&#x2F;better-language-models&#x2F;l...</a><p>Consider the compression ratio for such text, and you need 2-word &#x2F; 3-word to 1000-word dictionary, and at 1000-word length, it will be very sparse. I would guess an accelerated structure for such lookup would take around 60GiB. It would require quite a bit of engineering to get it right otherwise can easily blow-up pass that.')