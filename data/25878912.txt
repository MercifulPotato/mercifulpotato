Item(by='csense', descendants=None, kids=[25878941, 25880510, 25879050], score=None, time=1611368245, title=None, item_type='comment', url=None, parent=25878657, text='One of the problems with this is opening up to criticism, liability, and further argumentation.<p>If you say &quot;We removed them because of their political views,&quot; then you get criticized for being openly partisan.<p>If you say &quot;We removed them because their comments were inciting violence,&quot; you&#x27;ll get arguments about how their comments didn&#x27;t actually tell people to do violent things (for example, in the US I&#x27;m sure for the near future, a lot of people will be arguing very loudly whether the specific phrases &quot;fight like hell&quot; and &quot;trial by combat&quot; are metaphorical rhetorical devices or literal instructions to take violent actions.)<p>People will start finding and pointing out situations where you made different decisions in near-identical circumstances, and criticize you for that too.<p>It&#x27;s better for users if the platform&#x27;s transparent.<p>But the platform has no incentive to do so.  Less than 1% of the time will users say &quot;Okay you told us why this person was banned, we now see the ban&#x27;s fair, and we&#x27;ll shut up and go home.&quot;  Instead, 99% of the time they&#x27;ll turn your explanation into another reason to be enraged at you, or argue you need to reconsider, or even sue you because that line of reasoning makes you liable.<p>From the platform&#x27;s point of view, it&#x27;s usually <i>better</i> to let the reasoning behind a ban be a question mark.<p>&gt; &quot;a big neural network said so&quot;<p>This is part of it too.  With ML-driven bans or other actions, it&#x27;s possible that <i>nobody</i> understands what criteria it uses to make decisions.')