Item(by='gfodor', descendants=None, kids=[24953878, 24958300, 24953874, 24954258, 24954030], score=None, time=1604168129, title=None, item_type='comment', url=None, parent=24951024, text='Without releasing data, Tesla is forcing analysts to focus on anecdotes, which suffer from all the usual problems like selection bias. What we need to know:<p>- How often do drivers intervene?<p>- How often do drivers fail to intervene and the system causes an accident?<p>- How often does the system take an action which has a high likelihood of having prevented an accident?<p>- How often does the system do so when it seems likely a human driver would have failed to do so?<p>- Integrated together, what&#x27;s the expected dynamics of these probabilities and their net impact insofar as the system being releases more widely creates training data to help improve them more quickly over time?<p>It could very well turn out to be the case that this system is purely positive, strongly net positive, or neutral in harm reduction. The question then is, given that, what ethics should inform its release: is putting the stress on the driver sufficient if it say, saves 1000 lives in the next six months and will harm noone in exchange, other than some drivers having to intervene and have moments of stress? What if it will save 1000 and 10 people will be harmed by failing to intervene? What if it&#x27;s an even swap, harming people who fail to intervene in exchange for saving people who would have inevitably been lost to accidents they couldn&#x27;t have prevented?<p>Saying &quot;this is wrong to do&quot; based upon anecdotes is dumb analysis. Saying &quot;this is wrong to do&quot; based upon an absolutist form of ethics is fair, but it also means you reject that we should be trying to solve self driving. If you think it&#x27;s wrong to do and are not doing either, you ought to articulate what scenario in terms of data would justify the action vs not, even if that data is unknown right now.')