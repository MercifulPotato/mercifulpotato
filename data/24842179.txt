Item(by='PeterisP', descendants=None, kids=None, score=None, time=1603226651, title=None, item_type='comment', url=None, parent=24841937, text='Nope, we have loads of experimental evidence (and I mean, it&#x27;s something one can verify at home for smaller datasets, it does not take <i>that</i> much compute) that neural MT gets significantly better results than what we could (and still can) achieve with &quot;pre-neural&quot; SMT methods on the exact same corpus. A general benchmark for comparing the effectiveness of different MT approaches are the WMT conference series (e.g. <a href="http:&#x2F;&#x2F;www.statmt.org&#x2F;wmt20" rel="nofollow">http:&#x2F;&#x2F;www.statmt.org&#x2F;wmt20</a>) shared tasks where the systems are trained on the same corpus.<p>They&#x27;re still not perfect, and sure, you get weirdness, but it has become significantly better according to all metrics including human comparisons of different translation aspects, which are expensive&#x2F;rare to do but have been done quite a few times; there&#x27;s a clear consensus that deep learning &quot;works&quot; for ML.<p>There are certain niches where other methods may still be better (IIRC languages with <i>very</i> little data, and translation of specific &#x27;controlled language&#x27; domains), but for mainstream MT I think that nowadays nobody would decide to build a non-neural system.')