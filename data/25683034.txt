Item(by='activatedgeek', descendants=None, kids=None, score=None, time=1610097883, title=None, item_type='comment', url=None, parent=25682792, text='&gt; I tried a great deal of ML approaches, only to find recently that by better accounting for the optical specificities of the problem I was trying to solve<p>I want to point out, that any serious machine learning researcher is not oblivious to this, despite the deep learning boom suggesting to the contrary. Modern methods have shown that we are capable of building predictors with surprisingly complex representations, that can solve large-scale downstream tasks. i.e. our models are &quot;flexible&quot; enough.<p>The next challenge is whether they favor the &quot;right kind&quot; of solutions. For instance, Convolutional Neural Networks (CNNs) are architecturally just sparse version of Fully-Connected Neural Networks. Why is it then that CNNs perform far better on images? A key reason is that &quot;inductive biases&quot; afforded by MLP aren&#x27;t strongly favored towards images. Another instance of this is the covariance functions used is Gaussian Processes - the Squared Exponential Kernel is very flexible and can in principle fit anything possible. Nevertheless, if the problem has specific structures, say periodicity, one better use the Periodic Kernel because it&#x27;s inductive biases rightly align with the kind of solutions we expect.<p>&gt; The lesson is : physics beats AI.<p>As a consequence, the single biggest reason physics would beat a generic AI in the short-term is precisely due to our ability to explicitly provide inductive biases that align with our expectations from the physical system.<p>We haven&#x27;t found the secret sauce for every possible system in the universe. I don&#x27;t think we can, either. But what we can do is devise ways to &quot;control&quot; such inductive biases we can encode in machine learning systems, which align with our expectations of the way the system should behave.')