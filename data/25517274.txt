Item(by='usgroup', descendants=None, kids=None, score=None, time=1608729245, title=None, item_type='comment', url=None, parent=25516910, text='Under the hood Stan attempts to find globally optimal parameter values for your function which you&#x27;ve expressed as a joint probability density. To do this it relies on the same MCMC theoretical results which indicate how the recursive process of sampling and posterior updating leads to the global optimum. The big deal about Stan is that its algorithm for doing this is state-of-the-art, and that it can work with a huge variety (including custom) density functions by utilising auto-differentiation.<p>Sampling is a slow approach when there are other alternatives. For example, if you are after OLS regression, you can do the equivalent with Stan but it may be an order of magnitude slower than plain OLS. Further, the calculation of your likelihood function will scale linearly with the size of the data. But adding new parameters will scale exponentially, so you may find that a model with 2 free parameters which takes 10 minutes to fit takes 2 hours with 3 parameters.<p>A good thing about Stan however, is that it is parallelisable so you can run it on many cores (and it will scale linearly for a good while) and you can also run it on MPI across many machines. Some regression functions with very large matrices support GPUs (although Stan requires double precision to work). So to some extent you can &quot;throw more money at it&quot; to get a result out and it has been used for very big data problems in astronomy for example which however utilised something like 600k cores if memory serves correctly.')