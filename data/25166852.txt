Item(by='Taniwha', descendants=None, kids=[25167176], score=None, time=1605919172, title=None, item_type='comment', url=None, parent=25159704, text='A simplistic answer is &quot;every instruction can be completed in 1 clock at speed&quot; (at speed means 1GHz or whatever your system clock is)<p>I say &#x27;simplistic&#x27; because some operations (multiply is a great example) really can&#x27;t and are likely pipelined - so let&#x27;s change it to &quot;every instruction can be issued in 1 clock at speed&quot;.<p>IMHO CISC is from an era (an era I learned to program and design hardware) when memory was expensive, and memory bandwidth was doubly so - the first mainframe I spent quality time with had physical core (actual little ferrite rings hand threaded onto planes) read cycle time was one microsecond - at some point in the late 70s we bought 1.5 megabytes of core for US1.25 million dollars. The machine we used it on (a Burroughs 6700) had a highly encoded instruction set, most were a byte in length. This was a smart choice at a time when memory bandwidth was so low (and caches often non-existant). A common design paradigm was microcode - turning a tightly encoded instruction into many clocks of work inside the CPU.<p>Things changed in the mid-to-late 80s, especially at the point (or just before the point) where we had the space to move caches on-chip (or very close to on-chip) this allowed designers to take the time and space they&#x27;d previously used to decode complex (but compact) instructions and use simpler but large instructions and use faster clocks (and shallower pipes) - that was the &#x27;RISC revolution&#x27; (even though some people had been using those ideas before).<p>I think Intel was the best positioned to come out of the CISC era - its instruction set was the most RISC-like of its CISC competitors')