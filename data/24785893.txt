Item(by='monocasa', descendants=None, kids=[24785974, 24785989], score=None, time=1602747245, title=None, item_type='comment', url=None, parent=24785823, text='LDM&#x2F;STM are awful for modern cores, and was even microcoded on at least early ARM cores despite them being, you know, RISC.<p>As for why it&#x27;s awful, it&#x27;s the amount of loads and stores that can be in flight associated with a single instruction when an exception is taken, and how the instruction is restarted afterwards.  ARM-M cores make this a little better by exposing &quot;I&#x27;ve executed this many loads so far in the LDM&quot; in architectural state so it can be restarted without going back and reissuing loads (so it can be used in MMIO ranges), but the instruction really only shines in simple, in order designs.  They&#x27;re almost as bad to implement on modern cores as the load indirect instructions you see on some older CISC chips.<p>Additionally, LDM&#x2F;STM really shined in either cache-less or cache-poor designs where instruction fetch is competing for memory bandwidth with the transfer itself.  That doesn&#x27;t really apply to these modern cores with fairly harvard looking memory access patterns.  Therefore, getting rid of these instructions isn&#x27;t the biggest deal in the world.<p>So to answer your question, they absolutely could have done that, but chose to use the transition to AArch64 to remove pariahs like LDM&#x2F;STM from around their neck because they&#x27;re more trouble than they&#x27;re worth from a hardware perspective on modern OoO cores.  The LDP&#x2F;STP instructions are the bone they throw you to improve instruction density versus memory bandwidth to&#x2F;from the registers, but they don&#x27;t really want each instruction being responsible for more than a single memory transfer for core internal bookkeeping reasons.')