Item(by='strgcmc', descendants=None, kids=None, score=None, time=1603721429, title=None, item_type='comment', url=None, parent=24888503, text='Your twist on the analogy is better, but still misses one crucial element IMO. Forgetting your keys, driving your car into a lake, or running out of gas are all very, &quot;obvious&quot; and transparent failures or error states to the user, or you could say that for a user they can easily fail fast and also understand why that state is undesirable. The user is not left wondering, why would I need keys to start my car, or why doesn&#x27;t my car float on water, or why does my car need gas to run...<p>Forgetting to change the default password on a system before starting it up and putting it into production (negligently or not), is not a very &quot;obvious&quot; type of failure. Hey the software is working! People can us it to accomplish their daily tasks! Everything is fine! There are basically no signals to the average, non-sophisticated user that something is amiss, for the vast majority of security vulnerabilities&#x2F;misses.<p>So the real problem IMHO, is less about addressing systematic lack of competency or lack of oversight or licensing or things like that, and better tackled as questions of better UX, of failing fast and transparently to the user, or of making invalid&#x2F;undesired states impossible (and user education yes, to some degree... but cars really do not require that crazy of an investment in training to operate, though different countries certainly set different expectations&#x2F;standards). These are the sorts of problems that tech is used to solving, that the tech industry is optimized around solving. Of course, for tech to care about working on these problems, requires market incentives to be there (and by and large, the incentives are not there today). Which is what one of the GP ideas about fines and insurance costs&#x2F;premiums is trying to address.')