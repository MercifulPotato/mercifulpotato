Item(by='michaericalribo', descendants=None, kids=None, score=None, time=1611612058, title=None, item_type='comment', url=None, parent=25909034, text='This is a great illustration of the tensions between theory and practice in ML &#x2F; AI:<p>To some degree, this may be &quot;just&quot; aa matter of a poor training set: too few &#x2F; unbalanced examples. Can&#x27;t we just use a better dataset?<p>But this also reflects a more systemic problem with descriptive &#x2F; learned behavior, rather than ontological representations: we will <i>always</i> have these dataset problems, to be fixed after the fact when someone points out the flaw. See also facial recognition of non-white faces.<p>We want intelligent systems that can learn from the world, but that&#x27;s not an accurate mental model of how <i>humans</i> learn. Often, we learn our values and develop opinions <i>in spite</i> of empirical representationsâ€”I don&#x27;t read the newspaper to find out what Muslim people are like, I start from my values &#x2F; convictions (all men are created equal, every person deserves respect and the benefit of the doubt prima facie) and <i>maybe</i> update on the basis of observations of media depictions.<p>Yes, a computer may be able to imitate human speech after reading a lot of text...but that doesn&#x27;t mean it understands the content of that text.')