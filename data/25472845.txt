Item(by='coder543', descendants=None, kids=[25472913], score=None, time=1608330412, title=None, item_type='comment', url=None, parent=25472612, text='Preface: If you have actually encountered applications that <i>must</i> be run on 8-socket systems because those are <i>literally</i> the only fit for the application... I would love to hear about those experiences. With the advent of Epyc Rome, most use cases for these 8-socket systems vanished instantly. It would be fascinating to hear about use cases that still exist. Your experiences are obviously different than mine.<p>If you need more than 8TB of RAM, <i>with the right application design</i> you can <i>probably</i> do better with fast Optane Persistent Memory or Optane SSDs, and an effective caching strategy. You can have many dozens of terabytes of Optane storage connected to a single system, and Optane is <i>consistently</i> low latency (though not as low latency as RAM, obviously).<p>If you need more compute power, you can <i>generally</i> do better with multiple linked machines. You can only scale an 8-socket system up to 8 sockets. You can link way more machines than that together to get more CPU performance than any 8 socket system could dream of.<p>----------<p>I didn&#x27;t expect you to read and respond so quickly, so I had edited my previous comment before you submitted your reply.<p>This was a key quote added to my previous comment:<p>&gt;&gt; So you really have to be in a very obscure situation which can&#x27;t fit onto a dual socket Rome server, but can fit within a machine less than 2x larger. (28 cores * 8 sockets is less than 2x larger than 64 cores * 2 sockets)<p>In response to your current comment,<p>&gt; If the interconnect is your bottleneck, you spend money on the interconnect to make it faster. Basic engineering: you attack the bottleneck.<p>Exactly. Using a dual-socket Epyc Rome system would be <i>more than</i> half as powerful as the biggest 8-socket Intel systems, but it would reduce contention over the interconnect dramatically, which means that many applications that are simply <i>wasting money</i> on an 8-socket system would suddenly work better.<p>This also goes back to my comment about using accelerators instead of an 8-socket system.<p>The odds of encountering a situation that just happens to work well with Intel&#x27;s ridiculously complicated 8-socket NUMA interconnect, but <i>can&#x27;t</i> work well over a network, and <i>can&#x27;t</i> work well on a system half the size and <i>requires</i> enormous amounts of RAM to keep the cores fed, the odds seem vanishingly small... and in that case, we still have to consider whether an accelerator (GPU, FPGA, or ASIC) could be used to make a solution that is a better fit for the application anyways, and if so, you&#x27;ll save large amount of money that way as well.<p>So, to make buying an 8-socket system make sense, the application must require performance that is...<p>- less than twice a dual socket Epyc Rome system, but greater than one dual socket Epyc Rome system can handle<p>- not dependent on transferring huge amounts of data around the interconnect<p>- dependent on very low latency communication between NUMA nodes<p>- needs enormous memory bandwidth for each NUMA node<p>- needs huge amounts of RAM on each memory channel (so you can&#x27;t just use HBM2 on a GPU to get massive amounts of bandwidth, for example)<p>- etc.<p>It&#x27;s a niche within a niche within a niche.<p>As I said in an earlier comment, I probably <i>should</i> be more impressed instead of being so cynical about the usefulness of such a machine. They are engineering marvels... but in almost every case, you can save money with a different approach and get equal or better results.<p>That&#x27;s why 8-socket server sales made up such a small percentage of the market, even before Epyc Rome came in and completely obliterated almost all of the very little value proposition that remained.')