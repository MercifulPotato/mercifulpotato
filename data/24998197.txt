Item(by='JeremyNT', descendants=None, kids=[24998355], score=None, time=1604584460, title=None, item_type='comment', url=None, parent=24995985, text='&gt; <i>The polls were wrong, and 538 told us what would happen in that scenario</i>.<p>How do you define the mythical &quot;standard polling error&quot; though? They have historical data, but polling techniques are always shifting.<p>Per the article:<p>&gt; <i>Suppose we’d included wider uncertainty intervals so the outcome was, say, within the 50% predictive interval. Fine. If we’d given Biden a 75% chance of winning and then he wins by a narrow margin, the forecast would look just fine and I’d be happier with our model. But the polls would still have messed up, it’s just that we would’ve better included the possibility of messing up in our model.</i><p>If presidential polls are consistently off by large margins, and 538 considers those margins to be unlikely every time, (which seems to be where we&#x27;ll land this year as well as we did in 2016), then maybe it&#x27;s not just the polls that are wrong, but the 538 <i>assumptions</i> about poll accuracy as well.<p>2016 on its own doesn&#x27;t condemn the 538 methodology, but 2020 is starting to look like a trend. Yes, outlier events happen, but if they happen <i>consistently</i>... well, maybe your definition of &quot;outlier&quot; is incorrect. The sample size on presidential elections is small, and maybe there&#x27;s just too much uncertainty for a model like 538&#x27;s to end up being useful.')