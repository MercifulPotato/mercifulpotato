Item(by='hellepardo', descendants=None, kids=[25063197, 25062383, 25062692, 25062672, 25062514], score=None, time=1605124333, title=None, item_type='comment', url=None, parent=25061438, text='So, don&#x27;t get me wrong.  This is a fun project and a neat little device.  I don&#x27;t mean to take away from it in any way.<p>However it is really important to consider <i>why</i> baby monitors are so primitive: because the cost of a false negative is huge.  I didn&#x27;t see any mention of this in the author&#x27;s experiments (only a &#x27;&gt;98% accuracy&#x27; note).  So let&#x27;s talk about this a little bit: is &quot;accuracy&quot; what we want?  Probably not---I don&#x27;t care if I get accidentally notified, but I care <i>very much</i> if I <i>don&#x27;t</i> get notified when the baby is crying.  So you want to weight your classifier&#x27;s predictions heavily <i>against</i> false negatives (at the price of false positives).  It would be good to make an ROC curve to characterize this behavior.  More importantly though, any predictive model assumes a stationary distribution; i.e., training conditions accurately reflect test conditions.  But will they in real life?  What about when your neighbor&#x27;s house is under construction?  Can interference from chainsaws cause the model to fail to detect the baby crying?  What about the dude down the street with his super loud motorcycle?  What happens then?  I bet the training set doesn&#x27;t have situations like this.<p>I really, really don&#x27;t want to come off like a wet blanket here.  But I feel obligated to, because this is a model that directly impacts the welfare of a human, and so we should at least talk about or discuss potential drawbacks.  (Again, cool weekend project, just, we need to be clear about the implications of outsourcing the decision of whether the baby is crying to a black-box model where we can&#x27;t interpret what it&#x27;s doing.)')