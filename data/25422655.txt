Item(by='HexagonalKitten', descendants=None, kids=None, score=None, time=1607976984, title=None, item_type='comment', url=None, parent=25416944, text='&gt; I posit that it&#x27;s more about a lack of knowledge&#x2F;education around good software security design&#x2F;implementation than spec changes or scope creep.<p>It probably depends on what your team is good at. I tend to work in fairly security conscious teams. They check the length of buffers, don&#x27;t rely on anything from the user, etc. So most of our issues tend to be from our architecture being drastically changed to integrate our solution into something else, or vice versa. Stuff is dropped on us, opaque libraries and internal code, and entirely new goals created around the big stuff, data persistence, key management, and so on.<p>But when I had an actual stable product that could not get spec changes (it was custom hardware) we knew from the beginning how the project would go and how all of the pieces would fit and it went together cleanly. But, it turns out many embedded engineers don&#x27;t (didn&#x27;t maybe, it was a while ago) program as defensively because they&#x27;re used to owning the entire scope. If they put a value somewhere they expect it to be there, unmolested, when they want it. That project was full of buffer overflows and bad crypto.<p>&gt; If good security design practices were stressed when teaching software development (and&#x2F;or included in resources for autodidacts)<p>Yeah. I&#x27;m not sure how but it does tend to be lacking. Everyone probably needs a different hook. For me it was quality. How could I say my software does X if you have a way to break it?')