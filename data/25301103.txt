Item(by='RegW', descendants=None, kids=[25301499, 25302800, 25302894, 25301531, 25301990, 25301185], score=None, time=1607080320, title=None, item_type='comment', url=None, parent=25296900, text='I wrote a spelling checker in the 1980&#x27;s<p>In my first job I worked for Tasman in Leeds and produced a Word Processor for IBM PC compatibles in 8086 assembler with some help, and then a spelling checker.<p>For the spelling checker I did a whole load of analysis on a  70,000 word list from Collins and produced a list of tokens to represent common strings of letters.  However, in the end I really had to cut the original word list down to get the whole thing onto a single 360K floppy.<p>After I left Tasman, I was lying in bed one night still thinking about it and realised where I had gone wrong.  The tokenising thing, which someone else had put me onto had blinded me.  I had stared at word lists for months and hadn&#x27;t pinned down the obvious pattern.  All but 26 words in the 70,000 word list shares the bulk of their characters with the word before it.<p>So the solution is use 5 bits of the first byte as a count of chars from the word before, 3 bits indicate commons enddings (ship, s, ing) or that the following bytes are tokens for the rest of the word.  With this I got the word list compressed to less than 2 bytes per word.<p>I took this back to Tasman.  They put all 70,000 words and the spelling checker onto a 175K floppy for the ZX Spectrum +3.<p>[Edited for typos]')