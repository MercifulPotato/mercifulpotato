Item(by='xiphias2', descendants=None, kids=None, score=None, time=1603806368, title=None, item_type='comment', url=None, parent=24907263, text='NVIDIA is adding more features, like super-scaling to games, and machine learning models are improving faster than Moore&#x27;s law. I expect those fancy features, like tensor cores to be a must for 4K gaming in the future.<p>What&#x27;s funny is that the same strategy (leaving out specialized instructions from consumer level hardware) that worked extremely well for CPUs won&#x27;t work for GPUs in my opinion.<p>If you look at ray tracing hardware (I have it on my RTX 2070 Max-Q card in my laptop), it sucks right now, but it&#x27;s improving very fast as machine learning algorithms improve.<p>I just found this:<p><a href="https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;amd-big_navi-rdna2-all-we-know" rel="nofollow">https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;amd-big_navi-rdna2-all-we-...</a><p>One thing that I forgot is that AMD can just focus on inferencing hardware (INT16 operations), and leave out tensor cores...so actually you are right, I&#x27;ll just stay with NVIDIA GPUs.')