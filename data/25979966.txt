Item(by='joefourier', descendants=None, kids=None, score=None, time=1612110881, title=None, item_type='comment', url=None, parent=25979829, text='In their basic form, genetic algorithms essentially approximate gradient descent by randomly sampling the search space to find the direction of steepest descent (similar to a finite differences method). If the loss function is differentiable, you are essentially wasting computing resources on calculating paths that are known not to be optimal, hence, why not apply gradient descent directly instead of a slower approximation?<p>Brute force search would be even more inefficient, genetic algorithms at least throw away unpromising directions.')