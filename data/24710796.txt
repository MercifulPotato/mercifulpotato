Item(by='msamwald', descendants=None, kids=[24711160], score=None, time=1602093206, title=None, item_type='comment', url=None, parent=24710588, text='&gt; GPT-3 didn&#x27;t even get SOTA on SuperGLUE.<p>Of course neither GPT-3 nor the PET paper claim SOTA on SuperGLUE. They used a few-shot learning setup with 32 examples per task The normal SuperGLUE setup has hundreds or thousands of examples per task [1].<p>&gt; In general, paper with &quot;new variation of cloze pre-training task for this specific task&quot; is a new section of the literature that is rapidly becoming sort of mundane and uninteresting because there are so many papers doing small variations of the same basic idea.<p>Could you please link to some of the work you are referring to?<p>[1] Table 1 in <a href="https:&#x2F;&#x2F;w4ngatang.github.io&#x2F;static&#x2F;papers&#x2F;superglue.pdf" rel="nofollow">https:&#x2F;&#x2F;w4ngatang.github.io&#x2F;static&#x2F;papers&#x2F;superglue.pdf</a>')