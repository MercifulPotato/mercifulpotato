Item(by='meowface', descendants=None, kids=None, score=None, time=1607568029, title=None, item_type='comment', url=None, parent=25366643, text='&gt;Not illegal in the US.<p>Federally, no. It appears to be illegal in 46 states, though. And I personally do think (actual) rape should be illegal to film, distribute, or profit off of. (I&#x27;m a major detractor of any kind of censorship, but I believe the three exceptions should be [actual] child pornography, [actual] rape pornography, and the sort of porn where humans or non-human animals are [actually] tortured&#x2F;killed.)<p>&gt;In fact, there is plenty of consensual porn made to look like women are being assaulted or coaxed into sex.<p>Of course, but that&#x27;s completely different, and is indirectly addressed in the article:<p>&gt;To be clear, most aren’t of 13-year-olds, but the fact that they’re promoted with that language seems to reflect an effort to attract pedophiles.<p>&gt;The issue is not pornography but rape. Let’s agree that promoting assaults on children or on anyone without consent is unconscionable. The problem with Bill Cosby or Harvey Weinstein or Jeffrey Epstein was not the sex but the lack of consent — and so it is with Pornhub.<p>This is also part of the core problem. If some content is reported, and it&#x27;s not clear to a moderator if it&#x27;s consensual or non-consensual, it may not get removed even it turns out it actually was non-consensual. Same with porn of minors: if it&#x27;s not unambiguous that an actual minor in a video is below 18, Pornhub often wouldn&#x27;t remove the content.<p>---<p>&gt;It monetizes all content, even illegal content which hasn&#x27;t been reviewed or reported. This is how it works for all sites that allow user generated content, including YouTube.<p>I agree it wasn&#x27;t appropriate of the reporter to sandwich &quot;racist and misogynist content&quot; between those other far more horrible things. I don&#x27;t necessarily endorse 100% of the article; just the core point.<p>YouTube manages to largely avoid this problem by doing something closer to a whitelist approach - if content doesn&#x27;t fall into the category of &quot;non-pornographic&quot;, it&#x27;s semi-automatically blocked and removed.<p>&gt;There are plenty of BDSM videos including footage of women being asphyxiated in plastic bags. You can find men and women with hot wax being poured on their genitalia or needles piercing their genitalia. You can find golden showers and scat too. None of that content is illegal in the US.<p>It is indeed very important to distinguish between things that some may find extreme or disgusting and things that are unethical and should be or are illegal. The key issue is consent vs. non-consent. If a woman has a BDSM fetish and is filmed being asphyxiated, that&#x27;s completely different from a group of people attacking an unsuspecting woman and asphyxiating her.<p>The article should&#x27;ve gone to greater lengths to separate this when writing sentences like that, but it also makes itself very clear in other parts that the issue is the non-consensual acts, and specifically the fact that Pornhub is directly profiting off of them and not effective at detecting or removing the vast majority of it.<p>&gt;There is only so much a site can do to weed out illegal user generated content.<p>Exactly. As you say, any large platform that allows arbitrary uploads won&#x27;t be able to be effective at this - hence why a whitelist approach is required here instead of a blacklist, which is sensibly now what they&#x27;re going to be doing.<p>&gt;You can find videos of adults beating children with all manner of household items. You can find videos of vehicles running over people with their twisted, mangled bodies laying in the street. You can find killings of rival gangs in Mexico and Brazil, or state executions in Iran and Iraq. None of this stuff is illegal in the US.<p>Of course, but how is any of that relevant to the sentence you&#x27;re quoting? Recordings of gore and violence are legal. Child pornography isn&#x27;t legal. It&#x27;s a completely different situation if Twitter, Reddit, and Facebook are swarming with gore vs. swarming with child pornography.<p>&gt;It would be nice if the author focused on the illegal content, rather than the stuff he finds distasteful.<p>I agree, and it is important to be very precise and explicit when advocating for censorship, so I understand the need for semantics and picking apart the reporter&#x27;s fuzzy condemnations.<p>But I don&#x27;t understand being seemingly dismissive of the core issue discussed in the article - massive amounts of real non-consensual porn and child pornography being directly profited off of while also being ineffectively combatted (and being largely infeasible to properly combat), growing every day, accessible to the world.<p>It&#x27;s quite plausible they made and are making tens of millions of dollars directly from that content. This doesn&#x27;t necessarily mean they encouraged it or were turning a blind eye to it, but I think they were kind of glancing away from it until a big enough fire was lit under their ass by NYT and payment processors. They had a large, financial, perverse incentive (pun intended) to not dedicate a massive amount of resources to the problem until now. And now it appears they are dedicating those resources by implementing this whitelist approach, which is painstaking but is the only way to do it safely.<p>You&#x27;re absolutely right to critique the article in the areas where it lacks rigor or slides a bit further down a slippery slope, but there&#x27;s still a big elephant in the room here regarding the actual situation, independent of the article&#x27;s faults.<p>Let&#x27;s separate the core problem away from the greater cloud of general moral panic. There&#x27;s a kind of &quot;fallacy fallacy&quot; here (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Argument_from_fallacy);" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Argument_from_fallacy);</a> the existence of a moral panic doesn&#x27;t necessarily mean there isn&#x27;t a real, harmful problem that originally sparked it. They can contain kernels of reality that need to be addressed - e.g. the &quot;vapes killing people&quot; moral panic containing the actual truth that many bootleg THC vapes contain harmful filler compounds.')