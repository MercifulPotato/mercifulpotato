Item(by='MrPowers', descendants=None, kids=[25510550, 25509960, 25509911], score=None, time=1608663382, title=None, item_type='comment', url=None, parent=25509123, text='Some additional context:<p>* Companies are querying thousands &#x2F; tens of thousands of Parquet files stored in the cloud via Spark<p>* Parquet lakes can be partitioned which works well for queries that filter on the partitioned key (and slows down queries that don&#x27;t filter on the partition key)<p>* Parquet files contain min&#x2F;max metadata for all columns.  When possible, entire files are skipped, but this is relatively rare.  This is called predicate pushdown filtering.<p>* Parquet files allow for the addition of custom metadata, but Spark doesn&#x27;t let users use the custom metadata when filtering<p>* Spark is generally bad at joining two big tables (it&#x27;s good at broadcast joins, generally when one of the tables is 2GB or less)<p>* Companies like Snowflake &amp; Memsql have Spark connectors that let certain parts of queries get pushed down.<p>There is a huge opportunity to build a massive company on data lakes optimized for Spark.  The amount of wasted compute cycles filtering over files that don&#x27;t have any data relevant to the query is staggering.')