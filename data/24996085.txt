Item(by='dragonwriter', descendants=None, kids=None, score=None, time=1604557588, title=None, item_type='comment', url=None, parent=24995669, text='&gt; My issue with 538’s models is how wide their margins are. I believe they use “results are within the 80% bounds of our model” to gauge whether or not it reasonably “worked.”<p>They also assess their models in aggregate, but that obviously isn&#x27;t applicable to the Presidential model in isolation, since it has so few runs; OTOH, it does validate their broad approach while leaving open the possibility that there is something especially wacky with the Presidential model. But without lots of runs of that model, you can&#x27;t really clearly single that out from chance errors, and if there are isolated instances of unusual effects that show up only in the Presidential election and only, say, for one or two cycles (e.g., if there is an effect around Trump which is atypical but not new normal) that aren&#x27;t accounted for but also aren&#x27;t long-term features, you probably can&#x27;t even reliably detect them once there are large numbers of runs of the Presidential model. Unfortunately, validating a statistical predictive model isn&#x27;t itself a a probabilistic exercise subject to small-N problems.')