Item(by='rstuart4133', descendants=None, kids=[25680352], score=None, time=1610062292, title=None, item_type='comment', url=None, parent=25665666, text='&gt; I think the current consensus among experts is that the instruction set is not the limiting factor.<p>Yes and no.  Yes, because modern super scalar CPU&#x27;s don&#x27;t execute the instructions directly, but rather use a different instruction set entirely (the &quot;micro-ops&quot;) and effectively compile the native instructions into that.  This makes them free to choose whatever micro-ops they want.  Ergo the original instructions don&#x27;t matter.<p>But .... that means there is a compile step now.  For a while that was no biggie - it can pipelined if the encoding is complex.  But now the M1 has 12 (iirc) execution units. In the worst case that means they can execute 12 instructions simultaneously, so they must decode 12 instructions simultaneously.  The is a wee exaggeration as it isn&#x27;t that bad.  In reality the M1 appears to compile 8 instructions in parallel.<p>This is where the rot sets in for x86.  Every ARM64 instruction is 32 bits wide.  So the M1 grabs 8 32 bit words, compilers them in parallel to micro-ops.  Next cycle, grab another 8 32 words, compile them to micro-ops, and so on.  But the x86 instructions can start on any byte boundary, and can be 1 to 16 bytes in length.  You literally have to parse the instruction stream a byte at time before you can start decode it.  In practice they cheat a bit, making speculative guesses about where instructions might start and end, but when you&#x27;re being compared to someone who effortlessly processes 32 bytes at a time that&#x27;s like pissing in the wind.<p>So the instruction set may not matter, but how you encode that instruction set does matter, at lot.  Back in the day, when there we few caches and every instruction fetch cost memory accesses, you were better off using tricks like using one byte for the most common instructions to squeeze the size of instruction stream down.  That is the era x86 and amd64 hark from.  (Notably, the ill-fated Intel iAPX 32 took it to an extreme, having instructions start and end on a bit boundary.)  But now with execution units operating in parallel, and on chip caches putting instruction stores on chip right beside the CPU&#x27;s, you are better off making storage size worse in order to gain parallelism in decoding.  That&#x27;s where ARM64 harks from.<p>It&#x27;s interesting watch RISC-V grapple with this.  It&#x27;s a very clever instruction set encoding that scales naturally between different word sizes.  This also naturally leads to a very tight, compressed instruction set.  But in order to achieve that they&#x27;ve got more coupling between instructions than ARM64 (but far, far less than x86), and any coupling makes parallelism harder.  Currently RISC-V designs are all at the small, non-parallel end, so it doesn&#x27;t effect them at all.  In fact at the low power end it&#x27;s almost certainly a win for them.  But I get the distinct impression the consensus of opinion here on HN is it will prevent them from hitting the heights ARM64 and the M1 achieve.')