Item(by='shoo', descendants=None, kids=[24862960, 24862555], score=None, time=1603394288, title=None, item_type='comment', url=None, parent=24860688, text='In some applications, e.g. modelling something defined by PDE, you might need to solve a very large linear system of equations Ax=b, where the vector b has 100,000,000 elements, and A is a 100,000,000 by 100,000,000 element matrix. So for a naive dense matrix representation of A using 1 byte per entry you&#x27;d need around 9 petabytes of storage. In practise, for many interesting systems of equations that encode &quot;local&quot; properties, A will be a sparse matrix where the overwhelming majority of matrix entries are zero. There are various sparse matrix formats designed to avoid storing zeroes: the simplest is COO where you store triples of the form (i, j, A_ij) for all nonzero entries A_ij of A. More efficient formats are CSR or CSC ( compressed sparse row &amp; column respectively) that use even less memory by avoiding storing repeated indices where there are runs of nonzero elements.<p>To solve one of these huge linear systems Ax = b , we hardly ever want [1] to go to the expense of computing and storing an explicit matrix representation of the inverse A^{-1} . \n Instead, we merely need to have some function f that is able to map an input vector b to some output vector f(b)=x that happens to satisfy Ax=b .<p>If the matrix A has sufficiently nice properties [2] we can use an efficient iterative method such as the conjugate gradient method to map the input vector b to an output vector x by solving Ax=b. To evaluate the conjugate gradient method, we don&#x27;t need to necessarily have an explicit dense or sparse representation of the matrix A, we merely need a function that encodes how A acts on a vector by matrix multiplication. E.g. some function g that maps an input vector x to the output vector g(x) = Ax.<p>So if we&#x27;ve got some appropriately defined conjugate gradient method algorithm cg(g, b), we can define the function f(b) := cg(g, b)<p>Calling f(b) will return x that satisfies g(x) = b, i.e. f is the inverse of g. So now we&#x27;re expressing linear algebra without having any explicit matrices anywhere: but our functions g and f are of course both linear operators that encode the action of matrix multiplication by A and A^{-1} respectively.<p>[1] Why not? It&#x27;s a lot more effort to compute A^{-1} which could be used to evaluate A^{-1}b for any given b, but often we only want to know A^{-1}b for a specific b. Also, if the matrix A encodes some local property of a physical system , then A^{-1} will encode a global property -- so it will need to be a dense matrix.<p>[2] Sufficiently nice properties: if it&#x27;s symmetric positive definite, which arises naturally in a bunch of situations. See eg <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Conjugate_gradient_method" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Conjugate_gradient_method</a>')