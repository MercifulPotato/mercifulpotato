Item(by='benkuhn', descendants=None, kids=[25131842, 25132114, 25131595, 25131572, 25131681], score=None, time=1605656418, title=None, item_type='comment', url=None, parent=25122071, text='This is a shoddy analysis.<p>1. There&#x27;s a statistical gadget specifically for doing this—a &quot;scoring rule&quot; [1] which is a principled way to compare different probabilistic predictions. A bunch of scatterplots of random quantities against each other are... not that.<p>By comparing only binary win&#x2F;loss predictions instead of probabilities, like in the first chart, you throw away <i>almost all information</i> contained in the probabilistic estimates—if Democrats win a state, there&#x27;s no bonus for predicting (say) 95% Dem instead of 55% dem.<p>It&#x27;s plausible that 538 would actually win under a proper scoring rule, because betting markets were underconfident (relative to 538) in deep dem&#x2F;rep states (predicting e.g. &lt;95% Dem win in VT, vs 538&#x27;s &gt;99%). [2]<p>2. The calibration analysis assumes that different state win&#x2F;loss rates are independent, but that&#x27;s really untrue: 538&#x27;s predictions were specifically <i>not</i> independent because they assumed polling errors were correlated between states.<p>3. Many of the other scatterplots look outlier-driven and don&#x27;t include r^2 or p-values. With so few datapoints, it&#x27;s unclear if they are meaningful at all.<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Scoring_rule" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Scoring_rule</a><p>[2]: Maybe we should cut prediction markets some slack here because liquidity constraints make them inaccurate for small probabilities. If that&#x27;s the article&#x27;s position, though, they should address this instead of just... not using a scoring rule.')