Item(by='SubiculumCode', descendants=None, kids=[24695083, 24696676], score=None, time=1601962910, title=None, item_type='comment', url=None, parent=24694996, text='&quot;100x more computationally expensive&quot; but also, intrinsically more parallelizable than back prop. From the paper:<p>&quot;In terms of computational cost, one inference iteration in the predictive coding network is about as\ncostly as a backprop backwards pass. Thus, due to using 100-200 iterations for full convergence, our\nalgorithm is substantially more expensive than backprop which limits the scalability of our method.\nHowever, this serial cost is misleading when talking about highly parallel neural architectures. In the\nbrain, neurons cannot wait for a sequential forward and backward sweep. By phrasing our algorithm\nas a global descent, our algorithm is fully parallel across layers. There is no waiting and no phases\nto be coordinated. Each neuron need only respond to its local driving inputs and downwards error\nsignals. We believe that this local and parallelizable property of our algorithm may engender the\npossibility of substantially more efficient implementations on neuromorphic hardware&quot;')