Item(by='subjectsigma', descendants=None, kids=[25726235], score=None, time=1610340528, title=None, item_type='comment', url=None, parent=25717767, text='My biggest hope is that we will have better tools for introspection and generation of code. Others in the comments have already talked about machine learning, and yeah, I get it, machine learning is powerful and it would be cool if computers could write code for us. But I don&#x27;t think that&#x27;s going to happen. Rather, I hope it will look something like this:<p>* You start writing a piece of code. Halfway through the first function, your IDE realizes that a library already exists with a similar interface, and asks if you want to download it.<p>* You agree to download it, and after checking it meets your needs, start integrating it into your program. You do this by building a high-level model of what the code is supposed to do and then plugging it into the existing boilerplate. IE, the concept of &#x27;files&#x27; or &#x27;sockets&#x27; or &#x27;memory&#x27; isn&#x27;t necessary, you just define the algorithms on high-level, mathematical data structures, and all the messy details are figured out for you afterwards. If the computer isn&#x27;t 100% sure it knows what you&#x27;re talking about, it asks.<p>* After some tweaking, you commit the code and try to push to your VCS. The code is automatically linted and refactored for you to match the style guide and appropriate design patterns, but the system refuses to let you push because after several rounds of completely automated testing (static analysis, fuzzing, etc.) it was found that a small bug exists. You download the results in the form of an image that allows you to step through the entire environment at any level of abstraction, instruction-by-instruction if you want (but nobody ever does that), with all the state and data at any given time during testing available to you. You can scan forward and backwards through windy stack traces instantaneously, as easy as dragging a slider. You can &#x27;fork&#x27; the image by stopping at a certain point and modifying the internal data structures - this is, again, translatable to any level of abstraction. This makes it trivial to identify the problem and fix your code.<p>* You finally commit&#x2F;push and go grab a well-deserved beer. Your code gets deployed <i>somewhere</i> - you don&#x27;t know and you don&#x27;t really care, the system picks an appropriate set of cloud servers and handles the hot-swapping for you. A week or so later, you get an email saying that your new feature didn&#x27;t survive the mandatory A&#x2F;B testing - the system automatically but cautiously presented it to a small set of users, who discovered yet another bug. After the bug was tripped by an intelligent constraint solver, the change you made was instantly rolled back and the alert sent out.<p>* Again you download a system image and rewind to see what happened. Inside the guts of your program, you find a bunch of user data, with private details intelligently obfuscated. Finding the problem, you submit your code again and now everything is fine. After a few weeks, the code is rolled out to everyone and your version becomes the known good version, merged alongside the changes of your coworkers. Your last step is to write a description, in the form of constraints or some other high-level DSL, describing what you found, and submitting it to the analysis tools in your pipeline so they can discover similar problems in the future.<p>TL;DR Writing code should be around 95% reading or thinking, and 5% writing. All boilerplate is handled for you, bit-fiddling or manual performance optimizations are rare occurrences. State should be much easier to manipulate and the computer records all intermediate states during execution. IDEs should be armed to the teeth with analysis tools that catch problems.<p>Will this happen? Probably not. But it would be nice.')