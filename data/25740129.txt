Item(by='qayxc', descendants=None, kids=[25745973], score=None, time=1610409601, title=None, item_type='comment', url=None, parent=25737905, text='The fears and warnings of the AI safety guys are real and closer than you think.<p>Machine superintelligence is a bit on the extreme side, but formulating safety protocols for autonomous machines is a real challenge. We <i>do</i> know that optimisation functions can indeed create harmful and outright dangerous results.<p>It&#x27;s not an unknown that AI safety researchers fear, it&#x27;s actual outcomes or real experiments extrapolated into the future. Optimisation algorithms are impartial to humans,  their moral code and -survival.<p>Simple (and often quoted) example to illustrate this point: say we create an autonomous machine that we want to help us with our stamp collection. We run an optimising algorithm and naively use the amount of unique and rare stamps collected as our target function. The loss function is determined by time taken and money spent.<p>Possible outcomes: the algorithm figures that the most effective way of achieving this is to hack a number of bank accounts to quickly get enough money to buy a nice collection.<p>Now that&#x27;s bad, but maybe something we can incorporate in our training procedure: only use the money given to you.<p>Another possible outcome: the machine creates a set of robots that roam the planet and steal all the stamps from all over the world.<p>Again, you&#x27;d have to consider that in your training method: no stealing.<p>But in the end, you cannot foresee <i>every</i> possible outcome, especially since you <i>expect</i> the machine to come up with an unconventional solution, since otherwise you wouldn&#x27;t need it in the first place.<p>Restricting the space of possible solutions to safe and desired ones, is a very hard (and potentially undecidable) problem. This paper is just another reminder that we have to be very careful lest we accidentally end human civilisation by means of AI.')