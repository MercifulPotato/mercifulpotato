Item(by='jerf', descendants=None, kids=None, score=None, time=1603892454, title=None, item_type='comment', url=None, parent=24918122, text='&quot;By 7:50 AM, traffic hit the limit of the technology, around 100 page views per minute&quot;<p>(Tone note: Technical discussion about the modern era of development that just happens to be prompted by this article, not a criticism of the targeted site. I&#x27;ve written that sort of website myself enough times!)<p>Yeowch. That&#x27;s barely faster than a page view per second.<p>I have noticed in several sites (generally APIs rather than end-user sites but the same principles hold) I&#x27;ve built lately that as nice as databases can be, there&#x27;s a lot of places where things are coded to do a query per page view for things that just have no reason to be doing a query per page view. Even a &quot;no sql&quot; database can slow you down a lot vs. an in-process memory structure. I took one site from being able to serve a few hundred per second to tens of thousands per second by simply taking the relevant DB tables and slurping them wholesale into memory. Whenever someone makes a change to the underlying tables, a &quot;several times a week&quot; operation, I simply slurp the entire database tables in again from scratch. Slurping in the entire DB takes ~.25 seconds for all of its tens of thousands of rows on a low-end RDS and a low-end EC2 instance. Precomputing the answers to &quot;all the questions we saw last hour&quot; (as this is a service queried hourly by a lot of machines) takes another half a second or so. During the second this is happening it&#x27;s fine to serve stale results from the previous version of the memory contents.<p>My point is I see a lot of residual code and frameworks and habits from an era that come from an attitude of 5 megabytes being a <i>lot</i> of stuff, but it really isn&#x27;t anymore. Obviously you can&#x27;t do that to thousands of things without some issue, but almost every application has these little tables like a sidebar or the list of types of X or all kinds of other things where you&#x27;re better off just slurping the table into RAM and slurping the table into RAM again if there are any changes rather than constantly hitting a network database over and over again, because even if it&#x27;s a completely cached query it&#x27;s still vastly more load on your systems than a hash table lookup. (There is a bit of trickiness around making sure you detect changes, but one nice thing about &quot;just reload it all from scratch again&quot; is it&#x27;s feasible. &quot;Update just the changes&quot; always turns into a problem because of the way an error, once made, echos forever, but &quot;just reload it all from scratch&quot; is a feasible level of complexity.)<p>I also blame the &quot;shared-nothing&quot; architecture for hanging on longer than it needed to. It is OK to use the architectural patterns without <i>literally</i> throwing everything away on every web request. I think what I describe above can still just be considered a glorified DB cache if you do it correctly, which is fine to &quot;share&quot;. There&#x27;s a ton of websites like this in the world where every page load makes dozens or hundreds of DB queries that don&#x27;t change their results more than &quot;several times a day&quot; and as a result are very slow for no good reason.<p>(Many of these websites could also just run the queries every hour and serve the results with little to no loss in most cases. You want your &quot;published stories&quot; to update immediately, but you probably don&#x27;t need adding a site to the sidebar to be reflected instantly, etc.)')