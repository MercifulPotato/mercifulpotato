Item(by='FL33TW00D', descendants=None, kids=[24900598], score=None, time=1603732117, title=None, item_type='comment', url=None, parent=24895191, text='I&#x27;ve just recently been on a journey to understand transformers in and out, here are the resources I&#x27;ve found managed to drill it into my head:<p>1. Chapters 7, 9, 10<p><a href="https:&#x2F;&#x2F;web.stanford.edu&#x2F;~jurafsky&#x2F;slp3&#x2F;" rel="nofollow">https:&#x2F;&#x2F;web.stanford.edu&#x2F;~jurafsky&#x2F;slp3&#x2F;</a><p>This was really useful to really build up to the concepts of attention (although the actual attention section is still brief).<p>2. <a href="https:&#x2F;&#x2F;jalammar.github.io&#x2F;visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jalammar.github.io&#x2F;visualizing-neural-machine-transl...</a><p>This was great for visualization and understanding that attention wasn&#x27;t exclusively for transformers and actually was for RNNs first.<p>3. <a href="https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-transformer&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-transformer&#x2F;</a><p>Getting to understand how transformers actually work visually.<p>4. <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=S27pHKBEp30" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=S27pHKBEp30</a><p>This lecture by Leo Dirac was extremely helpful to finish off with, not only because it actually includes some pseudocode but it also revisits some key topics and covers why transformers are needed.<p>One of the big confusion points for me was that the concept of ATTENTION and SELF-ATTENTION are not the same thing.<p>Hope this helps.')