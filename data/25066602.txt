Item(by='ChuckMcM', descendants=None, kids=[25066726], score=None, time=1605157110, title=None, item_type='comment', url=None, parent=25066425, text='Many of the same pieces solving a slightly different problem. In the NetApp case it was pushing the edge of the de-duplication for efficient archival storage problem. EMC had discovered that MD5 hash collisions made deduping on a document level dangerous (you could think you had a document when you didn&#x27;t). Those collisions came from dissimilar sized documents and indeed you could &quot;attack&quot; MD5 signatures in that way. With a fixed document size, the probabilities went back to the actual MD5 collision probabilities. Those probabilities were acceptably small. On the &quot;fast&quot; part of the archival server, instead of storing 8K blocks you could store 16 byte &quot;block identifiers&quot; (but still using all of the standard WAFL file system layout, it thinks it is using 16 byte blocks. Those could be stored in &quot;fast&quot; storage (think SSD) and the actual data on slow &quot;cold&quot; storage. Your back-end server does do &quot;content addressable&quot; kinds of things in terms of hash-&gt;block location services but it is simple, only three operations &quot;does block &lt;x&gt; exist?&quot;, &quot;give me block &lt;x&gt;&quot; and &quot;store this block as &lt;x&gt;.&quot;<p>For a write mostly fabric attached archival device it had some benefits over the SATA based filers (higher density, lower watts&#x2F;terabyte, less CPU load on the filer head (spread out to the storage retrieval unit which could have many of the hash-&gt;block tranalators) etc. I don&#x27;t believe NetApp ever built a complete one though. Just &quot;too many degrees off their bow&quot; as an engineer I knew would say.')