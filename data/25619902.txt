Item(by='luizfelberti', descendants=None, kids=[25626328, 25620253], score=None, time=1609654021, title=None, item_type='comment', url=None, parent=25619348, text='&gt; This seems to be a reply to something, but it nos not clear to what? I saw no efficiency claims of trinary on the Tunguska page, so this seems unrelated to it<p>Sorry if it seems like it&#x27;s just a floating comment, indeed the article made claims of the kind, and I really think the project (and others like it) is nice. My comment is directed towards the practical concerns and real-world feasibility of a ternary computer, especially with regards to comments that threads like this inevitably attract (as seen on this comment section) that &quot;ternary computers would revolutionize X&quot;, and is partly fueled by me having had non-fuitful conversations with coworkers that insisted on this point in the past. Hopefully this helps contextualize my comment a bit better.<p>&gt; That said, I don&#x27;t understand how you arrived at [point 3b]. One can definitely create a circuit which will perform a ternary operation in a (some) constant time. The real question is: can you make a ternary computer which is somehow better than a binary one?<p>I think you and I are in agreement here, so let me try to clarify what I meant:<p>My point is <i>precisely</i> regarding the arbitrariness with which &quot;constant time&quot; is defined. Asymptotic analysis of algorithms always treats complexity in the &quot;time&quot; dimension as a measurement of discrete &quot;ticks&quot; on computational state, meaning: you can pick any encoding of data you want, and any set of operations you want, and have any &quot;operation&quot; execute instantly as a tick. When analysing &quot;ticks&quot;, an algorithm might seem extremely efficient, however that might be a non-reifiable performance gain when you translate from ticks to milliseconds.<p>For example: assume a binary NAND gate takes 1ms to propagate a stable output signal. <i>All other conditions being equal</i> with respect to manufacturing technology, voltage, and other things you mentioned, I don&#x27;t see how you could perform &quot;more&quot; computation at the same cost. Sure you can implement a ternary gate that runs in &quot;constant time&quot;, but would that constant time still be 1ms? If yes, would it take up the same amount of die space? If yes, would it consume the same amount of power and dissipate the same amount of heat? etc<p>You&#x27;ll eventually be tickling the Landauer limit [0] for whatever your manufacturing technology is at some point, no matter if you&#x27;re doing binary or whatevernary logic, but getting &quot;more compute&quot; for free would be cheating thermodynamics. Improving manufacturing technology is, of course, a viable strategy for squeezing &quot;more computation&quot; into the same physical space, but switching bases will not immediately make Moore&#x27;s law wither away.<p>A more succinct way of putting what I&#x27;m trying to say into words I guess would be that &quot;all other things being equal, the choice of base is arbitrary with regards to computational and representational power&quot;<p>Hopefully this helps clear things up :)<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Landauer%27s_principle" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Landauer%27s_principle</a>')