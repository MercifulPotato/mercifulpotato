Item(by='Jugurtha', descendants=None, kids=[25163197], score=None, time=1605877647, title=None, item_type='comment', url=None, parent=25151273, text='Do you not think that using spreadsheets for ML experiment tracking is a symptom of broken tooling? I&#x27;m asking because one of the reasons we&#x27;re building our platform[0] with automatic experiment tracking, collaborative notebooks, and a bunch of things, is because experiment tracking was inconsistent between team members.<p>Differences in tools used: (spreadsheets, flat files, logs, pen and paper, human memory). Forgetting to do it. Snippets to do it flying around. Different locations (laptop, group workstation, git repository, cloud sheet). Dissociated from the notebook that produced the model.<p>Tighter tracking should answer questions like: what notebook ran on which data and produced which model with which parameters and which scores? Then questions like: give me all notebooks that ran on this dataset which produced a model with scores that are [condition].<p>Once you do that, the &quot;spreadsheet&quot; can just be a &quot;view&quot; of the underlying data. Something you can export as, but not the thing itself.<p>I think it&#x27;s good there are tools with this granularity that can be composed.<p>- [0]: <a href="https:&#x2F;&#x2F;iko.ai" rel="nofollow">https:&#x2F;&#x2F;iko.ai</a>')