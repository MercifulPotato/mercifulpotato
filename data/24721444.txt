Item(by='sillysaurusx', descendants=None, kids=[24722376], score=None, time=1602177835, title=None, item_type='comment', url=None, parent=24721397, text='Not only is it misleading, it even somehow tricked you. :)<p>We’re not talking about a small 10% reduction in performance here. We’re talking like 40x differences.<p>If it seems unbelievable, and like it can’t possibly be true, well: now you understand my frustration here, and why I’m trying to break the myth.<p>Notice not a single benchmark has ever gone head to head in MLPerf using pytorch on TPUs. And that’s because using pytorch on TPUs requires you to feed <i>each image manually</i> to the TPU on demand, <i>from your VM</i>. Meaning the TPU is always infeed bound.<p>Engineers should be wincing at the sound of that. Especially anyone with graphics experience. Being infeed bound means you have lots of horsepower sitting around doing nothing. And that’s exactly the situation you’ll end up in with this technique.<p>There’s a way to settle this decisively: train a resnet classifier on imagenet, as quickly as possible. If you get anywhere <i>near</i> the MLPerf v0.6 benchmarks for tensorflow on TPUs, I will instantly pivot the other direction and sing the praises of pytorch on TPUs far and wide.')