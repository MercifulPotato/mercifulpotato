Item(by='Someone', descendants=None, kids=[25952775], score=None, time=1611869152, title=None, item_type='comment', url=None, parent=25947901, text='Title of this HN post is correct in the sense that it’s the title of the referenced article, but that title is wrong for the article’s content.<p>The article claims computer science hasn’t moved forward (much) since 1978, claiming that the changes in the 40 years between 1940 and 1980 were much larger than those between the next 40 years (1980 and 2020)<p>I think a simple model could explain that. Let’s say computing went from ‘1’ to ‘41’ in those first forty years, an improvement of about a factor of 40. Keeping the same speed, it would have gone from ‘41’ to ‘81’ in the second 40 years, an improvement of about a factor of 2. That’s an order of magnitude less than that factor of 40.<p>You can change those numbers to get any difference you want. Point is that, if progress is more or less linear and improvements are measured as relative size differences, improvement rate always will slow down over time.<p>Now, whether that model or something like it holds, I’m not sure about, but I think it might hold for many new fields (what happened after the invention of the alphabet, for example? A few thousand new glyphs, emoji, etc. were added, but there’s not much really new there)')