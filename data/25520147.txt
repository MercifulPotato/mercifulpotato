Item(by='chriswarbo', descendants=None, kids=[25521423, 25522326, 25524543], score=None, time=1608746030, title=None, item_type='comment', url=None, parent=25518651, text='I think this is partly Stockholm syndrome. Having a finite stack will cut off infinite recursion before it consumes the whole memory, and infinite recursion is often an error (e.g. if we get some branching logic wrong, our base case may never be taken). Over time this has become self-reinforcing:<p>- Recursive algorithms may hit stack limits, which makes them less attractive, so programmers are less likely to choose them<p>- Error rates presumably remain pretty constant, so this avoidance decreases the proportion of recursive programs that are deliberately so, and increases the proportion of recursive programs that are accidentally so<p>- Stack limits remain justified, due to how rarely programmers are choosing recursive approaches, and how often stack overflows help us to stop buggy programs<p>I don&#x27;t think the &#x27;stops buggy programs&#x27; argument is particularly compelling, since that could apply to pretty much any restriction (e.g. we could limit RAM usage to 64K to prevent buggy programms allocating too much, those which want more can edit files in a tempfs; processes should run for no longer than 1 minute, to stop buggy programs, those which need more time can spawn children; etc.).<p>I appreciate that low-level languages are operating under some pretty tight constraints (both in what&#x27;s available to them, and the performance their users expect). What I find very strange is how many high-level scripting languages impose such stack limits, and don&#x27;t even eliminate tail-calls. The raison d&#x27;etre for such languages is to be more ergonomic and powerful than bare-metal languages, and they don&#x27;t mind sacrificing <i>a lot</i> of performance to do so (using interpreters, boxed values, dynamic dispatch, etc.); yet they&#x27;re somehow fine with inheriting these weird restrictions from C &amp; co.?')