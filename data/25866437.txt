Item(by='1MachineElf', descendants=None, kids=[25868775, 25871443, 25870788, 25867955, 25866875, 25867198], score=None, time=1611273616, title=None, item_type='comment', url=None, parent=25861422, text='I&#x27;m thankful for their OpenZFS tuning doc which they developed as part of this server migration: <a href="https:&#x2F;&#x2F;github.com&#x2F;letsencrypt&#x2F;openzfs-nvme-databases" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;letsencrypt&#x2F;openzfs-nvme-databases</a><p>The one thing that I get hung up on when it comes to RAID and SSDs is the wear pattern vs. HDDs. Take for example this quote from the README.md:<p><i>We use RAID-1+0, in order to achieve the best possible performance without being vulnerable to a single-drive failure.</i><p>Failure on SSDs is predictable and usually expressed with Terabytes Written (TBW). Failure on spinning disk HDDs is comparatively random. In my mind, it makes sense to mirror SSD-based vdevs <i>only</i> for performance reasons and <i>not</i> for data integrity. The reason is that the mirrors are expected to fail after the same amount of TBW, and thus the availability&#x2F;redundancy guarantee of mirroring is relatively unreliable.<p>Maybe someone with more experience in this area can change my mind, but if it were up to me, I would have configured the mirror drives as spares, and relied on a local HDD-based zpool for quick backup&#x2F;restore capability. I imagine that would be a better solution, although it probably wouldn&#x27;t have fit into tryingq&#x27;s ideal 2U space.')