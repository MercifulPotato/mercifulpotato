Item(by='hajile', descendants=None, kids=[25132408], score=None, time=1605647224, title=None, item_type='comment', url=None, parent=25126740, text='I&#x27;d note that both arguments have merit.<p>A SIMD is basically controller + ALUs. A wider SIMD gives a better calculation to controller ratio. Fewer instructions decreases pressure on the entire front-end (decoder, caches, reordering complexity, etc). This is more efficient overall <i>if fully utilized</i>.<p>The downsides are that wide units can affect core clockspeeds (slowing down non-SIMD code too), programmers must optimize their code to use wider and wider units, and some code simply can&#x27;t use execution units wider than a certain amount.<p>Since x86 wants to decrease decode at all costs (it&#x27;s very expensive), this approach makes a lot of sense to push for. If you&#x27;re doing math on large matrices, then the extra efficiency will make a lot of sense (this is why AVX512 was basically left to workstation and HPC chips).<p>Apple&#x27;s approach gambles that they can overcome the inefficiencies with higher utilization. Their decode penalty isn&#x27;t as high which is the key to their strategy. They have literally twice the decode width of x86 (8-wide vs 4-wide -- things get murky with x86 combined instructions, but I believe those are somewhat less common today).<p>In that same matrix code, they&#x27;ll have (theoretically) 4x as many instructions for the same work as AVX512 (2x vs AVX2, so we&#x27;d expect to see the x86 approach pay off here. In more typical consumer applications, code is more likely to use intermittent vectors of short width. If the full x86 SIMD can&#x27;t be used, then the rest is just transistors and power wasted (a very likely reason why AMD still hasn&#x27;t gone wider than AVX2).<p>To keep peak utilization, M1 has a massive instruction window (a bit less than 2x the size as Intel and close to 3x the size of AMD at present). This allows it to look far ahead for SIMD instructions to execute and should help offset the difference in the total number of instructions in SIMD-heavy code too.<p>Now, there&#x27;s a caveat here with SVE. Scalable vector extensions allow the programmer to give a single instruction along with the execution width. The implementation will then have the choice of using a smaller SIMD and executing a lot or a wider SIMD and executing fewer cycles. The M1 has 4 floating point SIMD units that are supposedly identical (except that one has some extra hardware for things like division). They could be allowing these units to gang together into one big SIMD if the vector is wide enough to require it. This is quite a bit closer to the best of both worlds (still have multiple controllers, but lose all the extra instruction pressure).')