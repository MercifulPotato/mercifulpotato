Item(by='core-questions', descendants=None, kids=None, score=None, time=1610913247, title=None, item_type='comment', url=None, parent=25814105, text='&gt; Saying browsers won&#x27;t support JS in 10 years is an idiotic claim.<p>I think his reductionist take on it has resulted in you missing his point. Lemme take a shot at expanding on that for you.<p>1. Yes, browser will run JS in 10 years. However, in 10 years, the market will have changed. Firefox might be dead, and everything might standardize on a Chromium-derived monoculture. Backwards compatibility will be kept to a modest extent but we may see a new &#x27;quirks mode&#x27; shift in the ecosystem or other attempts to carve off backwards compatibility.<p>Look at how Apple uses this to their advantage to have internal agility, where MS uses the opposite approach and sacrifices it for stability. The former can eat the latter&#x27;s lunch when it comes to technology on the cutting edge, even if the developer ecosystem and end user pays the price in subtle ways. If Google follows this lead with their monopolistic control of Chromium, we could easily have a world in 10 years where JS on a lot of sites that use it deeply, exploring the limits of the APIs, sites that already experience cross-browser compat issues, etc. will be slightly broken.<p>The problem may only get worse over time; after all, we already have many use cases within IT to run a VM of Windows XP and IE 6 to access older devices that simply don&#x27;t work any more with anything newer. If they&#x27;d been designed with simple, server-side frameworks and vanilla Web 1.0 stuff they would have aged a lot better. Isn&#x27;t it possible that we&#x27;re on the wrong evolutionary path here if that&#x27;s the case?<p>Think also of &quot;applications&quot; written in Node that you might want to run on your own server - it might not be possible to build them anymore, the dependencies might have dried up, making them difficult to rehydrate without doing extensive surgery.<p>2. Archival is another layer of the problem. If your single-page app doesn&#x27;t present some mechanism where a scraper can meaningfully put in a URL and get back some amount of HTML reflecting the desired article &#x2F; content &#x2F; etc, it may not end up in the Wayback Machine or other archive sites, which are absolutely critical civilizational infrastructure if we want to have any hope at giving future historians anything even approaching a balanced and truthful perspective on this pivotal age.<p>This is not an overstatement, and it&#x27;s something people with all political beliefs should be able to agree with for their own reasons. Things that are published need to be retained, in ways that give us confidence their contents haven&#x27;t been altered. If your site requires I execute a bunch of Javascript just to be able to see the content, then that is not guaranteed; even with a headless browser rendering the output, it&#x27;s hard to know that what comes out the other end is actually a useful copy or not.<p>Face it: Javascript is a third-tier language that occupies its position in the landscape due to first-mover advantage and inertia. Javascript frameworks paper over the fact that browsers weren&#x27;t designed as application platforms by inserting gobs of difficult-to-understand code between developers and the actual underlying system. GUI things that worked trivially 20+ years ago are difficult and end up being omitted from most systems (you know, things like sortable tables, keyboard-accessible forms with complex controls, drag-and-drop, and other 90s gold). It&#x27;s a big pile of trash.')