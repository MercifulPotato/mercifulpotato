Item(by='cs702', descendants=None, kids=[25325565], score=None, time=1607270829, title=None, item_type='comment', url=None, parent=25314830, text='&quot;Here we show that every model learned by this method [SGD], regardless of architecture, is approximately equivalent to a kernel machine [i.e., a support vector machine or SVM] with a particular type of kernel&quot; -- a type of kernel which Domingos, the author, calls a &quot;path kernel.&quot;<p>As defined in the paper, a &quot;path kernel&quot; measures, for any two data points, how similarly a model varies (specifically, how similarly the model&#x27;s gradients change) at those two data points during training via SGD. This isn&#x27;t exactly your usual, plain-vanilla, radial-basis type of kernel.<p>We&#x27;ve known for a long time that SVMs are universal approximators, i.e., in theory they can approximate any target function. The importance of this work is that it has found a new, surprising, deep connection between <i>any</i> model trained via SGD and SVMs, which are well understood :-)')