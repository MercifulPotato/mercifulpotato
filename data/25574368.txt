Item(by='kajecounterhack', descendants=None, kids=[25575737], score=None, time=1609273210, title=None, item_type='comment', url=None, parent=25573704, text='&gt; Tesla’s approach of vision based, AP disengagement-led training will scale better ... [about weather] they’re special cases for other companies (from what’s been revealed; Comma might be an exception).<p>&gt; all it takes for Tesla’s NN approach to pay off is one leap in explainability.<p>It&#x27;s not vision and disengagement-led training that&#x27;s conferring the scale advantages you&#x27;re talking about, because other AV companies actually do train on and attempt to drive in various kinds of weather, and safety drivers do the same thing normal drivers do except that they&#x27;re more trained. The difference is the approach Tesla has to safety (vs the others). Which is to say: Tesla doesn&#x27;t worry about training their supervising drivers, at the risk of those folks&#x27; lives.<p>Tesla also doesn&#x27;t seem to worry about _proving_ the safety of their systems: explainability isn&#x27;t simply going to be a matter of a new algorithmic breakthrough. How do you prove that your software is safe if you don&#x27;t sample heavily from different operating domains you want your car to work in, and then either simulate on those miles or actually test your car? Before letting a driver behind the wheel of an autopilot, companies like Cruise&#x2F;Waymo&#x2F;Aurora all do manual driving first in a new domain, and then move to safety drivers (even 2 to start, for extra safety). This is not because their software is incapable of doing the same things as Tesla, but because it&#x27;s very worthwhile to sample disengage rates and assess safety criticality to various issues before allowing the car to act autonomously. And even then there are safety nets in place to further mitigate risk. (You&#x27;re probably right that L5 is a long ways off, but this is how we&#x27;ll get to L4.)<p>Of course it&#x27;s more &quot;scalable&quot; to avoid all this and put the responsibility of risk on users who, as your system improves, become less and less attentive. And so this is what Tesla does. I suspect solving the long tail of issues is going to be more of a challenge for Tesla because at least in my experience it&#x27;s easier to {convert a naive rules&#x2F;simple model --&gt; complex model} than to {debug the complex model --&gt; handle failure modes with logic}.')