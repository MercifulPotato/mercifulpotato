Item(by='foota', descendants=None, kids=None, score=None, time=1607860387, title=None, item_type='comment', url=None, parent=25404976, text='To be clear, I wasn&#x27;t trying to make a moral statement on whether they should do this, I just think it&#x27;s interesting.<p>I think that forcing companies to recognize and deal with vulnerabilities is a good thing, so to the degree this kind of setup would do that it wouldn&#x27;t be all bad, but trying to extract additional gains beyond that exposure isn&#x27;t good (e.g., companies would pay more to prevent bad PR from a threat to expose something than just to fix a vulnerability due to the risk it presents them, and the former is &#x27;artificial&#x27; in this case so it wouldn&#x27;t be efficient for a group to try to extract that from someone).<p>That is, today companies have some existential risk that a cyber security incident causes great harm to them (think: sony hacks, cambridge analytica), the existence of white hats researching these vulnerabilities and disclosing them responsible gives companies an avenue to address this risk, likely at a lower cost premium relative to hiring security teams to try and find them. I think that it&#x27;s easier for companies to recognize and deal with these risks now than it used to be, and easier for security researchers to get paid for it. These are both good things, but it is quite possible that the risk posed by cyber security threats to companies is generally worth more than they&#x27;re paying in aggregate (2 million in vuln fees quotes in their latest report, not much at all given the impact), so I think that some structure that would allow researchers to force companies to up the ante would be a good thing, but this is hard since it&#x27;s a completely one sided market and companies can just accept the risk of an incident occurring rather than pay, even if it&#x27;s inefficient.')