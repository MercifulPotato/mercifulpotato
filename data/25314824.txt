Item(by='mlthoughts2018', descendants=None, kids=None, score=None, time=1607179156, title=None, item_type='comment', url=None, parent=25311402, text='The summarized sections of the paper seem very, very weak to me.<p>1. Environmental footprint of technology must always be considered as a trade-off for what you get in return. Why do we spend energy on a giant render farm for Pixar? Because the cinematic artwork is worth that environmental cost for many people. Obviously we should pursue improvements in environmental outcomes, but that is not a goal unto itself in a vacuum apart from all other side effects of a technology. Is it worth ~5 car-lifetimes to train GPT? I would say overwhelmingly yes. It reminds me of an anecdote from The Beginning of Infinity by David Deutsch, where some ethicists argued about whether it was a useful human endeavor to invent color TV monitors back when they first hit the market. Why would you need to spend resources creating something besides black and white TV? Yet today and for decades, color monitors are used as critical tools to diagnose diseases and save lives.<p>2. Nobody is required to accept “wokeness” vocabulary, and indeed one of the signs of a crank or a quack is making up a fiefdom of new vocabulary and collecting rent in the form of social authority for the validity and requirement of that new vocab. Nobody is required to be on the cutting edge of how activist language changes, and it seems like a disingenuous way to try to make a cottage industry out of one’s own expertise in rapid changes to activist language. As long researchers are stating what corpus is used, and making tools available that allow peer reviewers to audit that corpus, they are meeting their obligations to their professional field and to society. We should be <i>happy</i> that language researchers would produce lots of papers and lots of models on many sets of corpora, and as the cost of training these models gets cheaper, and the cost of curating the corpora gets cheaper, we can expect to see better variety of curated large corpora, domain-specific corpora and other things.<p>3. Researcher opportunity cost is perhaps the most ridiculous objection. Researchers are free agents to decide what they want to study. In most PhD programs, especially in machine learning, the project selection is entirely up to the student. If Timnit wants there to be different research priorities, well, news flash, but she is only one of eight billion. Unless she wants to raise money to give as research grants that tie the researchers to her pre-decided methods of inquiry, she really has nothing to say here.<p>4. Inscrutable models - this is the only one where there is any point. <i>If</i> the models can produce harmful outcomes and they are inscrutable, then debugging or safeguarding them is a real problem. But this has been true for almost all types of computer science algorithms. Of course we should work on methods that synthesize clarity from the prediction mechanism of large neural nets, but that is also <i>not</i> a criticism of neural nets. That’s just a need for <i>more technology.</i><p>Overall the main points of this paper seem full of themselves, arrogant and overly self-important, especially with wildly ridiculous connections to “wokeness” vocabulary.<p>Given the immediate nuclear option of the ultimatum and email that Timnit sent, I suspect the full text of the paper would be even more unacceptable.<p>Google has plenty of bullshit issues with the way it treats employees and transparency of decision making. Rejecting this publication approval was not one of them.')