Item(by='blululu', descendants=None, kids=[25250768, 25250745, 25250928, 25251360, 25250918, 25250725, 25256535, 25251027, 25250913, 25253255, 25250802, 25252362, 25253500, 25250741, 25251072, 25256133, 25255450], score=None, time=1606709370, title=None, item_type='comment', url=None, parent=25250455, text='This is a good exposition of some formal definitions for &#x27;interpretability&#x27; in the context of machine learning, but I am still not really clear on why such a property is necessary or even desirable in the context of high dimensional statistical learning algorithms. In some sense the power of modern machine learning (as opposed to a set of heuristics + feature engineering + a linear classifier) is that it is not limited by what its designers are able to imagine or understand. If it were possible to give a simple explanation of how a high dimensional classifier works then it would also likely be unnecessary to have so many parameters.<p>As an example, if we consider natural language processing, then we might say that we want our NLP algorithm to be interpretable. This is clearly a tall order since the study of linguistics is still full of unsolved riddles. It seems silly to insist that a computational model of language must be significantly easier to understand than language itself. If interpretability is not feasible with language - a construct that is intimately connecting to the faculties of the human brain - then why should we expect it to be feasible (or desirable for the wide range of applications that do not come naturally to people?')