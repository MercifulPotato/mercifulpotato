Item(by='move-on-by', descendants=None, kids=None, score=None, time=1602037010, title=None, item_type='comment', url=None, parent=24699534, text='I enjoyed the read, but it glossed over some really important details:<p>&gt; Failovers took more than 20 minutes  to complete and would often get stuck requiring manual intervention. Messages were often lost in the process as well.<p>Data loss while RabbitMQ is down is certainly a problem.<p>&gt; The Kafka-consumer process is responsible for fetching messages from Kafka, and placing them on a local queue that is read by the task-execution processes.<p>This is a bit confusing, so now instead of a centralized outage, grinding everything to a halt, the risk is distributed data loss as local queues go down? As long as they stay down, it should be limited data loss to the size of the queues times the number of consumers, but if an instance is flagging it could eat through a whole lot of messages before someone notices the problem. I guess itâ€™s really dependent on the types of messages being processed and if they are idempotent enough to be replayed without consequence.')