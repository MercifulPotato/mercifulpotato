Item(by='sillysaurusx', descendants=None, kids=None, score=None, time=1611964231, title=None, item_type='comment', url=None, parent=25963556, text='I noticed something about Adam. Suppose you want to do gradient accumulation. Easy: compute the gradients of the loss with respect to each model parameter, and accumulate it over N training examples. Then pass the result into Adam, performing a single step. This is standard gradient accumulation; it&#x27;s equivalent to running a mega-GPU that can process 64 training examples at once, rather than a small-GPU that can only process 8. It just takes longer to train, since you&#x27;re performing an adam update every 8 steps in that case.<p>But, I was staring at the Adam formula and thought of something. I&#x27;m not sure if it makes sense, but there seems to be an &quot;alternate&quot; way to accumulate gradients:<p>For each training example, compute the gradients <i>and apply the gradients</i>. However, we apply them in a special way. The final step of adam normally looks like this:<p><pre><code>  param = param - (lr * m_t &#x2F; (v_sqrt + epsilon_t))\n</code></pre>\nI propose accumulating the gradients like this:<p><pre><code>  accum = accum + (lr * m_t &#x2F; (v_sqrt + epsilon_t))\n</code></pre>\nThen after N training samples, when you want to do the actual variable update:<p><pre><code>  param = param - accum\n  accum = 0\n</code></pre>\nThe advantage of this approach (if it works at all) is that Adam updates continuously. Every training example would cause Adam&#x27;s mean and variance estimators to update. (Recall that the whole point of Adam is that it tracks mean and variance <i>for every parameter</i> in the entire model.)<p>So, in traditional gradient accumulation, those mean and variance slots would only update every N training examples. With this approach, they would update every training example, and then the model params update every N training examples.<p>It might seem like a small tweak, but adam&#x27;s variance stats are crucial; it&#x27;s what makes adam effective. Updating the variance 8x more frequently might be an advantage.')