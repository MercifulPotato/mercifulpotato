Item(by='beagle3', descendants=None, kids=None, score=None, time=1604775325, title=None, item_type='comment', url=None, parent=25017301, text='Indeed, this is all true. But do remember that the kolmogorov-arnold theorem says a 3-layer (n:2<i>n+1:m) network is a universal continuous approximator (using an unknown neuron transfer function) -- people in the &#x27;80s were looking at 3 layer networks as sufficient partly because of that.<p>I have no time to go look at all those sources now, but having dabbled in nets since the late &#x27;80s myself, I remember vanishing gradients were sort-of a surprise, because everyone was under the impression that simple backpropagation should just work, and it didn&#x27;t.<p>A lot of that early work you refer to was also mostly about linear transfer functions, and though the exact type of non-linearity doesn&#x27;t matter, some of its properties do - and as I mentioned, sigmoids - which were all the rage in the &#x27;80s - are a dead end with the wrong kind of nonlinearity.<p>Nothing about the </i>structure* of multilayer models is new. But successfully <i>training</i> them - which didn&#x27;t happen until Schmidhuber and Hinton (depends on who you ask ...) - is relatively new; and that advance is responsible for the term &quot;deep learning&quot;.<p>We do not disagree about the details; but we do seem to disagree about the historical context and narrative.')