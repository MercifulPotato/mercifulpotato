Item(by='ianhowson', descendants=None, kids=None, score=None, time=1603819816, title=None, item_type='comment', url=None, parent=24908717, text='&gt; if you&#x27;re implementing some algorithm directly at the gate level for cryptography or signal processing or whatever then being able to arrange inputs outputs into dataflows is a big win with no roundrips to general purpose registers or bypass networks<p>This is true, but keep in mind that that sort of algorithm runs <i>insanely</i> well on any CPU or GPU because they, too, do not want to touch main memory. You would be blown away by how much work a CPU can do if you can keep the working set within L1 cache.<p>Re. ASICs, it&#x27;s a continuum:<p>- &quot;flexible, low performance, cheap in small quantities&quot; (CPUs)<p>- &quot;reasonably flexible, better performance, cheap-ish in small quantities&quot; (GPUs)<p>- &quot;inflexible, best performance, expensive in small quantities&quot; (ASICs)<p>FPGAs fit somewhere between GPUs and ASICs -- poor flexibility, maybe great performance, moderate small-quantity price.<p>If your problem is too big for GPUs, as you say, sometimes it&#x27;s easiest to jump straight to an ASIC. But it&#x27;s such a narrow window in the HPC landscape. The vast majority of customers, even with large problems, are just buying a lot of GPUs. They&#x27;re using off-the-shelf frameworks even though a custom CUDA kernel would give them 10x performance and 10% cost. The cost to go to an FPGA is too great and the performance gain simply isn&#x27;t there.')