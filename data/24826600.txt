Item(by='deeplstm', descendants=None, kids=None, score=None, time=1603117280, title=None, item_type='comment', url=None, parent=24826599, text='This video explains a legendary paper, BERT. It leverages the Transformer encoder and comes up with an innovative way to pre-training language models (masked language modeling). BERT has a significant influence on how people approach NLP problems and inspires a lot of following studies and BERT variants.<p>Code\n<a href="https:&#x2F;&#x2F;github.com&#x2F;google-research&#x2F;bert" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;google-research&#x2F;bert</a> (TensorFlow)\n<a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers</a> (PyTorch)')