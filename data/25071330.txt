Item(by='abeppu', descendants=None, kids=[25071539, 25071400, 25071472, 25071500], score=None, time=1605197538, title=None, item_type='comment', url=None, parent=25070224, text='I guess, to re-hash the same conversations people have been having for years -- if the expectation is that drivers must be alert and prepared to intervene, that places them in the gray area of not knowing whether or when to trust the system. Even if the self-driving system is relatively good, is the human+self-driving system really better?<p>In some sense, the self-driving system gets to be trained with experiences of failure, probably under the expectation that no intervention will occur (i.e. the &quot;policy&quot; cannot include human intervention). But the human driver doesn&#x27;t get to learn from experience when to not trust the self-driving system (because getting it wrong can be disastrous).<p>Perhaps a missing piece that human drivers _should_ participate in, but almost certainly won&#x27;t, is training in a simulator. There&#x27;s a methodological challenge here -- ideally the simulator training should focus on scenarios where the self-driving system is weakest (i.e. where it&#x27;s plausible that the driver will need to intervene), but those are likely also cases where we&#x27;re worst at simulating the behavior of other cars&#x2F;pedestrians&#x2F;cyclists&#x2F;loose object.')