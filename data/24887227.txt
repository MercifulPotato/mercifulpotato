Item(by='mdifrgechd', descendants=None, kids=None, score=None, time=1603641417, title=None, item_type='comment', url=None, parent=24887170, text='One possible counterpoint- sample based explanation techniques can tell what training data was most influential in a ML model&#x27;s prediction, and this has been considered in language models[0]. So you could argue that if there are training examples within a corpus that are having an outsized influence on the model output, then maybe it is derivative. This would be pretty cool to look at - are some GPT or other language model outputs relying strongly on a few sources?<p>[0] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1810.03611" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1810.03611</a>')