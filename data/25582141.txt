Item(by='hprotagonist', descendants=None, kids=[25584516], score=None, time=1609339873, title=None, item_type='comment', url=None, parent=25582122, text='yes, agency is also a real pain to define. what does it mean for a thing to “want” to do something?<p><a href="https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;EWD&#x2F;transcriptions&#x2F;EWD09xx&#x2F;EWD936.html" rel="nofollow">https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;EWD&#x2F;transcriptions&#x2F;EWD09xx&#x2F;E...</a> is also an ever-present problem: when anthroomorphic language is appropriate and when it is not is something of an open question, but the prior should be towards avoiding it. Possibly particularly in systems we’ve built.<p>You don’t have to go full-on radical behaviorist to be wary of this trend, (and tangentially i think behaviorism isn’t the be all and end all of explanations by a long shot) but once you’re aware of it you see it cropping up <i>everywhere</i>. Viruses don’t “want” to infect you, protein binding sites don’t “want” to bind amino acids, REST endpoints don’t “expect” a json datagram, GPT-3 doesn’t “know” anything and hasn’t “learned” anything, and so on. But we regularly speak of them as if that were the case.  What category errors are we missing when we do this that could open new doors of understanding?')