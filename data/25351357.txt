Item(by='lacker', descendants=None, kids=[25351577, 25351612, 25351443], score=None, time=1607461655, title=None, item_type='comment', url=None, parent=25346456, text='Attention Is All You Need<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1706.03762" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1706.03762</a><p>It&#x27;s from 2017 but I first read it this year. This is the paper that defined the &quot;transformer&quot; architecture for deep neural nets. Over the past few years, transformers have become a more and more common architecture, most notably with GPT-3 but also in other domains besides text generation. The fundamental principle behind the transformer is that it can detect patterns among an O(n) input size without requiring an O(n^2) size neural net.<p>If you are interested in GPT-3 and want to read something beyond the GPT-3 paper itself, I think this is the best paper to read to get an understanding of this transformer architecture.')