Item(by='millstone', descendants=None, kids=[24755514, 24758151, 24754947], score=None, time=1602491355, title=None, item_type='comment', url=None, parent=24751540, text='I think the author undersold the investigation they did. They understood the problem rather convincingly. But the blog post as written is misguided:<p>&gt; I never investigated why the test had suddenly started failing but I suspect that the timer frequency or the timer start point had changed,<p>Do not act on suspicion alone.<p>Sometimes sporadic test failures are problems in the test framework. Suspicion + evidence becomes a hypothesis. Find a way to reproduce the failure, that leads you to understand why the test usually passes, then you can fix it with confidence.<p>Other times sporadic test failures point to problems in deeper layers. Maybe some errant signal handler was borking the FP rounding mode. Maybe there&#x27;s real hardware errata. We may never know.<p>&gt; It bothered me that it required esoteric floating-point knowledge to understand this problem so I wanted to fix googletest<p>This is a different problem. The test is failing, and also Google Test is written in a way that requires esoteric FP knowledge. Fix the first, then refactor to fix the second. These should absolutely be two independent commits. I think this opportunity was missed.<p>&gt; I think that the googletest fix is ultimately more important than fixing the Chromium test<p>This is a false dichotomy. You can fix both!<p>If you increase the error bounds for some test, the failure no longer repros. That may or may not be the correct fix, but you have to be able to justify it.<p>In this case, one could mock the timer and search for times that would repro the failure. A nice FP trick is avoiding i386: its 80-bit fpu masks lots of other problems.')