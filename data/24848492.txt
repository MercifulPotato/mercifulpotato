Item(by='kentonv', descendants=None, kids=None, score=None, time=1603292326, title=None, item_type='comment', url=None, parent=24844398, text='&gt; If you malloc a big buffer, almost certainly page aligned anyway.<p>Not necessarily. Remember that malloc() must produce a pointer which can later be passed to free() without any other details. free() needs some way to figure out, at the very least, how big the allocation is, to properly free it. Many allocators, such as glibc&#x27;s, accomplish this by placing metadata in the bytes immediately before the allocated block -- which implies that a large allocation won&#x27;t be page-aligned.<p>Some other allocators, such as tcmalloc, manage to place the metadata somewhere else, and in that case maybe they return page-aligned buffers, but it&#x27;s hardly safe to assume.<p>(I just ran a quick test and verified -- a 4MB malloc() under glibc is typically 16 bytes off of the page boundary, but under tcmalloc it is aligned.)<p>&gt; 2. You can try the same with mmap, but large files won&#x27;t fit in (32 bit) memory, and read() is cheaper than mapping and unmapping as you go.<p>This thread is discussing the claim that the Linux kernel transparently &quot;optimizes&quot; large read() calls into mmap(), with me arguing that doesn&#x27;t make a lot of sense as an optimization.<p>If you&#x27;re saying that this &quot;optimization&quot; would actually be slower, then that supports my point.<p>You may be right. munmap() (or remapping with MAP_FIXED) triggers TLB shootdowns, which are rather expensive in multithreaded programs. This is another reason why &quot;optimizing&quot; read() to mmap() sounds awfully dubious to me.<p>That said, shootdowns would have a constant cost per read(), independent of the size. For a sufficiently gigantic read, the cost of the shootdown would be less than the cost of copying data into the target memory, so the &quot;optimization&quot; would possibly start to make sense at that point?')