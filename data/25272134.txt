Item(by='juliansimioni', descendants=None, kids=None, score=None, time=1606871021, title=None, item_type='comment', url=None, parent=25272068, text='One very common one is for situations where you might have a multi-step pipeline to process data<p>- step 1 generates&#x2F;processes data, stores it in S3, overwriting the previous copy. triggers step 2 to run<p>- step 2 runs, fetches the data from s3 for its own processing. However, because only a few seconds have elapsed, step 2 fetches the old version of data from the S3 bucket<p>You can work around this by, for example, always using unique S3 object keys, but then you have to coordinate across the data processing steps, and it becomes harder to manage things like storing only the 10 latest versions.<p>The Argo workflow tool (<a href="https:&#x2F;&#x2F;argoproj.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;argoproj.github.io&#x2F;</a>) is one example of a tool that can suffer from this problem.')