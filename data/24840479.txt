Item(by='ozborn', descendants=None, kids=None, score=None, time=1603216952, title=None, item_type='comment', url=None, parent=24840267, text='I think the point being made is that a deep network (GPT-3 was the example with 175B parameters) will (due to the virtue of its size) not have any &#x27;local optima&#x27; in the sense that there is no traditional &#x27;local&#x27; for these high dimensional places.  This is because as the # of dimensions increases it is easier to move away from or towards both better or worse parameter sets. Thus optimization algorithms don&#x27;t have to be concerned about being trapped in a &#x27;local optima&#x27;. Also because there are many good parameter sets not all parameters even need be used to get good results, thus processes like distillation can work.')