Item(by='ocdtrekkie', descendants=None, kids=None, score=None, time=1603980381, title=None, item_type='comment', url=None, parent=24925624, text='&gt; SESTA-FOSTA caused Craigslist to close their personals sections<p>I would argue this was an explicitly political move. Craigslist is primarily funded by paid job ads... personal isn&#x27;t a profit center for them. (So, it&#x27;s unlikely to apply to a more general eyeball-need like Google&#x2F;Facebook.) And they were using the threat of doing so as a reason SESTA-FOSTA should be blocked. &quot;Missed connections&quot; is both still alive, and heck, &quot;general&quot; has some very suggestive offers in it in my region. I hardly believe SESTA-FOSTA has murdered the ability to post personals online.<p>Which is to say, the shuttering of websites such as yours if this should come to pass is a self-fulfilling prophecy: You&#x27;re threatening to do it because you don&#x27;t want the law to change. That doesn&#x27;t mean the risk of liability is unreasonable because you intend to follow through with that threat. It doesn&#x27;t mean plenty of others aren&#x27;t willing to operate websites with normal levels of liability found in... every other type of business.<p>As to your questions, Section 230 allows platforms to moderate content without facing any liability for the content that remains. The issue is that the decisionmaking they&#x27;re allowed to employ here is completely unregulated and completely opaque... and tied very much to that revenue generation: Again, adult content sees a block because advertisers don&#x27;t like it, and crazy political theories thrive because it pushes engagement.<p>We&#x27;re not talking about &quot;oh no, Facebook might be liable for one post they decided not to remove&quot;. We&#x27;re talking about an entire decisionmaking regime that is built around their revenue. Why on earth <i>shouldn&#x27;t</i> they be liable for that? If Facebook at any point reviewed QAnon posts, decided &quot;nah, that doesn&#x27;t violate our policies&quot;, and let them continue... again, we wouldn&#x27;t be going after Facebook for the user-generated content, but <i>Facebook&#x27;s explicit decisions about how to handle that content</i>. Where the intent comes into the scenario.<p>And yeah, that absolutely means we need to remove the liability shield. Because at least until we can dig into the case and order access to emails and records, seeing what posts are left up is all we have. And sure, I&#x27;d love to come at this another way too: Require the search, recommendation, and ranking algorithms of major tech platforms to be publicly shared and all moderation decisions subject to human appeal. Another avenue to that appeal though, is that people must be empowered to take a company to court over those moderation issues when their appeals internally are exhausted, which rolls back to removing liability protection.')