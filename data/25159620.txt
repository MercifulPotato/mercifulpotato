Item(by='CodesInChaos', descendants=None, kids=None, score=None, time=1605873569, title=None, item_type='comment', url=None, parent=25134471, text='Logistic regression or neural networks with softmax  activation use $ln p$ when manipulating probabilities. Normalization to a total probability of 1 is usually delayed to the end (so intermediate values are actually $ln p_i + c$ where c will only be computed during normalization). This representation of probabilities has the nice property that Bayes&#x27; law turns into simple addition.<p>The linked approach sounds very similar, calling $-ln p$ rank. However it appears to use a different normalization approach.')