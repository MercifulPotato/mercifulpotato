Item(by='vinay_ys', descendants=None, kids=[25810402], score=None, time=1610878837, title=None, item_type='comment', url=None, parent=25809146, text='If you want to host a game state object in memory (vs serialized and saved to SSD) because you have lots of frequent write&#x2F;reads happening to that object in a very short window of time such that the IO cost and CPU cost (ser&#x2F;deserialisation) is higher and the incremental latency is a blocker, then this design of hosting a full-blown object in memory within your runtime makes sense (number of reads per game object per second must be high and must be sustained for a good number of seconds for this tradeoff math to be in this design&#x27;s favor, given today&#x27;s SSD costs vs memory costs).<p>But I wonder if you will suffer from random GC pauses, inability to carefully isolate different behaviors into different resource clusters, resulting in uncontrolled blast radius etc.<p>If you are anyway doing persistence (because you care to not lose game progress), and whenever a cluster node dies you need to resurrect game state from persistence, I wonder if you will get the game state restored within a bounded latency.<p>If this happens frequently enough (to affect say 5% of your users â€“ enough to kill your game experience), is the benefits of latency gain from in-memory object reads wiped out?')