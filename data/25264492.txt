Item(by='knuthsat', descendants=None, kids=None, score=None, time=1606824447, title=None, item_type='comment', url=None, parent=25258499, text='That statement you quote is completely understandable.<p>Let&#x27;s say you have discrete sequences that are a product of a particular distribution.<p>Unsupervised methods are able, by just reading these sequences, to construct a compact representation of that distribution.\nThe model has managed to untangle the sequences into a compact representation (weights in a neural network) that allows you to use it for other, higher level supervised tasks.<p>For example, the transformer model in NLP allowed us to not have to do part-of-speech tagging, dependency parsing, named entity recognition or entity relationship extraction for a successful language-pair translation system. The compact transformer model managed to remap the sequences into a representation that allows direct translation (people have inspected these models and figured out the internal workings of it and realized it does have latent information about a parse tree of a sentence or part-of-speech of a word).<p>Another interesting note is that designers of the transformer architecture did not incorporate any prior linguistic knowledge when they were designing it (meaning that the model is not designed to model language but just a discrete sequence).')