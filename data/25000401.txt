Item(by='pchiusano', descendants=None, kids=None, score=None, time=1604599358, title=None, item_type='comment', url=None, parent=24999515, text='GA-style search isn&#x27;t actually taking multiple samples to decide on a new point to move to - it&#x27;s taking multiple samples to decide on a smaller <i>region</i> to focus on. This can be more efficient than backprop, depending on how &quot;easy&quot; it is to tell via sampling which subregion has better performance.<p>Here&#x27;s a post on this: <a href="http:&#x2F;&#x2F;pchiusano.github.io&#x2F;2020-10-12&#x2F;learning-without-a-gradient2.html" rel="nofollow">http:&#x2F;&#x2F;pchiusano.github.io&#x2F;2020-10-12&#x2F;learning-without-a-gra...</a><p>Traditional GA has a practical problem for training large models - keeping a large population of model weights in memory is not feasible. Like if you had to keep 1000 variations of the GPT-3 model weights in memory for training that&#x27;s no good. Though people have ideas for how to solve for that as well (again, see the post I linked).')