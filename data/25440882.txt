Item(by='albertzeyer', descendants=None, kids=None, score=None, time=1608113557, title=None, item_type='comment', url=None, parent=25439483, text='Hey! Thanks for the answer!<p>By graph building, I actually meant graph compilation. In TF the first `session.run`, or in Theano the `theano.function`.<p>I did not get too much into the internals of the graph compilation + optimization (despite writing a couple of simple own optimization passes), so I don&#x27;t really know whether sth is done really inefficient, but I can easily believe that. I agree, if sth is inefficient there, it should be rewritten in a more efficient way. But I also think that even if you have it as efficient as it can be, it still would be slow, compared to a C&#x2F;C++&#x2F;Rust implementation, easily by a factor of 100 or so. And even in C&#x2F;C++ it can still be slow, when I consider how much time LLVM or GCC takes in their optimization passes.<p>Yes, TensorFlow does not have much optimization, although I think the idea was always to extend that. But then, as you say, this also is one of the reasons the graph compilation is so fast. But comparing the runtime performance of Theano vs TF, in most cases, TF was just as fast or faster (which is likely dependent on the specific model; but as far as I remember, that was the general observation by the community). So because of that, I was questioning whether all that heavy graph optimization is really worth it. Numerical stability is another topic, of course. But you can also have some simple logic for that, e.g. implement your own `safe_log`, which checks if the input is `softmax(x)`, and then directly returns `log_softmax(x)`. See e.g. here: <a href="https:&#x2F;&#x2F;github.com&#x2F;rwth-i6&#x2F;returnn&#x2F;blob&#x2F;6cd6b7b3b3d3beb33140824cc7d94e88fa4fb90a&#x2F;returnn&#x2F;tf&#x2F;util&#x2F;basic.py#L4150" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;rwth-i6&#x2F;returnn&#x2F;blob&#x2F;6cd6b7b3b3d3beb33140...</a><p>Btw, graph rewriting in TF is certainly also possible, and not so complicated. But it&#x27;s not really optimized for that. You cannot rewrite parts of the graph inplace. You would need to create a new copy. (Although, technically, I think it would not be too complicated to allow for more graph rewriting, also inplace. But it was&#x2F;is just not a high priority.)<p>About `Scan`: I think the main problem is the API itself. I think it is easier if the underlying op would be `WhileLoop` or so, very similar to `tf.while_loop`. Then everything becomes very natural. However, then you would need some good way to accumulate your outputs, if you actually want to have the logic of `scan`. Sth like `ys = concat(ys, [y])` inside the loop. And then it probably is necessary to have specific optimizations on that to make that efficient. Or introduce sth like `TensorArray`. But in both cases, I think this is easier than working with `Scan` as the underlying op for loops.<p>Btw, in the blog post, it is written that TF is focusing on dynamic graphs now. While this indeed was an important focus when TF2 was introduced, I&#x27;m not sure whether they might take a step back again. Of course this is just speculation. But I think even internally, they are seeing the problems with dynamic graphs, and many groups still use the non-eager mode with static graphs and don&#x27;t have any intention to switch away from that.')