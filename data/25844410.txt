Item(by='visarga', descendants=None, kids=None, score=None, time=1611135393, title=None, item_type='comment', url=None, parent=25844096, text='Apparently not. Regarding energy usage, which has been discussed at length on HN and reddit, and is actively being researched in thousands of papers on efficient, scalable, edge-capable AI.<p>&gt; “It is past time for researchers to prioritize energy efficiency and cost to reduce negative environmental impact and inequitable access to resources,”<p>What? She should read previous work on the topic first - quantization, sparsification, distillation, fine-tuning pre-trained models, etc. I couldn&#x27;t find her actionable ideas because she doesn&#x27;t have any original ones. It&#x27;s a hard topic which requires a complete rethinking of both algorithm and hardware. And she doesn&#x27;t recognize that Google is investing in both to do that - TPUs on the hardware side, and improved algorithms such as the Reformer - a variant of the Transformer, reducing complexity from O(n^2) to O(n).<p>Then regarding dataset bias - I couldn&#x27;t find their actionable ideas. I mean, except telling people to be careful about how they select training data, but nothing about how to replace the model they criticize with an unbiased model that works just as well in the general case.<p>It&#x27;s easy to point out problems while not providing any solutions and making yourself the critic of those who are doing the hard work. See how she treated Yann LeCun on Twitter - she basically told him to go educate himself on her papers and fuck off because she doesn&#x27;t have time for debating him, that after denouncing his tweets.')