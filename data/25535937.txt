Item(by='mjburgess', descendants=None, kids=[25537761], score=None, time=1608897072, title=None, item_type='comment', url=None, parent=25531375, text='If you look at the communication, it seems that the Ad people didn&#x27;t have a clear idea how well-targeted anything was.<p>It&#x27;s easy to see how the statistical problems compound.<p>FB does not have exact earning levels, so you have to infer that from, say, likes.  Let&#x27;s say you can build a model salary(likeBMW, likeTravel, ...) = %likeBMW + %likeTravel +...<p>This gives you an estimate which is (70-80)% accurate, so you predict &gt;Â£250k&#x2F;yr 70pc of the time. In c. 25% you mispredict.<p>Now it seems to me that this 25% is going to compound across several categories: when you say &quot;College AND HighEarner AND ...&quot; you get <i>more than 50%</i> of your target group not matching this exact criteria (all you need to fail is <i>one</i> condition to fail to match this conjunction).<p>And according to FB comms, it looks like &gt;50% didn&#x27;t match client&#x27;s chosen criteria.<p>I think this is the right analysis of the issue. ML systems of this kind are very bad at making targeted judgements. It&#x27;s really little more accurate than guessing the mean of something (eg., salary) for your group.<p>All ML has to do, for FB&#x2F;Google&#x2F;etc. is improve targeting <i>a few percent</i> to have a significant value proposition.<p>However, the propaganda has it that ML systems can &quot;target&quot; you, etc. Only in the way a nuclear bomb &quot;targets&quot; a house.')