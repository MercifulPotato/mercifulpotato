Item(by='srean', descendants=None, kids=None, score=None, time=1606660945, title=None, item_type='comment', url=None, parent=25236328, text='I was having a conversation on HN a few days ago about how the mean of a set can be seen as minimizer of an optimization problem -- roughly, it can be interpreted as that location that minimizes the &#x27;distance&#x27; to all other points in a set. By choosing the &#x27;distance&#x27; appropriately one can get all sorts of fun generalizations that includes all mentioned in the post. The familiar squared Euclidean distance yields the familiar arithmetic mean.<p>One fruitful family of such &#x27;distances&#x27; that lead to arithmetic mean, geometric mean, Holder&#x27;s mean [1] and <i>beyond</i> etc is the Bregman divergence [0]. The sort of means that fall out from it have the form<p>f⁻¹ ( 1&#x2F;n ∑ᵢ₌₁ⁿ f(xᵢ) ) where f is a smooth monotonically increasing function. By f⁻¹ I mean the inverse function, not the reciprocal.<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bregman_divergence" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bregman_divergence</a><p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Generalized_mean" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Generalized_mean</a><p>[2] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25221587" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25221587</a>')