Item(by='IshKebab', descendants=None, kids=None, score=None, time=1603352435, title=None, item_type='comment', url=None, parent=24854057, text='Good example, although what if you just assigned it a reward of like 100 trillion dollars? It might not be <i>exactly</i> correct but then you&#x27;re assuming that exactly correct rewards are required for AGI which seems like a pretty big assumption.<p>Actually I thought about this some more, and maybe money wasn&#x27;t the best example, but I think there must be <i>some internal measure of utility that humans use that can be represented by real numbers</i>.<p>Imagine you are presented with an array of possible actions with associated (possibly estimated) rewards. You can only pick one. Maybe there are some doors but you can only open one - behind the first is $1m, behind the second is a superdollar, behind the third is a button that cures world hunger, behind the 4th is your loving family, whatever.<p>As a human I <i>can</i> pick one. No matter what the rewards are. Even if one reward is &quot;you essentially become God&quot;. That means I can order them, and therefore that they can be represented by real numbers (plus infinity for the god option).<p>I don&#x27;t see why the infinity would cause an issue: the &quot;you can now do literally anything&quot; reward <i>is</i> worth more than every other reward, but it&#x27;s the only one. Also it doesn&#x27;t actually exist so who cares?<p>Actually I guess it can exist in games, e.g. God mode in Quake. But that <i>should</i> have an infinite reward and agents should choose it over everything else so I can&#x27;t see the problem really.')