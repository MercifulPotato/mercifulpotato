Item(by='johncolanduoni', descendants=None, kids=None, score=None, time=1605114452, title=None, item_type='comment', url=None, parent=25059901, text='So there is technically such a known limit due to a link between information-theoretic entropy and thermodynamic entropy, which provides a lower bound on energy usage for a particular digital circuit via the second law of thermodynamics. In simpler terms, there is an unavoidable generation of heat when you &quot;throw bits away&quot; like AND and OR gates do. However we are several orders of magnitude away from that efficiency bound in today&#x27;s chips, so your analogy to LED bulbs is more apt than you may realize: LED bulbs are still far away from their theoretical maximum efficiency, but they&#x27;re still a massive improvement over incandescent bulbs.<p>If you want to know more about this limitation, I suggest looking at a way of organizing computation that avoids this issue called &quot;reversible computing&quot;[1]. As I said, it won&#x27;t be of practical significance for classical computing for a long while, but it&#x27;s actually pretty fleshed out theoretically.<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Reversible_computing" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Reversible_computing</a>')