Item(by='dev_tty01', descendants=None, kids=None, score=None, time=1608394759, title=None, item_type='comment', url=None, parent=25474731, text='You are probably right.  The problem is that it is all grey.  Some people do work with large datasets that absolutely perform much better with larger RAM.  Some people had bad experiences in the past with slow swap systems and now always believe they need more RAM.  Some people look at monitor apps and see that programs allocated large virtual blocks (because they can) and assume they need more physical RAM.  Some people had 16 GB systems in the past that they really didn&#x27;t need and now run an M1 system with 8 GB and find that everything runs fine.<p>What we need are concrete A&#x2F;B tests.  Two identical (other than RAM size) x86 systems running large data set memory hungry tasks that performs significantly better on a 16 GB system than a 8 GB system.  Then the same tasks (running native ARM code) on 8 and 16 GB M1 systems with the identical OS version.  Ideally a sequence of tests with varying memory demands.  Tests like that would at least get in the ballpark of revealing if the M1 systems are somehow more capable with less memory.  The rest is just anecdotal.<p>I suspect that M1 systems might handle light swapping a bit more efficiently than the latest x86 systems, but tasks with truly large datasets will ultimately be limited by SSD access rates.  Of course, that is just another useless opinion until reasonable test data is available.')