Item(by='dahart', descendants=None, kids=[25621369], score=None, time=1609623986, title=None, item_type='comment', url=None, parent=25616372, text='&gt; Can anyone comment on why Pixar uses standard CPU for processing instead of custom hardware or GPU?<p>A GPU enabled version of RenderMan is just coming out now. I imagine their farm usage after this could change.<p><a href="https:&#x2F;&#x2F;gfxspeak.com&#x2F;2020&#x2F;09&#x2F;11&#x2F;animation-studios-renderman&#x2F;" rel="nofollow">https:&#x2F;&#x2F;gfxspeak.com&#x2F;2020&#x2F;09&#x2F;11&#x2F;animation-studios-renderman&#x2F;</a><p>I’m purely speculating, but I think the main reason they haven’t been using GPUs until now is that RenderMan is very full featured, extremely scalable on CPUs, has a lot of legacy features, and it takes a metric ton of engineering to port and re-architect well established CPU based software over to the GPU.')