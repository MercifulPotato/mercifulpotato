Item(by='littlecranky67', descendants=None, kids=[25070523], score=None, time=1605191574, title=None, item_type='comment', url=None, parent=25068317, text='I&#x27;m sorry to disappoint you, but your benchmark methodology is flawed. You did not consider TCP congestion control&#x2F;window scaling. TCP connections between to peers are &quot;cold&quot; (=slow) after the 3-way handshake, and it takes several roundtrips to &quot;warm&quot; them up (allow data to be sent at a level that saturates your bandwidth). The mistake you (and most other people performing HTTP load benchmarks) made, is that the Kernel (Linux, but also all other major OS Kernels) caches the state of the &quot;warm&quot; connection based on the IP adress. So basically, when you run this kind of benchmark with 1000 subsequent runs, only your first run uses a &quot;cold&quot; TCP connection. All other 999 runs will re-use the cached TCP congestion control send window, and start with a &quot;hot&quot; connection.<p>The bad news: For website requests &lt;2MB, you spend most of your time waiting for the round-trips to complete, say: you spend most of the time warming up the TCP connection. So its very likely that if you redo your benchmarks clearing the window cache between runs (google tcp_no_metrics_save) you will get completely different results.<p>Here is an analogy: If you want to compare the acceleration of 2 cars, you would have race them from point A to point B starting at a velocity of 0mph at point A, and measure the time it takes to reach to point B. In your benchmark, you basically allowed the cars to start 100 meters before point A, and will measure the time it takes between passing point A and B. Frankly, for cars, acceleration decreases with increasing velocity; for TCP its the other way around: the amount of data allowed to send on a round trip gets larger with every rountrip (usually somewhat exponentially).')