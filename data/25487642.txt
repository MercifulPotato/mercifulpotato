Item(by='angry_octet', descendants=None, kids=None, score=None, time=1608483733, title=None, item_type='comment', url=None, parent=25486782, text='I did indeed read that, but it has nothing real to say about it. Instead it says this:<p>&gt; Apple uses memory which serves both large chunks of data and serves it fast. In computer speak that is called low latency and high throughput. Thus the need to be connected to separate types of memory is removed.<p>That is just hand waving. It is possible to produce such memory, but it involves ultra wide busses, far wider than optimal for filling CPU caches, and preferably directly connected to the GPU rather than a multi-master bus or switch.<p>There is the possibility that Apple has built a very fancy memory interposer that leverages the short distances in the SOC to present the memory both wide (to GPU) and narrow (for filling a queue of L2 misses), so that cache fills pause while GPUs read&#x2F;write. That would be a highly interesting piece of logic. But of course it can&#x27;t scale outside of the SOC.')