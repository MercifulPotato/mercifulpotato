Item(by='maxwells-daemon', descendants=None, kids=[24845186], score=None, time=1603234890, title=None, item_type='comment', url=None, parent=24835336, text='I don&#x27;t think we can say for sure that early stopping is the main reason deep networks generalize. Double descent [1] shows that models continue to improve even once they&#x27;ve &quot;interpolated&quot; the training data (fit every point perfectly), and critical periods [2] suggest that the early part of training is responsible for most of the generalization performance even though much of the numerical improvement happens later.<p>Overall it looks like gradient descent is a strong regularizer -- we know it tends to prefer small and low-variance weights, for example. So part of deep generalization has to do with how SGD is able to pick &quot;good&quot; features early, and then optimization pushes the unimportant weights to zero later (hence lottery tickets).<p>[1] <a href="https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;deep-double-descent&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;deep-double-descent&#x2F;</a> and other papers.\n[2] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1711.08856" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1711.08856</a> and others.')