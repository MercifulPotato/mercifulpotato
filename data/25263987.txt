Item(by='disgruntledphd2', descendants=None, kids=[25265197], score=None, time=1606817946, title=None, item_type='comment', url=None, parent=25258845, text='&gt; Models can be of arbitrarily large sizes to the point where people really can’t understand them. How do you go about dissecting a 2 layer NN with 10^40th nodes?<p>I&#x27;d probably take the output of the first layer, and cluster it.<p>It would very much depend on what this NN was attempting to do.<p>Like, all of the work in this field does suggest that people value this feature, and its incredibly useful for debugging which is generally pretty hard in ML&#x2F;statistics.<p>&gt; It’s a vastly larger solution space. So it’s really the reverse that would be surprising.<p>Imagine a world in which linear models returned the entire matrix by observation rather than the coefficients. People would argue that it was uninterpretable, but it&#x27;s a problem of tools.<p>I actually think that if you can instrument a model appropriately, then you can definitely build an interpretation layer on top of it. Clearly that doesn&#x27;t make the model perform worse.<p>Even if you can&#x27;t instrument it, you can run thousands of experiments changing one feature at a time and then estimate the impact of this feature on the model. Granted, that&#x27;s not practical on many problems, but neither were deep NN&#x27;s a decade ago.')