Item(by='mumblemumble', descendants=None, kids=[25474739], score=None, time=1608347251, title=None, item_type='comment', url=None, parent=25473942, text='It depends a lot on how you frame it. The absolute time spent waiting on memory has improved, although not at nearly the rate of other things[1]. When I was young and rosy cheeked, the performance gap was only order of magnitude, but it now spans more than three. So the cost of a cache miss, in terms of number of CPU cycles lost (which is what I meant by the term &quot;relative&quot;) has grown by quite a bit. Worse, while we keep adding CPU cores, there&#x27;s still only the one memory bus. Even if one core is waiting on a memory access, the others can keep working - but only for as long as they don&#x27;t need to access memory, either. Which may not be for very long if, for example, you&#x27;re allocating lots of short-lived objects, or using persistent data structures. In memory intensive applications, things can quickly back up so that all the cores are blocked up, waiting in line behind each other for data. If you look at the cost of this in terms of per-core CPU time rather than wall clock time, things start to look pretty icky. And this can be a devious thing, because it&#x27;s an effect that isn&#x27;t typically shown in the output of a performance profiler.<p>On the cache side, it&#x27;s true that caches are bigger. But then we run into that old saw about software people&#x27;s greatest achievement being to negate to efforts of hardware people. Caches get twice as big, and programmers decide the best thing to do with all that extra space is using 64-bit numbers as a matter of habit, or perhaps even switching to programming languages that don&#x27;t have 32-bit numbers. Or by switching to dynamic languages that cram the cache full of pointers and object headers. Things like that.<p>Which isn&#x27;t to say that all of these practices are objectively bad - spending computing resources on human productivity is typically a very good trade-off. Just that there&#x27;s no such thing as a free lunch. Memory usage still has a performance cost, and, in stark contrast to how things worked a quarter century ago, it now kicks in long before the system starts experiencing actual memory pressure.<p>1: See, for example: <a href="https:&#x2F;&#x2F;assets.bitbashing.io&#x2F;images&#x2F;mem_gap.png" rel="nofollow">https:&#x2F;&#x2F;assets.bitbashing.io&#x2F;images&#x2F;mem_gap.png</a>')