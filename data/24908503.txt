Item(by='nicoffeine', descendants=None, kids=[24910201], score=None, time=1603813431, title=None, item_type='comment', url=None, parent=24907965, text='&gt; For all the talk about how language models &quot;just&quot; learn correlations, there&#x27;s a remarkable dearth of evidence that humans do something qualitatively different.<p>GPT3 doesn&#x27;t know the difference between a given set of characters and the idea&#x2F;object the characters represent. It can associate &quot;river&quot; and &quot;stream&quot; and &quot;water&quot; but has no understanding beyond that they appear in patterns together. It couldn&#x27;t possibly make the connection that river and streams are bodies of water, because there is no association with reality.<p>GPT3 wouldn&#x27;t even know the difference between human language and characters derived from some random data source.<p>The only thing it does is identify deeply complex patterns, as long as there are humans around to notify it when it&#x27;s doing a good job. It&#x27;s going to be very useful for auto-complete, and jumping in to help users finish repetitive tasks, along with the other stuff ML is good at, but it&#x27;s simply a GIGO pattern recognition system.<p>So I think you have it exactly backwards -- there is a dearth of evidence that AGI is even remotely possible. We have known the full anatomy of the C. Elegans ringworm since 1984 -- it&#x27;s 1mm long and has 300 neurons. There is a foundation dedicated to replicating it&#x27;s behavior[1], and all they have achieved is complex animation.<p>[1] <a href="http:&#x2F;&#x2F;openworm.org&#x2F;" rel="nofollow">http:&#x2F;&#x2F;openworm.org&#x2F;</a>')