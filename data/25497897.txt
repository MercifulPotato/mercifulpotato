Item(by='itamarst', descendants=None, kids=[25498033, 25500534, 25498359, 25499333, 25498009], score=None, time=1608575880, title=None, item_type='comment', url=None, parent=25497435, text='Hey, been meaning to reach out.<p>There&#x27;s a bit in the Metaflow docs that talks about choosing resources, like RAM: &quot;as a good measure, don&#x27;t request more resources than what your workflow actually needs. On the other hand, never optimize resources prematurely.&quot;<p>The problem is that for memory, too little means out-of-memory crashes, so the tendency I&#x27;ve seen is to over-provision memory, which ends up getting very expensive at scale.<p>This choice between &quot;my process crashes&quot; and &quot;I am incentivized to make my process organizationally expensive&quot; isn&#x27;t ideal. Do you have any ways you deal with this at Netflix, or have you seen ways other Metaflow users deal with it?<p>I have some ideas on how this could be made better (some combination of being able to catch OOM situations deterministically, memory profiling, and sizing RAM by input size for repeating batch jobs), based in part on some tooling I&#x27;ve been working on for memory profiling: <a href="https:&#x2F;&#x2F;pythonspeed.com&#x2F;fil" rel="nofollow">https:&#x2F;&#x2F;pythonspeed.com&#x2F;fil</a>, so would love to talk about it if you&#x27;re interested.')