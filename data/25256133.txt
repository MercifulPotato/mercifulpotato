Item(by='derbOac', descendants=None, kids=None, score=None, time=1606756444, title=None, item_type='comment', url=None, parent=25250638, text='I agree that this is a nice piece, but I still think it&#x27;s kind of fuzzy, and maybe not formal enough, and this might be related to your question.<p>To back up a bit, let&#x27;s say you have some device (algorithm, black box, meta-DL model) that translates ML models into human-comprehensible language meeting some interpretability criterion.<p>Let&#x27;s say that some ML model fails according to this device, that the device says &quot;this is not translatable.&quot;<p>There&#x27;s different possible reasons for this, but one might be that the ML model is itself at some level unlearnable, in the sense of being incompressible or unmodelable in itself, even by another black box machine. We can say that the model might meet some cross-validation criteria, etc., but if it&#x27;s unexplainable in a meta-modeling sense, by anything, it implies maybe that the ML model is specious, that it&#x27;s meeting some superficial criteria but not really doing what it&#x27;s supposed to.<p>This &quot;meta-modeling&quot; is part of the process by which new ML models are developed by the way, at some level. It&#x27;s often implicit, but we assume we understand something -- that is, there&#x27;s some level of interpretability -- by virtue of the fact we can say &quot;such and such type of DL structure is better for this type of domain&quot; etc.<p>Of course, if the translating device says an ML model is untranslatable, it could just be that humans can&#x27;t understand it, but the danger is that we don&#x27;t really know at this point how to distinguish that from the case where the ML model is specious. It&#x27;s a sort of meta-verification problem.<p>I also think that humans have some deeper understanding that isn&#x27;t formalized yet into decision criteria regarding what constitutes a successful ML model. That is, we have some intuitive understanding of causality, and the idea of some things being causally &quot;closer&quot; to what we are interested in, in a vague sense. So when, e.g., photos of Obama are being classified based on silhouette position, we understand that there will be misclassification under a different set of stimulus conditions that are more comprehensive than what the model was trained and tested on. In that case, interpretability is tied, again, to speciousness and a failure of the model development process.<p>Incidentally, these arguments about interpretability parallel very closely debates in the psychological measurement literature in the 60s and 70s, about measures being selected based on their empirical performance (&quot;empirically keyed&quot; measures) versus other characteristics (e.g., internal structural considerations or theoretical interpretability criteria). There, there were similar arguments, in that some would say &quot;it doesn&#x27;t matter if the measure makes sense, the predictive performance matters&quot;. There were subsequent meta-analytic evaluations of how different approaches fared, and it turned out not to matter empirically in the long run. The reasons for this are difficult to explain in a small space, but one way to think about it is that when the empirically keyed measures were considered in a broader context than what they were developed on, they started to have limitations that were not initially considered (e.g., what happens when you have multiple empirically keyed measures simultaneously?). I think eventually the theoretically-based and internal-structural based approaches gained ground because they were easier to develop -- that is, you could improve them more and imbue them more easily with the types of characteristics you wanted them to have.<p>I think there&#x27;s a lot to be learned from that debate in psychology. E.g., if you can&#x27;t interpret an ML model, how do you achieve a set of goals in model development? What happens when constraints start to be introduced? You could approach this blindly but it seems you need some priors at least, which I think are kinda what human interpretability provides.<p>Maybe some day AI will be so well-developed that humans won&#x27;t matter at all (e.g., it suffices to have some ML-to-AI translation device rather than an ML-to-human translation device), but I think we&#x27;re far from that point.')