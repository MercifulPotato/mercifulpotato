Item(by='Galanwe', descendants=None, kids=None, score=None, time=1606017706, title=None, item_type='comment', url=None, parent=25172842, text='&gt; it&#x27;s nowhere near reading&#x2F;writing speeds of e.g. a single 600MB file.<p>I have to disagree here. If you look at benchmarks on internet, yes, it will look like S3 is dead slow. But that is a client problem, not an S3 problem. For instance, boto3 (s3transfer) is an awful implementation that was so overengineered with a reimplementation of futures, task stealing, etc, that the download throughput is pathetic. Most often it will make you top below 200MB&#x2F;s.<p>But S3 itself scales very well if you know how to use it, and skip boto3.<p>From my experience and benchmarks, each S3 connection will deliver up to 80MB&#x2F;s, and with range requests you can easily have multiple parallel blocks downloaded.<p>I wrote a simple library that does this called s3pd (<a href="https:&#x2F;&#x2F;github.com&#x2F;NewbiZ&#x2F;s3pd" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;NewbiZ&#x2F;s3pd</a>). It&#x27;s not perfect and is process based instead of coroutines, but that will give you an idea.<p>For reference, using s3pd I can saturate any EC2 network interface that I found (tested up to 10Gb&#x2F;s interfaces, with download speed &gt;1GB&#x2F;s).<p>Boto is really doing bad press to S3.')