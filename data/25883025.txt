Item(by='derefr', descendants=None, kids=[25884441, 25883521, 25884266, 25883104], score=None, time=1611416854, title=None, item_type='comment', url=None, parent=25882922, text='I don’t fault Java programmers for their “love” of getters&#x2F;setters; they’re taking something that really is a best-practice in <i>certain</i> situations, and which mostly doesn’t hurt otherwise (that best-practice being “making the contractual interface for an object its methods, so that the implementation can change while the interface remains stable”) and just deciding to do it all the time rather than only when it will likely have future benefits.<p>Which makes a lot of sense, if you look at it through the lens of cultural anthropology. Enterprises dictate approaches like this, because the alternative is leaving the choice to the individual programmer — and these same enterprises don’t hire experienced-enough programmers to <i>trust</i> their judgement. And so all the programmers who work for these enterprises end up absorbing this approach as a social norm, just “something you do”, rather than “independently rediscovering the need for it” in a way that would lead to them actually knowing when it’s useful. So, even when not locked into an enterprise that forces it on them, they just keep doing it, because that’s the culture that’s been inculcated on them (and how all the examples look, how all the libraries do it, etc.)<p>Personally, I’m not in theory against “unilateral” use of getters&#x2F;setters, either. I just kind of which they worked in Java the way they work in Ruby: where referencing a field on any object other than `this` would actually just be <i>sugar</i> for a call to the getter&#x2F;setter. Where the Java optimizer would then take special care to optimize-away the call frame for known-‘trivial’ getters&#x2F;setters.')