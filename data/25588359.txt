Item(by='joe_the_user', descendants=None, kids=None, score=None, time=1609376709, title=None, item_type='comment', url=None, parent=25586593, text='ML hardware is no doubt progressing. Whether we can expect a rapid rate is a different matter. The various Moore&#x27;s Laws have been breaking down (chip speed stopped doubling a while back). GPU are the sort of chip that could most benefit from just transistors doubling (last Mooore&#x27;s law standing) but even this Moore&#x27;s Law is breaking down.<p>ML model size has had it&#x27;s own Moore&#x27;s Law, with standard model growing exponentially in size [1]. And this implies model are going butt up against the limits even more than they have already. Whether the &quot;bitter lesson&quot;[2] of ML is true inherently is up for a question. That current researchers have accepted it seems given.<p>[1] <a href="https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;ai-and-compute&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;ai-and-compute&#x2F;</a><p>[2] <a href="http:&#x2F;&#x2F;www.incompleteideas.net&#x2F;IncIdeas&#x2F;BitterLesson.html" rel="nofollow">http:&#x2F;&#x2F;www.incompleteideas.net&#x2F;IncIdeas&#x2F;BitterLesson.html</a>')