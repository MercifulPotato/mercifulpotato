Item(by='arjunnarayan', descendants=None, kids=None, score=None, time=1607455374, title=None, item_type='comment', url=None, parent=25348318, text='Hi! I&#x27;m one of the two authors here. At Materialize, we&#x27;re definitely of the &#x27;we are a bunch of voices, we are people rather than corp-speak, and you get our largely unfiltered takes&#x27; flavor. This is my (and George&#x27;s from Fivetran) take. In particular this is not Frank&#x27;s take, as you attribute below :)<p>&gt; SQL is declarative, reactive Materialize streams are declarative on a whole new level.<p>Thank you for the kind words about our tech, I&#x27;m flattered! That said, this dream is downstream of Kafka. Most of our quibbles with the Kafka-as-database architecture are to do with the fact that that architecture neglects the work that needs to be done _upstream_ of Kafka.<p>That work is best done with an OLTP database. Funnily enough, neither of us are building OLTP databases, but this piece largely is a defense of OLTP databases (if you&#x27;re curious, yes, I&#x27;d recommend CockroachDB), and their virtues at that head of the data pipeline.<p>Kafka has its place - and when its used downstream of CDC from said OLTP database (using, e.g. Debezium), we could not be happier with it (and we say so).<p>The best example is in foreign key checks. It is not good if you ever need to enforce foreign key checks (which translates to checking a denormalization of your source data _transactionally_ with deciding whether to admit or deny an event). This is something that you may not need in your data pipeline on day 1, but adding that in later is a trivial schema change with an OLTP database, and exceedingly difficult with a Kafka-based event sourced architecture.<p>&gt; Normally you&#x27;d have single writer instances that are locked to the corresponding Kafka partition, which ensure strong transactional guarantees, IF you need them.<p>This still does not deal with the use-case of needing to add a foreign key check. You&#x27;d have to:<p>1. Log &quot;intents to write&quot; rather than writes themselves in Topic A\n2. Have a separate denormalization computed and kept in a separate Topic B, which can be read from. This denormalization needs to be read until the intent propagates from Topic A.\n3. Convert those intents into commits.\n4. Deal with all the failure cases in a distributed system, e.g. cleaning up abandoned intents, etc.<p>If you use an OLTP database, and generate events into Kafka via CDC, you get the best of both worlds. And hopefully, yes, have a reactive declarative stack downstream of that as well!')