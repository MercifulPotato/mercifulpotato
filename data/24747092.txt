Item(by='dmw_ng', descendants=None, kids=[24748324, 24748154, 24747543, 24747262], score=None, time=1602434844, title=None, item_type='comment', url=None, parent=24746057, text='There is a ton of conflation in this post, yesterdays post, and the comments.<p>* Comparing parallel implementations (Julia) to single thread (Python csv module). This is still quite relevant because in places like Python, it is impossible to parallelize the csv module without essentially updating its native implementation. That&#x27;s not true in other places, where some simple wrapper code may be all required to get that &quot;10x faster&quot; number.<p>* Comparing concrete object decoding (Python csv module) to array decoding (PyArrow). Producing concrete objects representing every row (e.g. tuples of strings) will always be slower because of the pressure it puts on the heap and memory. Storing 10 million rows with 10 cols of 8-byte floats in an array might only generate 800 MB of memory bandwidth, the equivalent concrete list-of-floats PyObjects would come out at 6160 MB bandwidth and a ton of CPU burned in the allocator. There are use cases where either representation is preferred, and using e.g. PyArrow&#x27;s parser then simply iterating the result as concrete objects, worst case the result will be slower than directly decoding to PyObject to begin with.<p>* Lumping floating point decoding (a problem with large performance-correctness tradeoffs) in with CSV parsing. It&#x27;s hard to decode floats both quickly and precisely, it&#x27;s also impractical to describe in the context of a comparison of CSV parsers which language&#x2F;implementation might decode floats better and why that is better.')