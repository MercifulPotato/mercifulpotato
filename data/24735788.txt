Item(by='anarazel', descendants=None, kids=[24738147, 24737326, 24736329], score=None, time=1602290372, title=None, item_type='comment', url=None, parent=24735710, text='&gt; Oltp databases typically cannot process more than 5 x cpu cores without having to time-slice among cpus<p>I think this is entirely wrong in reality. In most read-mostly OLTP workloads, due to client&lt;-&gt;server latency and application processing times, my experience as well as measurements show that at that connection count the machine will not be utilized sufficiently.<p>5x may be around the peak throughput when a benchmark client runs on the same system and connects via unix socket or localhost tcp. But even just benchmarking over a fast local (10GBe, &lt;10m cables, one switch) connection moves peak throughput quite a bit higher than that.<p>Few real world scenarios have clients sending queries back-to-back without any gaps in between. They have to process the results e.g. go through some template engine, shuffle the data to the web server.<p>I think for OLTP - and many other workloads - any guidance based on CPU cores is going to be so off for 90% of workloads to be more misleading than useful. The request patterns and thus bottlenecks vary far too much.<p>For write heavy OLTP workloads on medium-high latency storage (cheaper SSDs, all network block stores, spinning disks) a larger number of connections also can actually be <i>good</i> from a cost perspective - due to group commit fewer iops will be needed than when the same workload were split over two instances.')