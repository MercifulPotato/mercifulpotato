Item(by='sliken', descendants=None, kids=None, score=None, time=1606804715, title=None, item_type='comment', url=None, parent=25261955, text='Well my comment mentioned &quot;random (but TLB friendly)&quot;, which I define as visiting each cache line exactly once, but only with a few (32-64) pages active.<p>The reason for this is I like to separate out the cache latency to main memory and the TLB related latencies.  Certainly there are workloads that are completely random (thus the term cache thrashing), but there&#x27;s also many workloads that only have a few 10s of pages active.  Doubly so under linux when if needed you can switch to HUGE pages if your workload is TLB thrashing.<p>For a description of the Anandtech graph you posted see:\n<a href="https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;14072&#x2F;the-samsung-galaxy-s10plus-review&#x2F;5" rel="nofollow">https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;14072&#x2F;the-samsung-galaxy-s10p...</a><p>So the cache friendly line is the R per RV prange for the 5950X latencies is on the order of 65ns, the similar line for the M1 is dead on 30ns at around 128KB and goes up slightly in the 256-512KB range.  Sadly they don&#x27;t publish the raw numbers and pixel counting on log&#x2F;log graphs is a pain.  However I wrote my own code that produces similar numbers.<p>My numbers are pretty much a perfect match, if my sliding window is 64 pages (average swap distance = 32 pages) I get around 34ns.  If I drop it to 32 pages I get 32ns.<p>So the M1, assuming a relatively TLB friendly access pattern only keeping 32-64 pages active is about half the latency of the AMD 5950.<p>So compare the graphs yourself and I can provide more details on my numbers if still curious.')