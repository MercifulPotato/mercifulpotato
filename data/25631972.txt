Item(by='moron4hire', descendants=None, kids=None, score=None, time=1609771756, title=None, item_type='comment', url=None, parent=25628043, text='They do use the Lidar data in the viewer. You can see it with the positioning of the cursor on &quot;surfaces&quot; when you&#x27;re looking around. The reprojection animation they do between photospheres is based on the Lidar data, it&#x27;s just that&#x27;s about the best you can do with the low-res data they&#x27;ve collected.<p>Imagine a situation where you&#x27;re standing on a street corner with a mailbox or something standing on the corner. You move to the next photo position, which takes you past the mailbox. The smearing and warping you are seeing is from the continous mesh they&#x27;ve created from the Lidar data not matching the real world mesh in areas that were occluded from the original point of view. You get a moment of seeing &quot;behind&quot; the mailbox, but there is no data for what is behind the mailbox, so it all gets interpolated from the data that is known in the surrounding visual area. The mailbox becomes a rectangular prism that extends all the way from the mailbox&#x27;s location through to the intersection with the ground that we can see behind the mailbox.<p>These are just the problems with single-image mesh recreation. You can&#x27;t really get around them without some form of inference of the data that doesn&#x27;t exist. You even see it in Facebook&#x27;s images in the linked article, if you look closely. They try to cut the videos early so you don&#x27;t see it, but it&#x27;s there if you know what to look for.')