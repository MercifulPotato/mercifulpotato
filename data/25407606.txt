Item(by='rcxdude', descendants=None, kids=None, score=None, time=1607876481, title=None, item_type='comment', url=None, parent=25400394, text='Such resources are only really good for a particular kind of distributed computing: mostly the kind which is easily verifiable, and easily broken down into small embarrassingly parallel chunks which need a lot of computation done relative to their size.<p>Training large machine models is almost the opposite of this: the datasets are huge and don&#x27;t split up easily, and iterations (which touch the entire model and a chunk of the dataset) are relatively fast. Even with dedicated hardware a lot of the challenge in getting these systems to work effectively is in moving the data around between different compute nodes. Something like folding@home just will not function for such a workload.')