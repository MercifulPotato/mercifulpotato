Item(by='jefftk', descendants=None, kids=[25586096, 25586032], score=None, time=1609360816, title=None, item_type='comment', url=None, parent=25584775, text='<i>&gt; One recent model called Bidirectional Encoder Representations from Transformers (BERT) used 3.3 billion words from English books and Wikipedia articles. Moreover, during training BERT read this data set not once, but 40 times. To compare, an average child learning to talk might hear 45 million words by age five, 3,000 times fewer than BERT.</i><p>The human brain is the output of an incredible number of generations of training, representing a vast consumption of energy.  Most of the learning that informs the brain happened before this hypothetical five-year-old was even born.')