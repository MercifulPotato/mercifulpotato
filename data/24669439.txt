Item(by='whimsicalism', descendants=None, kids=[24669592], score=None, time=1601698065, title=None, item_type='comment', url=None, parent=24668824, text='What a surprisingly condescending response, I thought you were genuinely curious.<p>I never said anything about lower accuracy - we are running full size transformer models for translation.<p>And wav2letter++ inference models are SOTA on the Librispeech leaderboards, so try again. This is a completely different architecture than Kaldi and, frankly, conflating the two is wrong.<p>&gt;  have problem with people who make a blanket statement about running model on CPU.<p>What was my &quot;blanket statement&quot;? I said that the statement &quot;For some task inference CPU can&#x27;t be real time... speech recognitions and friends, machine translation etc&quot; was false, because those tasks can be done in real time on CPU. The original claim seems to be much more of a blanket statement than my response.<p>Cheers.')