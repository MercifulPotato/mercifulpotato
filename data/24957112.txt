Item(by='riggsdk', descendants=None, kids=[24957259, 24958037, 24959183, 24957895, 24957215, 24957531], score=None, time=1604202948, title=None, item_type='comment', url=None, parent=24956184, text='Why isn&#x27;t it more widely used to &quot;simply&quot; use 64bit integers behind the scenes and then chose some (possibly user-defined?) precision? If you for example treat a 64-bit unsigned integer as a measure of length&#x2F;distance in nanometers then you can measure distances up to 18.446.744 kilometers.\nIf you chose micrometers as your smallest unit then you get to ~17 light hours (two times the diameter of neptune&#x27;s orbit (thanks Wolfram Alpha).\nWouldn&#x27;t it be better with a value representation that is more predictable like that? You are guaranteed a precision to however many decimal places that you see fit for your specific task and you don&#x27;t have to worry about all sorts of pitfalls regarding the internal representation. Algorithms to display these values in human readable form becomes trivial as well.')