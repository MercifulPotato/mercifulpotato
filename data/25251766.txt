Item(by='stdbrouw', descendants=None, kids=None, score=None, time=1606724469, title=None, item_type='comment', url=None, parent=25250874, text='&gt; This is a low dimensional bias. If there is increased risk of default from high A &amp; B &amp; C &amp; D but not high A or high B or high C or high D, then the combination or parameters is what matters even if it is not easy to explain.<p>However, even if we expect that a non-obvious combination of parameters will matter, we usually expect the hyperplane of our predictions to be at least a little bit smooth in various ways: monotonic or curved instead of jagged, small changes in input should cause only small changes in output, etc. Not just to make it easier to understand, also because the kinds of processes we study tend to behave that way.<p>For regions of high density, machine learning does exactly what you say it does: generate high-quality predictions or categorizations, even if the particular path that led it there is nonobvious or weird. But these models are generally not sensitive to how they categorize or predict unusual combinations of inputs and as to predictive quality for those edge cases, all bets are off. A very simple case is polynomial regression, which can be tuned to perfectly fit the training data but outside of the training set might oscillate wildly or go to infinity -- and this isn&#x27;t really the result of overfitting, it&#x27;s just what polynomials do.')