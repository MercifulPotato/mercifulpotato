Item(by='supernova87a', descendants=None, kids=[25253585], score=None, time=1606730415, title=None, item_type='comment', url=None, parent=25251229, text='For some reason the article&#x27;s explanation about &quot;software ate everything&quot; and jumping between each company isn&#x27;t resonating intuitively with me, so I offer my own take on the situation:<p>For a long time, the hardware&#x2F;data center offerings of Intel, AWS, etc. were close enough to what other companies needed to not make it worth their while to invest in inventing their own solutions to low % compute problems.<p>However, as:<p>-- compute loads and costs grew<p>-- types of compute became more specialized<p>-- design and build of one&#x27;s own silicon became more accessible&#x2F;differentiable<p>-- companies&#x27; needs and tolerances for paying a premium diverged enough from what Intel&#x2F;AWS was offering,<p>it then became worthwhile for large companies (who can sustain such hardware development efforts) to design and build their own chips, either for cost reduction or functionality-enhancing reasons.  Maybe they just saw the margins being achieved by sit-on-your-hands incumbents and decided, &quot;we could do this too, and better&quot;.')