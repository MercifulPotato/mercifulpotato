Item(by='visarga', descendants=None, kids=None, score=None, time=1610392967, title=None, item_type='comment', url=None, parent=25730082, text='Let&#x27;s just wait a couple of years to see if GPT-3 was any good in applications. Doesn&#x27;t matter what we think, what matters is if it is viable.<p>It&#x27;s younger sibling DALL-E is capable of language grounded in images, I expect the next version to be multi-modal as well. On another line of research there&#x27;s effort to tame the horse (GPT) by attaching a secondary neural net. This can monitor language, topic, style and bias and ensure increased accuracy in tasks by auto-learning good prompts. It would make development of applications much easier because the base model which was super expensive to train can be reused many times while the secondary net is small and fast to train. Other efforts are related to including a search engine on an inner loop, to make the language model able to query large collections. Also, there&#x27;s an open effort to create a huge text corpus, so far 800GB (The Pile). It improves on the GPT-3 training corpus on some categories that were lacking.<p>I think it&#x27;s safe to say the article is way off the current research level.')