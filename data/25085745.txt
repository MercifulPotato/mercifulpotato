Item(by='dogma1138', descendants=None, kids=[25089444, 25089486], score=None, time=1605292567, title=None, item_type='comment', url=None, parent=25085531, text='Using SSDs for ML is what vendors like NVIDIA and AMD are trying to solve because this is a fallacy as you will never have as much RAM as you want &#x2F; need compared to the size of your datasets.<p><a href="https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;gpudirect-storage&#x2F;" rel="nofollow">https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;gpudirect-storage&#x2F;</a><p>It’s the same fallacy as RAM size\nimpacts video editing, it doesn’t.<p>8K RED raw is 4.374 terabytes per hour... at that point it doesn’t matter if you have 8, 16 or even 256GB of RAM there if your storage is too slow to support it you’ll drop frames and or have huge seek times, more memory won’t really help you there is no amount of prefetch you can do to bridge gaps that big.<p>There is a huge advantage of having a unified memory architecture where the application can address a single memory address space to access your data and you do not need to perform memory copies, allocations and all that stuff between your storage, CPU and GPU...')