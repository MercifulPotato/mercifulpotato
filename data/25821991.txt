Item(by='mlthoughts2018', descendants=None, kids=[25822206], score=None, time=1610979541, title=None, item_type='comment', url=None, parent=25821825, text='&gt; “But having a performance boundary between components, why would that help?”<p>It helps precisely so you don’t pay premature abstraction costs to over-generalize the performance patterns.<p>One of my biggest complaints with Julia is that zealots for the language insist these permeating abstractions are costless, but they totally aren’t. Sometimes I’m way better off if not everything up the entire language stack is differentiable and carries baggage with it needed for that underlying architecture. But Julia hasn’t given me the choice of this little piece that does benefit from it vs that little piece that, by virtue of being built on top of the same differentiability, is just bloat or premature optimization.<p>&gt; “you should rewrite your algorithm in a lower level layer just to support complex numbers.”<p>Yes, precisely. This maximally avoids premature abstraction and premature extensibility. And if, like in Cython, the process of “rewriting” the algorithm is essentially instantaneous, easy, pleasant to work with, then the cost is even lower.<p>This is why you have such a spectrum in Python.<p>1. Create restricted computation domains (eg numpy API, pandas API, tensorflow API)<p>2. Allow each to pursue optimization independently, with clear boundaries and API constraints if you want to hook in<p>3. When possible, automate large classes of transpilation from outside the separate restricted computation domains to inside them (eg JITs like numba), but never seek a pan-everything JIT that destroys the clear boundaries<p>4. For everything else (eg cases where you deliberately don’t want a JIT auto-optimizing because you need to restrict the scope or you need finer control), use Cython and write your Python modules seamlessly with some optimization-targeting patches in C&#x2F;C++ and the rest in just normal, easy to use Python.')