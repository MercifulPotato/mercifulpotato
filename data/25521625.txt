Item(by='cgreerrun', descendants=None, kids=[25522106], score=None, time=1608754212, title=None, item_type='comment', url=None, parent=25521186, text='What you can do is checkout the algorithm at particular stages of development. AlphaZero&amp;Friends start out not being very good at the game, then over time they learn and eventually become super human. You typically checkpoint the weights for the model at various stages. So early on, the algo would be like a 600 elo player for chess and then eventually get to superhuman elo levels. If you wanted to train using an AlphaX algo, you can gradually play against underdeveloped versions of the algo until you can beat them by loading up the weights at increasing stages of deveopment.<p>If you&#x27;re curious how it would work, I implemented AlphaZero (but not Mu yet) using GBDTs instead of NNs here: <a href="https:&#x2F;&#x2F;github.com&#x2F;cgreer&#x2F;alpha-zero-boosted" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;cgreer&#x2F;alpha-zero-boosted</a>. Instead of saving the &quot;weights&quot; for a GBDT, you save the split points for the value&#x2F;policy model trees, but the concept is the same.')