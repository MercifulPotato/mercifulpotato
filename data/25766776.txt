Item(by='winter_blue', descendants=None, kids=[25772843], score=None, time=1610565797, title=None, item_type='comment', url=None, parent=25766372, text='&gt; Not that that&#x27;s necessarily a bad thing, if we&#x27;re getting more than proportionally more utility out of the processors, but I worry about that too<p>I have two points to comment on this matter.<p>Point 1: The only reason I would worry or be concerned about it is if we are using terribly-inefficient programming languages. There are languages (that need not be named) which are either 3x, 4x, 5x, 10x, or even 40x more inefficient than a language that has a performant JIT, or that targets native code. (Even JIT languages like JavaScript as still a lot less efficient because of dynamic typing. Also, in some popular complied-to-native languages, programmers tend to less efficient data structures, which results in lower performance as well.)<p>Point 2: If the inefficiency arises out of <i>more actual computation</i> being done, that&#x27;s a different story, and I AM TOTALLY A-OK with it. For instance, if Adobe Creative Suite uses a lot more CPU (and GPU) <i>in general</i> even though it&#x27;s written in C++, that is likely because it&#x27;s providing more functionality. I think even a 10% improvement in overall user experience and general functionality is worth increased computation. (For example, using ML to augment everything is wonderful, and we should be happy to expend more processing power for it.)')