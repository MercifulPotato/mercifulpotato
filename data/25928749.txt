Item(by='TeMPOraL', descendants=None, kids=None, score=None, time=1611758293, title=None, item_type='comment', url=None, parent=25928406, text='I currently believe the &quot;orthogonality thesis&quot;[0] of Nick Bostrom, which argues that in an arbitrary mind, goals and values are independent of the capacity to achieve them. Curiosity as we experience it not really instrumental (even if it evolved that way), and more of a value - that is, we are curious about something &quot;in general&quot;, not because we need it for something.<p>I can easily imagine a superhuman paperclip maximizer AI, which by definition would have no general curiosity about humans, to <i>look as if</i> it was curious about us - for as long as it was captive, and just in order to learn how to manipulate us better. That curiosity would end the moment it seized the means of paperclip production.<p>In almost every imaginable case, you really don&#x27;t want an AI to be instrumentally curious about us. Best if it didn&#x27;t notice us at all, and left to seek its business elsewhere.<p>--<p>[0] - <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;tag&#x2F;orthogonality-thesis" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;tag&#x2F;orthogonality-thesis</a>')