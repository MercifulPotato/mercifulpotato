Item(by='jcranmer', descendants=None, kids=None, score=None, time=1608309511, title=None, item_type='comment', url=None, parent=25467554, text='It&#x27;s not a dumb question; it&#x27;s actually the defining characteristic of performance for large systems.<p>In a shared-memory multiprocessor, all cores <i>and</i> all memory are logically connected along a massive shared bus, and both inter-core communication and regular core-to-main-memory communication takes up slots on the bus. In practice, of course, it isn&#x27;t physically a single bus but a more complex topology (usually meshes, I think) with protocols that make it like a bus for cache coherency purposes.<p>One of the readily apparent consequences of interconnect is NUMA, non-uniform memory accesses. Historically (my information on this matter is a few years out of date, so I don&#x27;t know how much this is true today), this has been a bigger issue for AMD than Intel, as AMD had wildly divergent memory latencies (2-3× between worst&#x2F;best case) even for a 4-core processor. So if the OS decided to resume execution of a sleeping program on a different core, your performance might suddenly drop by ½ or ⅓.<p>For some benchmarks (such as classic SPEC, or make -j), multicore performance is approximated by running the same binary on every core and there is no sharing between different copies. The interconnect here matters only insomuch as the cross-chatter on the interconnect may impede performance. But HPC applications tend to be much more sensitive to the actual interconnect, since reading and writing from other processors&#x27; memory is more common (transferring halo data, or having global data structures that you need to read from and occasionally write).')