Item(by='brilee', descendants=None, kids=None, score=None, time=1610550989, title=None, item_type='comment', url=None, parent=25759430, text='Ironically, a lot of the tricks Stockfish is using here are reminiscent of tricks that were used in the original AlphaGo and later discarded in AlphaGoZero.<p>In particular, the AlphaGo paper mentioned four neural networks of significance:<p>- a policy network trained on human pro games.\n- a RL-enhanced policy network improving on the original SL-trained policy network.\n- a value network trained on games generated by the RL-enhanced policy network\n- a cheap policy network trained on human pro games, used only for rapid rollout simulations.<p>The cheap rollout policy network was discarded because DeepMind found that a &quot;slow evaluations of the right positions&quot; was better than &quot;rapid evaluations of questionable positions&quot;. The independently trained value network was discarded because co-training a value and policy head on a shared trunk saved a significant amount of compute, and helped regularize both objectives against each other. The RL-enhanced policy network was discarded in favor of training the policy network to directly replicate MCTS search statistics.<p>The depth and branching factor in chess and Go are different, so I won&#x27;t say the solutions ought to be the same, but it&#x27;s interesting nonetheless to see the original AlphaGo ideas be resurrected in this form.<p>The incremental updates are also related to Zobrist Hashing, which the Stockfish authors are certainly aware of.')