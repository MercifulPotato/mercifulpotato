Item(by='at_a_remove', descendants=None, kids=None, score=None, time=1609298305, title=None, item_type='comment', url=None, parent=25577693, text='I can think of <i>two</i> reasons why a hypothetical future AGI would care in the slightest about its own survival.<p>The first reason is that someone programs it to, just as a general &quot;might as well throw in Asimov&#x27;s Third Law&quot; impulse.<p>The second reason would be in the case that someone creates a general AGI, it is asked to do something, and it carelessly destroys itself in accomplishing that task.  &quot;Drats,&quot; say the programmers, &quot;we&#x27;ll have to get a copy from backup.  In the meantime, let&#x27;s make sure it doesn&#x27;t destroy itself next time.&quot;')