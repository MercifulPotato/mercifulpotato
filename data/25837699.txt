Item(by='bfrink', descendants=None, kids=[25838064], score=None, time=1611084597, title=None, item_type='comment', url=None, parent=25836633, text='In your example, you&#x27;re only a penny off if you truncate 0.199999999999999996, rather than rounding (which is described in IEEE 754!). Here&#x27;s a real simple example. Let&#x27;s say your model depends on the average of the last three ticks. The last three ticks are $1.00, $1.00, and $2.00. Ok, what&#x27;s the (exact!) average without being off by a fraction of a penny? This is the point - as soon as you start manipulating numbers in anything other than the most trivial way, you run into the dreaded floating point error, because that&#x27;s how the real numbers work.<p>I am unaware of fixed precision types that have hardware optimized (other than FPGAs which are used for feed handling in HFT anyway). If you are modeling discrete things like Minimum Price Variations, then yes, use fixed precision, or even encode it in a way that saves space. But if you&#x27;re numerically solving a partial differential equation, e.g., Black Scholes, it&#x27;s difficult to see how fixed precision numbers are going to have an advantage.')