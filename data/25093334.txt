Item(by='omarhaneef', descendants=0, kids=None, score=1, time=1605371712, title='Ask HN: State of the art on using RNNs or Transformers to assist with fiction?', item_type='story', url=None, parent=None, text='This is strictly for fun.<p>I am sure many of you have seen Karpathy train an RNN to predict the next character (sometimes word) of the Shakespeare corpus.<p>If you feed it a few input lines, it gives lots of very Shakespearen looking and sounding text.<p>Similar tricks can be done to train an RNN to reproduce things that look like Simpson scripts and so on.<p>The other interesting thing is that with GPT3, we see that there is some kind of higher order memory too and the paragraph can be on the same topic.<p>I am technical enough to experiment with these texts, download and run them and so on (except when there is a dependency collision in which case obviously no one can do anything).<p>But what is the state of the art in computer assisted fiction? I am thinking specifically of:<p>1. You outline a story, and then write out the first line of each one, and let it expand the paragraph.<p>2. You actually use it generate character dialog so that each character sounds different. You could have very authentic Homer Simpson talks to William Shakespeare dialog<p>3. Perhaps even have the computer generate a half dozen outlines based on feeding it 100s of outlines.<p>Any tools to make it easier to experiment? Right now I can try to run Karpathy&#x27;s minGPT on Jane Austen, or just run a transformer example from the pytorch examples on that text but if anyone has already done some work, has resources, paper citations, or just thought through it, I am all ears.')