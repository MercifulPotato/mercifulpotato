Item(by='PaulHoule', descendants=None, kids=None, score=None, time=1611585876, title=None, item_type='comment', url=None, parent=25898884, text='The &quot;decimal floats&quot; specified in JSON are exactly what is wrong with it;  Javascript approaches numerics like Applesoft basic did,  you have floats that can often stand in for ints.  The JSON specification promises one thing,  but it&#x27;s not supported by the &#x27;reference implementation&#x27; that it is based on.<p>Also it is a lose-lose situation.<p>Not only are floats wrong in many ways (e.g. I think 0.1 + 0.2 = 0.300000000000002 makes many people decide &quot;computing is not for me&quot;) but parsing floats (really any ASCII numbers) is astonishingly slow and people get &quot;frog boiled&quot; into accepting it.  (e.g.  there is no problem with the speed of parsing 10 floats,  but when you are parsing a million floats you have a real problem,  and a million floats is just 4 MB of core,  well in the range that a web or other application could handle on anything bigger than an 8-bit micro-controller.)<p>Like the numerous problems that cause programmers to not use the SIMD instructions in your Intel CPU,  there are multiple problems with floats,  each of which can be dismissed by apologists,  but when you add them up it&#x27;s a major drag on the industry.')