Item(by='logicchains', descendants=None, kids=[24679517], score=None, time=1601822757, title=None, item_type='comment', url=None, parent=24679329, text='&gt;If we want to give AI a Asimovian ethic, we&#x27;re just concerned with making it do what we want it to do.<p>Isn&#x27;t that the whole point of most &quot;safe AGI&quot; research? Not that it&#x27;s necessarily a good thing, but I imagine that if there was a chance that the most ethical option was in fact to destroy all humans, most people would prefer AI to have &quot;do what we want it do&quot; ethics than &quot;do what&#x27;s most ethical&quot; ethics.')