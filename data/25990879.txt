Item(by='cle', descendants=None, kids=None, score=None, time=1612200816, title=None, item_type='comment', url=None, parent=25989055, text='&gt; During the incident, AWS engineers were alerted to our packet drops by their own internal monitoring, and increased our TGW capacity manually. By 10:40am PST that change had rolled out across all Availability Zones and our network returned to normal, as did our error rates and latency.<p>Sounds like AWS knew how to handle it too.<p>Given how AWS has responded to past events like this, I&#x27;d bet there&#x27;s an internal post-mortem and they&#x27;ll add mechanisms to fix this scaling bottleneck for everyone.<p>Although one thing I&#x27;m not clear on is if this was really an AWS issue or if Slack hit one of the documented limits of Transit Gateway (such as bandwidth), after which AWS started dropping packets. If that&#x27;s the case then I don&#x27;t see what AWS could have done here, other than perhaps have ways to monitor those limits, if they don&#x27;t already. The details here are a bit fuzzy in the post.')