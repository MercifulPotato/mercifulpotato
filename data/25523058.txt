Item(by='wahern', descendants=None, kids=[25526223], score=None, time=1608762817, title=None, item_type='comment', url=None, parent=25520853, text='&gt; You can&#x27;t do things like share a page cache and dedupe inodes and dentries AFAIK with MicroVM<p>I work on several large Kubernetes clusters each running diverse jobs and this would be a huge BENEFIT. The Linux virtual memory (VM) subsystem regularly locks under heavy contention (e.g. one process performing a ridiculous amount of local, buffered I&#x2F;O, dirtying a ton of pages, while another process struggles to page back in its executable code), leaving random processes frozen for seconds or minutes. (Default frozen lock failsafe triggers after 120s, but when things are really bad I&#x27;ve seen processes frozen for much longer.) It&#x27;s a persistent source of alerts and complaints from people. It was much worse last year (2019) owing to some kernel regressions, but it&#x27;s beginning to creep back up as the clusters see more production use so I&#x27;m not looking forward to 2021.<p>Kubernetes pods rarely need to share filesystems between themselves--at least not for heavy usage--and if they do it&#x27;s usually via NFS or something similar. So MicroVMs really only leave you with the fixed cost of duplicating kernel data structures, which given the amount of memory on most nodes isn&#x27;t that much. And there&#x27;s performance potential here precisely because processes in different VMs aren&#x27;t contending for as many of the same locks, etc. The tradeoffs very much mirror those of multi-process vs multi-threaded and monolithic vs microservice architectures.<p>Realistically, though, I can see some performance degradations due to the indirection needed for accessing local storage, except where you can make use of hardware passthrough. But I&#x27;ll take a few percentage losses in runtime costs over frozen processes.<p>Linux just isn&#x27;t as robust with these heavy, multi-tenant environments. That&#x27;s what operating systems like Solaris excelled at. It&#x27;s kind of ironic, but not really because that&#x27;s typically how these things play out, unfortunately :( People are attracted by the ease and low-cost of running popular Linux-based solutions, but then try to bend them to run the kinds of workloads and architectures the older systems were designed and optimized for. MicroVM architectures are better suited to Linux&#x27; strong points.<p>And that&#x27;s all before we ever consider security. User namespaces are just a band-aid over the problem of Linux security. The real issue is the endless parade of kernel exploits. That&#x27;s where seccomp comes in. But expecting SREs or even developers to properly seccomp-jail all the myriad containers deployed--homegrown and especially third-party--is not realistic. The whole conceit of containers is to lower the barrier of entry to <i>writing</i> and <i>deploying</i> applications at scale while pretending we can still protect people from themselves. If people couldn&#x27;t figure out how to run multi-tenant before containers (that is, leverage traditional APIs, process model, and networking stack), how could we ever expect them to do any better with even <i>more</i> <i>complex</i> infrastructure? We already tried all that with SELinux. seccomp is better than SELinux because it puts the application developer in control, who is best positioned to know when, where, and how privileges are needed (assuming they know at all). Moving that work back to, effectively, the system administrator isn&#x27;t going to turn out any better than SELinux did, or present-day container security for that matter.')