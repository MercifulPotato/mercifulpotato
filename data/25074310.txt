Item(by='scottlamb', descendants=None, kids=[25075402], score=None, time=1605212009, title=None, item_type='comment', url=None, parent=25063162, text='&gt; I have another (in this case, Rust) program that demonstrates 2x performance difference based on the order in which exactly two instructions are emitted. ... That loop dominated execution time, for an almost 2x overall result.<p>I think a program like this—where two instructions in one loop is almost the entire execution time—isn&#x27;t the right one to expect profile-guided optimization to greatly improve. I think instead it&#x27;s complex programs where there&#x27;s too much hot stuff to reasonably put human attention into it all. On programs like this, I really have gotten 15% improvements from an AutoFDO pipeline. Honestly I don&#x27;t know exactly how it achieves that. I can make some guesses but will probably be wrong. Eg I was surprised when the &quot;machine function splitter&quot; was introduced recently that it wasn&#x27;t already doing that. But generally speaking, I&#x27;d expect skilled human optimization to make the most difference at the smallest scales: less hot code, changing infrequently, targeting fewer critical (micro)architectures. I&#x27;d expect compiler optimization to make the most difference elsewhere. I don&#x27;t think profile-guided optimization is an exception to this general rule.<p>&gt; To choose correctly depends on the compiler knowing a great deal more about the target microarchitecture than can reasonably be expected of a compiler.<p>fwiw, it doesn&#x27;t seem crazy to me to expect the compiler to know what operations can be fused.')