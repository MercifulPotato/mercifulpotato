Item(by='bo1024', descendants=None, kids=None, score=None, time=1603657075, title=None, item_type='comment', url=None, parent=24888150, text='That&#x27;s a good question and it&#x27;s not clear. As someone mentioned, here &quot;chance&quot; includes both uncertainty (facts that we don&#x27;t know), and randomness of nature (things that will happen in the future that cannot be deterministically deduced from the state of the world today). Depending on your philosophy these may overlap. Next, someone mentioned Bayesian vs frequentist.<p>The frequentist interpretation is roughly that if I go around making my best possible predictions, and we lump together all the things that I predict at 10%, about 1 in 10 of those things happen and the rest don&#x27;t. But I wouldn&#x27;t be able to be more specific about which ones in that group are more likely than others.<p>The Bayesian interpretation is that I can really view the world as flipping coins -- I don&#x27;t care whether it&#x27;s due to my lack of knowledge or &quot;true&quot; randomness -- and as far as I can tell, the coin flip involved here is 1 in 10.<p>We can also use a gambling interpretation. Here&#x27;s one based on security of python&#x27;s random module. Imagine the following three lotteries I offer you. In lottery A, you get $100 if Trump is elected. In lottery B, you get $100 if the following python code returns true on my laptop:<p><pre><code>    random.random() &lt;= 0.09999\n</code></pre>\nIn lottery C, you get $100 if this code returns true:<p><pre><code>    random.random() &lt;= 0.10001\n</code></pre>\nIf you would rather have lottery A than B, and you&#x27;d rather have C than A, then in some sense that you believe Trump has a 1 in 10 chance.<p>Now there&#x27;s an interesting extra layer to all of this because it&#x27;s a model predicting, not a person. In a short space, I would basically say that we&#x27;ve trained models to predict in ways that are not inconsistent with any of the interpretations above, when put into situations where that is testable. Then we use them in situations where it might not be, like this.')