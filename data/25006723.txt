Item(by='formerly_proven', descendants=None, kids=[25007414], score=None, time=1604660596, title=None, item_type='comment', url=None, parent=25006359, text='The problem isn&#x27;t actually with the transfer protocol itself, it&#x27;s the invocation of the remote scp is done using &quot;$SHELL -c &lt;some string&gt;&quot; and that turns out to be somewhat annoying to secure. The other parts (server sending other file than you requested) are really just pretty obvious oversights in validation (when you are doing open(server_response.fname, O_WRONLY) you should really have validated that fname...<p>That being said, scp-the-protocol is actually very simple. There is no spec for it, but a number of interoperable implementations and the protocol is really damn simple (it&#x27;s basically goes &quot;file &lt;length of name&gt; &lt;name&gt; &lt;size&gt; write &lt;length&gt; &lt;data&gt; write &lt;length&gt; &lt;data&gt;&quot; and so on). It achieves good throughput (for large files) over SSH, but because every file involves a few ping pongs, it is RTT-bound for small files.<p>SFTP is much, much more complicated. And the spec situation is much worse, because there are like a dozen drafts and half a dozen different versions of the protocol. SFTP also pulls in half of the POSIX file semantics. SFTP naively is RTT bound for throughput; read size is limited to 64K in OpenSSH, so with 20 ms RTT you&#x27;re only going to get at most ~3 MB&#x2F;s with a naive client.<p>SFTP is essentially NFS, but over single SSH channel (and different). You get to ask for file handles, and then you can do requests on those handles. You get to opendir() remotely and get a directory handle and so on.<p>Like NFS, SFTP supports having multiple requests in flight (how many: implementation defined &#x2F; no way to find out), so you can request multiple reads and wait for them to get around the 64K limitation. Problem: maximum read size is implementation-defined &#x2F; no way to find out, which makes this really quite complex, since you have to account for reads coming back out of order and for reads being shorted than you requested <i>without having reached EOF</i>. Say you want to transfer a 500K file in 256K chunks, you schedule two reads of 256K and 500K-256K = 244K. Let&#x27;s call them r0 and r1. Now r1 comes back, but it only read 64K (or 8K or 16K or whatever the implementation felt like). Now you need to figure out that (1) you should hold this data back, because the data before the offset of r1 has not been read yet (2) you need to issue another read to get the contents from 320-500K where (3) you may figure out that the implementation probably only does 64K reads (note: SFTP read request length field is 32 bit... expectations and all), so you get smart and schedule a few more reads: r2 for 320-384, r3 for 384-448 and r4 for 448-500K. Now you wait for the responses and get, e.g. r3, r4, r1, r2. You need to hold all this data and shuffle it around correctly, <i>then</i> write it in-order to the file (assuming you want to write the file sequentially, which is very reasonable if you want to have any chance at all of resuming the transfer).<p>This is on top of SSH <i>already having</i> throughput issues in the basic protocol over long fat networks.<p>Madness.')