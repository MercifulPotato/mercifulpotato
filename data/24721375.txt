Item(by='sillysaurusx', descendants=None, kids=[24721464, 24721397], score=None, time=1602177455, title=None, item_type='comment', url=None, parent=24721238, text='No, you do not support the TPU infeed, and this is a crucial distinction. Saying that you do support this has caused endless confusion and much surprise. It’s almost not an exaggeration to say that you’re lying (sorry for phrasing this so bluntly, but I’ve seriously spent dozens of hours trying to break this misconception due to hype like this).<p>TPU support is real. Pytorch does in fact run on TPUs. But you don’t support TPU <i>CPU memory</i>, the staging area that you’re supposed to fill with training data. That staging area is why a TPU v3-512 pod can train an imagenet resnet classifier in 3 minutes at around 1M examples per second.<p>You will not get <i>anywhere near</i> that performance with pytorch on TPUs. In fact, you’re expected <i>to create a separate VM for every 8 TPU cores</i>. The VMs are in charge of feeding the cores. That’s insane; I’ve driven TPU pods from a single n1-standard-2 using tensorflow.<p>Repeat after me: if you are required to create more than one VM, you do not (yet!) support TPU pods. I wish I could triple underline this and put it in bold. People need to understand the limitations of this technique. Creating 256 VMs to feed a v3-2048 is not sustainable.')