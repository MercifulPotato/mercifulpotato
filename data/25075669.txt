Item(by='Der_Einzige', descendants=None, kids=None, score=None, time=1605218317, title=None, item_type='comment', url=None, parent=25070803, text='There are improvements within the GA field that try to approximate gradients or do other trickery to create a &quot;feedback loop to improve the speed of the walk&quot; while still being fundementally not much more than the traditional evolution loop. This is mostly nice in fields like Reinforcement Learning where computing gradients can be difficult.<p>GAs can be quite useful in situations where there are tons of very bad local minima&#x2F;maxima and you desire the global minima&#x2F;maxima. Unfortunately, neural networks have lots of really great local minima&#x2F;maxima and the state space is so large that you&#x27;ll likely never get the global minima&#x2F;maxima (and you wouldn&#x27;t know it if you did). This is why &quot;Neuroevolution&quot; of neural network weights hasn&#x27;t really caught on.')