Item(by='SoylentOrange', descendants=None, kids=[25609793], score=None, time=1609547660, title=None, item_type='comment', url=None, parent=25608241, text='I’m an ML researcher, but did my grad work in systems. Here are several crucially important technologies invented since 1996 not mentioned in the article, which are fundamental underpinnings of the research community:<p>- Jupyter notebooks, for teaching machine learning. I use these for teaching.\n- OpenCL and other libraries for running scientific simulations on GPUs, gpflow for training on GPUs\n- Keras and PyTorch (libraries for simple training of deep learning models). More than half of machine learning research exists on top of these libraries<p>Let’s not even get into the myriad recent discoveries in ML and libraries for them.<p>Parquet file format and similar formats for mass data storage. Spark and Hadoop for massive parallel computation. Hive and Athena further build upon these innovations. A good portion of distributed computing literature is built on these.<p>Eventually consistent databases and NOSQL. There’s so much here, hard to list everything.<p>ElasticSearch and Lucene and other such tools for text search.<p>Then there is all the low level systems research: new file systems like BTRFS and ZFS. wire guard is something that’s just been built that seems foundational.<p>I am running out of words but let’s conclude by saying the premise of this article is laughable')