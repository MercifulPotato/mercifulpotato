Item(by='sthatipamala', descendants=None, kids=[25314576, 25314595, 25312909, 25312853], score=None, time=1607143179, title=None, item_type='comment', url=None, parent=25311402, text='Are these numbers the energy to train a model? The whole point of these new NLP models is transfer learning, meaning you train the big model once and fine-tune it for each use case with a lot less training data.<p>5 cars worth of carbon emissions is not a lot given that it is a fixed cost. Very few are retraining BERT from scratch.<p>EDIT:<p>The other two points are also disingenuous.<p>* &quot;[AI models] will also fail to capture the language and the norms of countries and peoples that have less access to the internet and thus a smaller linguistic footprint online. &quot;<p>NLP in &quot;low resource&quot; languages is a major area of research, especially because that&#x27;s where the &quot;next billion users&quot; are for Big Tech. Facebook especially is financially motivated to solve machine translation to&#x2F;from such languages. <a href="https:&#x2F;&#x2F;ai.facebook.com&#x2F;blog&#x2F;recent-advances-in-low-resource-machine-translation&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ai.facebook.com&#x2F;blog&#x2F;recent-advances-in-low-resource...</a><p>* &quot;Not as much effort goes into working on AI models that might achieve understanding, or that achieve good results with smaller, more carefully curated datasets (and thus also use less energy).&quot;<p>This is also a major area of research. Achieving understanding falls under the purview of AGI, which itself carries ethical and safety concerns. There are certainly research groups working toward this. And reducing parameter sizes of big networks like GPT-3 is the next big race. See <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24704952" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24704952</a>')