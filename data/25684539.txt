Item(by='YeGoblynQueenne', descendants=None, kids=None, score=None, time=1610112879, title=None, item_type='comment', url=None, parent=25682439, text='In machine learning the &quot;bias&quot; that relates to the bias-variance tradeoff is <i>inductive</i> bias, i.e. the bias that a learning system has in selecting one generalisation over another. A good quick introduction to that concept is in the following article:<p><i>Why We Need Bias in Machine Learning Algorithms</i><p><a href="https:&#x2F;&#x2F;towardsdatascience.com&#x2F;why-we-need-bias-in-machine-learning-algorithms-eff0343174c0" rel="nofollow">https:&#x2F;&#x2F;towardsdatascience.com&#x2F;why-we-need-bias-in-machine-l...</a><p>The article is a simplified discussion of an early influential paper on the need for bias in machine learning by Tom Mitchell:<p><i>The need for bias in learning generalizations</i><p><a href="http:&#x2F;&#x2F;dml.cs.byu.edu&#x2F;~cgc&#x2F;docs&#x2F;mldm_tools&#x2F;Reading&#x2F;Need%20for%20Bias.pdf" rel="nofollow">http:&#x2F;&#x2F;dml.cs.byu.edu&#x2F;~cgc&#x2F;docs&#x2F;mldm_tools&#x2F;Reading&#x2F;Need%20fo...</a><p>The &quot;dataset bias&quot; that you and the other poster are discussing is better described in terms of sampling error: when sampling data for a training dataset, we are sampling from an unknown real distribution and our sampling distribution has some error with respect to the real one. This error manifests as generalisation error (with respect to real-world data, rather than a held-out test set), because the learning system learns the distribution of its training sample. Unfortunately this kind of error is difficult to measure and is masked by the powerful modelling abilities of systems like deep neural networks, who are very capable at modelling their training distribution (and whose accuracy is typically measured on a held-out test set, sampled with the same error as the rest of the training sample). It is this kind of statistical error that is the subject of articles discussing &quot;bias in machine learning&quot;.<p>Inductive bias has nothing to do with such &quot;dataset bias and is in fact independent from dataset bias. Rather, inductive bias is a property of the learning system (e.g. a neural net architecture). Consequently, it is not possible to &quot;eliminate&quot; inductive bias - machine learning is impossible without it! The two should absolutely not be confused, they are not similar in any context and should not be interpreted as in any way similar.')