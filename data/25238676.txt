Item(by='vitus', descendants=None, kids=[25238757], score=None, time=1606582606, title=None, item_type='comment', url=None, parent=25238353, text='To understand congestion control, we need to first understand congestion and congestive collapse. In a naive world, we might have every computer try to send all the packets at once (up to some buffer if you&#x27;re working with a reliable protocol), but if you send more than fits on the network path, routers along the path _may_ buffer a few packets, but at some point they can&#x27;t buffer any more, so they start dropping packets. If you have a reliable transport protocol such as TCP (which retransmits packets until the other side acknowledges them), then you&#x27;ll retransmit all the packets that weren&#x27;t acknowledged, thereby saturating the link again and again, ad nauseum.<p>Network congestion is a little bit like traffic congestion. There are a bunch of dissimilarities, but the important thing is that if there&#x27;s a lot of congestion, then everything slows to a halt.<p>(My terminology will be a bit loose, since I assume laypeople aren&#x27;t as interested in the nitty gritty details of the bandwidth-delay product, or the specifics of how congestion windows work.)<p>So, the primary job of congestion control is to determine how much capacity there is on the network, and to avoid going over that.<p>There are additional desirable properties, such as fairness (sharing the network evenly), or efficiency (sending as close to the capacity as possible), which other solutions might try to improve upon.<p>(For all of these, the core of the congestion control is in the steady state)<p>Some well known options in the space:<p>- TCP (New) Reno: a series of improvements that use at their core the principle of &quot;additive increase, multiplicative decrease (AIMD)&quot;. Basically, you&#x27;re always probing the network by increasing slowly, but when you see a packet drop, assume that it&#x27;s due to congestion, and immediately halve the amount of bandwidth you&#x27;re using. This performs well in terms of fairness, but has poor efficiency characteristics (see: &quot;tcp sawtooth&quot; which in the limit might achieve efficiency of ~75%)<p>- TCP CUBIC: the premise behind CUBIC is to improve upon New Reno&#x27;s efficiency. Instead of linearly climbing back up, what if we probed via a cubic spline interpolation? Thus, we climb quickly at first, then slow down as we approach our previous sending rate (pre-drop), then speed up once we pass that limit in case there was a period of congestion that&#x27;s since cleared up.<p>- TCP BBR: stands for &quot;bufferbloat reduction&quot;. Bufferbloat is the increase in latency that may precede drops due to congestion, whereas, say, issues with a flaky wifi connection might not manifest in the same way. So, the goal is to detect congestion earlier than relying on packet drops (which are jarring but not always due to congestion). Previous attempts to take advantage of latency as an early detection signal such as Vegas had issues with adoption around fairness, in that they would back off more quickly than, say, New Reno or CUBIC, and thus would achieve lower throughput; BBR is more aggressive in some situations but less aggressive in others.<p>Why do we want a census? So we can see what&#x27;s out there, what adoption rates look like, and how widely newer protocols are deployed in the greater ecosystem. For instance, BBR is called out as serving &gt;40% of downstream internet traffic, but over 39% of downstream traffic is from Google properties using BBR (unsurprising, given that BBR came out of Google).<p>How have prior reports been acquired? 2011 report: <a href="https:&#x2F;&#x2F;cse.unl.edu&#x2F;~xu&#x2F;research&#x2F;TCPcensus.html" rel="nofollow">https:&#x2F;&#x2F;cse.unl.edu&#x2F;~xu&#x2F;research&#x2F;TCPcensus.html</a><p>There&#x27;s a lot of existing work around inferring TCP congestion control (especially how the congestion window evolves in reaction to loss), and I&#x27;m sure this survey made use of a fair amount of the existing literature.<p>(I don&#x27;t know how IETF decisions have been impacted in the past, so I can&#x27;t answer that one. But I imagine that this helps inspire further research in the field.)')