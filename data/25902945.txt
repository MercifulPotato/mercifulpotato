Item(by='vitus', descendants=None, kids=None, score=None, time=1611581844, title=None, item_type='comment', url=None, parent=25900587, text='So, from a quick read, I think there are a few things at play here that allow for &quot;compression of random data&quot;.<p>One, and probably the biggest one, _this isn&#x27;t lossless compression_. As other commenters mentioned, this aggregates groups of points into line segments and stores their slopes (allowing for a pre-specified error of up to epsilon).<p>Two, while the sample input data is randomly generated, it then needs to be sorted before it can be used here. This completely changes the distributional qualities (see: order statistics sampled from a uniform distribution [0]). Just as a toy example, suppose this was a million randomly-generated binary digits. Sure, you could store the million digits in sorted order, or you could just use run-length encoding and say &quot;I have 499,968 zeroes and 500,032 ones&quot; [1].<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Order_statistic#Order_statistics_sampled_from_a_uniform_distribution" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Order_statistic#Order_statisti...</a><p>[1] I know, this is a dense sampling on the input space. But that&#x27;s the sort of intuition that allows you to compress sorted data better than you&#x27;d be able to compress the unsorted data. The provided C++ code provides a sparse sampling.')