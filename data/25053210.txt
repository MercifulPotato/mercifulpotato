Item(by='FridgeSeal', descendants=None, kids=None, score=None, time=1605048997, title=None, item_type='comment', url=None, parent=25051783, text='Not necessarily.<p>It can be surprisingly cost-effective to invest a few $k in a hefty machine(s) with some high-end GPU&#x27;s to train with due to the exceedingly hefty price of cloud GPU compute. The money invested up-front in the machine(s) pays itself off in (approximately) a couple of months.<p>The &quot;neural&quot; chips in these machines are for accelerating inference. I.e. you already have a trained model, you quantise and shrink it, export it to ONNX or whatever Apple&#x27;s CoreML requires, ship it to the client, and then it runs extra-fast, with relatively small power draw on the client machine due to the dedicated&#x2F;specialised hardware.')