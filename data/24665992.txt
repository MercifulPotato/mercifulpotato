Item(by='zetazzed', descendants=None, kids=[24666672], score=None, time=1601665780, title=None, item_type='comment', url=None, parent=24664440, text='To run inference on GPUs, people are typically using TensorRT or a similarly-optimized engine. That can make a big difference in cost tradeoffs vs. CPU. Ultimately, if you can keep a GPU reasonably well-fed, the GPU can come out much cheaper and lower latency. If your workload is very sporadic and infrequent, YMMV.')