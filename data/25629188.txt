Item(by='8fingerlouie', descendants=None, kids=None, score=None, time=1609752558, title=None, item_type='comment', url=None, parent=25621598, text='&gt; Also last time I checked (I’d love to be corrected about this) even RAID1 still isn’t ideal (in particular, when a drive fails in a RAID1 array, you only get one chance to mount it r&#x2F;w and rebalance your data - after that, the array is read-only, and you need to recover by creating a new array on new disks and copy all the data across)<p>I think you&#x27;re referring to this (<a href="https:&#x2F;&#x2F;www.spinics.net&#x2F;lists&#x2F;linux-btrfs&#x2F;msg63370.html" rel="nofollow">https:&#x2F;&#x2F;www.spinics.net&#x2F;lists&#x2F;linux-btrfs&#x2F;msg63370.html</a>) bug, which was fixed in kernel &gt;= 4.20 and &gt;= 5.0 (according to <a href="https:&#x2F;&#x2F;lore.kernel.org&#x2F;linux-btrfs&#x2F;CAJCQCtTRseEwoN4cbsAaE_YZz5hUYF1oCPB-aRvz7q2mYWJfMw@mail.gmail.com&#x2F;t&#x2F;#m38b32c8f3c53cb55bdd4d012006407647ac6cda1" rel="nofollow">https:&#x2F;&#x2F;lore.kernel.org&#x2F;linux-btrfs&#x2F;CAJCQCtTRseEwoN4cbsAaE_Y...</a>)<p>That being said, the bug only causes the filesystem to become permanently degraded if the number of devices fall below the required number of devices needed for the raid type, and IIRC was caused by btrfs being unable to write metadata correctly. If you had a 3 drive raid1 pool, and one of the drives failed, it would happily continue working in degraded state.')