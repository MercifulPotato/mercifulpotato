Item(by='psykotic', descendants=None, kids=None, score=None, time=1602295944, title=None, item_type='comment', url=None, parent=24736106, text='The main purpose of the testing was to compare compile-time performance on simple code that has straightforward equivalents in all languages. This meant integer and float operations, compound expressions, local variables, function definitions and calls, so the code wasn&#x27;t trivial unlike most marketing-oriented tests I&#x27;ve seen, but I also wasn&#x27;t claiming that this was directly representative of a real-world code base of comparable size.<p>I was working on a fast compiler (which generated machine code somewhere between -O0 and -O1 in code quality) and wanted to have something to compare it across languages and compilers while being able to easily vary test parameters like total code size, size of each module, complexity of the module graph, identifier&#x2F;whitespace&#x2F;comment length distribution, etc.<p>&quot;Lines of code&quot; is a pretty poor metric for engineering but unlike &quot;tokens of code&quot; or &quot;bytes of code&quot; it&#x27;s something for which programmers have an intuitive sense of scale, so it made more sense for a fly-by tweet. Tokens of code is the most useful code size metric for measuring one-pass compiler performance if you have to pick a single number. In an expression like &quot;x + 1&quot; you can assign the cost of the parsing, type checking and code generation of the expression (separate from the sub-expressions &quot;x&quot; and &quot;1&quot;) to the &quot;+&quot; token. Even the cost of lexing is often dominated by the per-token cost (the switch jump is usually a forced branch mispredict of ~15 cycles) if you do the per-byte handling for variable-length tokens like identifiers efficiently; I used a SIMD method with a mispredict-free fast path for identifiers shorter than 16&#x2F;32 bytes.')