Item(by='fossuser', descendants=None, kids=None, score=None, time=1608744900, title=None, item_type='comment', url=None, parent=25519774, text='A lot of the stuff really is an open question though.<p>For example if you’re training models on historical data to determine credit worthiness, you can end up with models that penalize people based on race, or zip code (as a proxy for race), or names (same thing again).<p>How to correct for this is not obvious, removing the fields from the data usually just has it appear in some other way that’s correlated to race.<p>My understanding of current approach is to leave the fields in and then check to see how out of whack things are and there are a few approaches to try and correct for this (also an open question).<p>The reason modeling risk on race is unfair is because while the color of your skin may be strongly correlated with being a bad (or good) credit risk due to a whole host of complex historical reasons that lead to bias in the data, it’s not <i>because</i> of someone’s skin color that they should be considered a bad (or good) credit risk. You want the models to be judging based on more causal details. Not data that’s biased due to historical favoritism or exclusion.<p>Doing this poorly can further cement the pattern in the data and make it worse over time in addition to just being less accurate.<p>That said, you want people working on this looking for pragmatic solutions and not just promoting their own agenda while spinning things to seem worse than they are with emotionally loaded language targeting the company. Making demands and then going public on Twitter about your failed ultimatum doesn’t inspire much confidence.')