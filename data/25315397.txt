Item(by='qsort', descendants=None, kids=[25315840, 25319844], score=None, time=1607183537, title=None, item_type='comment', url=None, parent=25315129, text='AI explainability is a concept that&#x27;s being thrown around frequently these days; it revolves around the idea that machine learning models should be &quot;explainable&quot;, that is, their predictions should be traceable not just to the mathematical operations that define it, but also to some properties of the input which should be understandable by a human.<p>While I won&#x27;t deny that the concept is interesting, it&#x27;s terribly difficult to understand what it would translate to, technically speaking. It&#x27;s true that machine learning models make predictions that are hard to check (and sometimes even understand), but they aren&#x27;t inherently &quot;less explainable&quot; than even the simplest statistical models, like linear regressions.\nFor example, it&#x27;s pretty weird to be angry that &quot;ML is not explainable&quot; but to be okay with things like Google search that have literally zero transparency.<p>My main problem with it is that people with poor understanding of computer science and math in general - let alone machine learning - throw around the term like it&#x27;s obvious what they mean, when their real goal is generating social media clout in the case of journalists and influencers, or, more darkly, to enforce political control of the industry in the case of politicians.')