Item(by='deeplstm', descendants=None, kids=None, score=None, time=1603377342, title=None, item_type='comment', url=None, parent=24858451, text='It&#x27;s a super cool paper that invents &quot;vokenization&quot; to generate a large amount of visually-grounded language datasets and trains visually-grounded models on those.<p>Most language models are trained on pure text data. Although it achieves significant success in recent years, this is not how humans acquire a language. It raises an interesting question &quot;Can language models achieve a high level of language understanding by reading the text input alone?&quot; The answer is probably &quot;no&quot;.<p>To push the boundary of language models, adding other learning signals in the learning process is the key to success. And the first thing that comes to my mind is vision (visual cue). However, the existing visually-grounded datasets are a level of magnitude smaller than pure text ones. This paper purposes &quot;vokenization&quot; method to overcome this problem, and uses the new data that generate to train visually-supervised language models.<p>More importantly, visually-grounded models show significant improvements over text-grounded only models.<p>Paper <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2010.06775" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2010.06775</a>  \nCode <a href="https:&#x2F;&#x2F;github.com&#x2F;airsplay&#x2F;vokenization" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;airsplay&#x2F;vokenization</a>')