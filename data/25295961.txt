Item(by='ryanbooz', descendants=None, kids=None, score=None, time=1607033760, title=None, item_type='comment', url=None, parent=25294589, text='&gt; Another suggestion I&#x27;d make is to run the Amazon Timestream clients across multiple AZs if you aren&#x27;t already. The blog post doesn&#x27;t mention whether all the t3 instances are in the same AZ or not.<p>They were all run in the same AZ. This brings up a good point, however, that we discussed internally when the first results came back. If there are tricks like this that might improve performance, it&#x27;s not (currently) spelled out in the documentation so there&#x27;s no way to know that. And we reached out for help in various forums with no response.<p>It&#x27;s worth noting that since we performed this analysis, Amazon did release their own tooling for a similar benchmark and created a post[0]. While neither it, nor the tooling documentation[1] specifically spell out how many threads or instances they ran to achieve their results, it&#x27;s hard to draw an apples-to-apples comparison. It does reveal that they used (had to use??) an m5.24xlarge instance (96 vCPU,384GB) to run their tests. As discussed in the article, one much smaller t3 instance was able to add &gt;1 million metrics&#x2F;second into TimescaleDB running in Timescale Forge.<p>[0] <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;database&#x2F;deriving-real-time-insights-over-petabytes-of-time-series-data-with-amazon-timestream&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;database&#x2F;deriving-real-time-ins...</a>\n[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;awslabs&#x2F;amazon-timestream-tools&#x2F;tree&#x2F;master&#x2F;tools&#x2F;perf-scale-workload" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;awslabs&#x2F;amazon-timestream-tools&#x2F;tree&#x2F;mast...</a>')