Item(by='jedbrown', descendants=None, kids=[24982391], score=None, time=1604426243, title=None, item_type='comment', url=None, parent=24981436, text='You&#x27;re arguing at a particular granularity and even that doesn&#x27;t hold uniformly.<p>A64FX is HBM2 at equivalent bandwidth to GPUs (with lower power). CCX is much finer granularity than an entire GPU, so not a direct comparison. L3 bandwidth on EPYC is multi-TB&#x2F;s.<p>Fat GPU nodes can readily overload network interfaces so if bisection bandwidth is your concern, CPU nodes are good.<p>&gt; HPC however, is specifically programmed to be bandwidth-bound instead.<p>This is wishful thinking. Lots of applications used to justify the US exascale program (and others) are latency-bound. Climate, weather, unsteady CFD, and much of mesoscale materials science and molecular dynamics are run at their latency limit in most scientific studies (one-off scaling studies notwithstanding). There&#x27;s an unfortunate disconnect between what scientific computing actually needs versus what funders and the media portray.')