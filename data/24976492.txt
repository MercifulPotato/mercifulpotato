Item(by='wutbrodo', descendants=None, kids=None, score=None, time=1604379724, title=None, item_type='comment', url=None, parent=24976387, text='&gt; one problem I&#x27;ve always had with Silver is that he&#x27;s very smug about this models but they often enough don&#x27;t work out.<p>What does it mean for a probabilistic prediction to &quot;not work out&quot;? If I tell you that the odds of flipping 2 heads in a row with a fair coin is only 25%, and it happens, did my model &quot;not work out&quot;?<p>&gt;  instead of reflecting on his models to improve them (at least publicly,) he just tells people they don&#x27;t understand probability or his models.<p>I really don&#x27;t intend this in any sort of rude way, but I think you might be interpreting his response as dismissive because it&#x27;s accurate...<p>It&#x27;s difficult to say a model &quot;doesn&#x27;t work out&quot; based on a single event, especially when it puts a significant probability on the less-likely option.  For example, 538&#x27;s 2016 forecast gave Trump a roughly 1 in 3 chance of winning: that&#x27;s _higher_ than the odds of flipping 2 heads in a row.<p>Measuring the quality of a model is more complicated: one way would be applying the model to the same outcome multiple times, but this isn&#x27;t possible for single-event forecasts. Another way to see how calibrated your predictions are is to aggregate multiple predictions and see how often they line up with reality: an event predicted to occur with X% probability should occur X% of the time.<p>Lucky for us, 538 has done exactly this analysis[1]! Naturally, it&#x27;s internal, so take it with a grain of salt, but it looks like their predictions are fairly well-calibrated.<p>[1] <a href="https:&#x2F;&#x2F;projects.fivethirtyeight.com&#x2F;checking-our-work&#x2F;" rel="nofollow">https:&#x2F;&#x2F;projects.fivethirtyeight.com&#x2F;checking-our-work&#x2F;</a>')