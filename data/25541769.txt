Item(by='ncmncm', descendants=None, kids=[25542165, 25547265], score=None, time=1608962795, title=None, item_type='comment', url=None, parent=25536961, text='What nobody says about binary search is that it is <i>very, very</i> slow on modern hardware.<p>The reasons it is slow are interesting and useful to understand.<p>On modern equipment, to get decent performance, everything needs to pipeline, so you start an operation and get the answer out 3, 5, sometimes 12 cycles later. In the meantime, you&#x27;re pushing new operations into the pipeline so that useful answers will come out the cycle after. To keep pipelines (plural; there are lots) full, we have &quot;branch prediction&quot;: the CPU doesn&#x27;t know which way an upcoming conditional branch will go, so it <i>guesses</i>. It has a neural network to remember how this branch went other times, that is quite clever. When it guesses right, all is good, and the pipeline stays full. When it guesses wrong, it has gone off doing lots of wrong computations that have to be thrown away, and the pipeline restarted at the correct spot.<p>Binary search presents the worst case for prediction. At each stage, any prediction is 50% likely to be wrong. You hardly get two steps into your pipeline before it turns out you were wrong, and have been doing the wrong thing.<p>There are ways to reduce the worst effects, but they make the algorithm much less pleasant to read, and even trickier to get right. One is to use clever arithmetic to arrange that the same instruction sequence runs whichever way the choice goes. Another is to split the range into more than two subranges, so the choice is made less often--log base 3 or -base 4 iterations, instead of two. Maybe you pre-fetch values you expect to look at soon: half of them will be ones you won&#x27;t actually need, and you won&#x27;t know which until later, but you didn&#x27;t need to wait for the right one.<p>Once a range gets small enough, you do better searching sequentially, because CPUs are tuned to do that very fast.<p>Hash tables avoid all this foolishness, and (so) can be very fast.<p>To write good, fast code for things like binary search, intuition is entirely inadequate. You really need to pay a lot of attention to &quot;performance counter&quot; registers modern chips have, that can tell you about prediction failures and cache misses that slow you to a crawl. It is humbling to discover how wrong all we learned in school, about algorithm performance, is.<p>When you home in on a code sequence that minimizes pipeline bubbles, you have to expect that the next generation of CPU chips will have completely different details, and that what was fast is now slow.<p>The deep lesson is that we never know if our code is fast until we find code that is faster; what seemed fast becomes slow in an instant. All the other old faithful algorithms, like Quicksort, can be easily 2x faster or slower according to obscure details that are hard to reason about.<p>It makes some people decide that 2x, 4x, 100x, or 1000 times slower than necessary ought to be considered fast enough.')