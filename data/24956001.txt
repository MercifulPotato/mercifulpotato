Item(by='Twisol', descendants=None, kids=[24961315, 24956700], score=None, time=1604186013, title=None, item_type='comment', url=None, parent=24954182, text='&gt; Second, and more substantively, it is not the case that mathematical fields inevitably turn into algebra.<p>Not to fully justify the OP (though it rings both true and not quite true to me), but I think topology is a good positive example. It&#x27;s impressive how much of continuity can be captured purely algebraically. For example, topologies are a kind of lattice, which cross both algebra and order theory. And then you get literal <i>algebraic topology</i>, which captures useful and interesting properties of individual topological spaces effectively.<p>&gt; A more substantive objection is that the sole purpose of parallelism is performance.<p>I disagree a lot more strongly here, though I will twist your words a little bit. Yes, parallelizing <i>a</i> program is about making it run faster; but parallelism is also a base fact of many problem domains, where you have multiple agents (up to and including humans) collaborating and interacting simultaneously.<p>Abstracting over the speed-up aspects of parallelism gets you to <i>concurrency</i> -- the independence of knowledge held by distinct agents that is not directly knowable by others -- which is far more fundamental than simply making programs run faster. In my experience, most properties of modular systems can be stated in terms of concurrency. Parallelism is an exploitation of that concurrency structure to schedule things efficiently, but it is by no means the only application.<p>The fact that &quot;one can achieve a 7x speedup on 8 cores by adding a handful of lines to a program&quot; tells me that Haskell is extremely good at letting you expose the concurrent structures of your problem domain.')