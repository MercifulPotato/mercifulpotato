Item(by='DoreenMichele', descendants=None, kids=None, score=None, time=1608201435, title=None, item_type='comment', url=None, parent=25454211, text='<i>I&#x27;d be willing to bet some money on it being simply the result of some employees saying, &quot;AI ethics is super important. We feel strongly there should be people working on AI ethics to make sure it&#x27;s not racist or sexist&quot;.</i><p>Oh, no doubt. But that scenario would be rooted in this larger context of how we generally relate to this topic.<p><i>Creating a dedicated ethics team was a dumb idea because it can only possibly end like this</i><p>I agree, which is why I say &quot;it&#x27;s probably stupidly conceived.&quot;<p>I find social stuff fascinating and I&#x27;ve had the following college classes: Intro to Psychology; Social Psychology; Negotiation and Conflict Management. I also raised two fairly Aspie-ish sons and I&#x27;ve had to explain social things to them in very exacting terms, which forced me to really up my game on being precise in order for it to be useful to them. My sons and I still look for things that meet our rigorous standards for &quot;How do you know that&#x27;s actually accurate?&quot; because social phenomenon are inherently hard to test: People behave differently when they know &quot;This is a test. This is only a test.&quot; than they do in real life (which is why I think UBI experiments are useless: These people know the study will end and the money will stop rolling in, so it doesn&#x27;t tell you how people would really behave if we really instituted a UBI).<p>Anyway, I find social stuff fascinating and people routinely interpret my social observations as being ethical or moral judgements and I get this reputation for being a real judgy bitch, basically, even though I&quot;m a lot less judgy than most people. But when push comes to shove, &quot;ethics&quot; is generally about determining which behaviors protect one or both of two things: individuals from group abuse or groups from individual abuse (&quot;tragedy of the commons&quot; type stuff et al).<p>So when I talk about what works or what doesn&#x27;t, in some sense I am talking about ethics, but I&#x27;m usually not trying to be some kind of moralizing priss, which is how that gets interpreted. I&#x27;m usually trying to say something like &quot;Objectively speaking, you can&#x27;t be both heavy and light at the same time. That fundamentally doesn&#x27;t parse.&quot; and then someone is all up in arms about &quot;Don&#x27;t talk about my momma!!!! grrrrr!&quot; type stuff.<p>So I feel strongly that what they really were probably wanting to study is &quot;How do we do AI well in ways that interact with this social stuff like racism?&quot; and out popped the word <i>ethics</i> because that&#x27;s how people think of that. And it was a terrible idea on the face of it because of the inherent assumption of judgment rather than a more objective framing about studying AI and social phenomenon and how does that work? When you start using AI to do things like parse faces, what are the practical pain points there that we need to be aware of and try to address?<p>But I think most people can&#x27;t readily separate that out in that way, which is part of why I am so often in hot water for saying &quot;Well, actually, I think that simply doesn&#x27;t work because....&quot; And when I say that about social phenomenon, I am often de facto talking about &quot;human ethics&quot; whether I want to be or not and it&#x27;s very often a giant pile of manure that I&#x27;ve just stepped into.')