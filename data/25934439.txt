Item(by='derefr', descendants=None, kids=None, score=None, time=1611784011, title=None, item_type='comment', url=None, parent=25934104, text='Right, but GPT-2 was the name of the particular ML <i>architecture</i> they were studying the properties of; not the name of any specific model trained on that architecture.<p>There was <i>a</i> pre-trained GPT-2 model offered for download. The whole &quot;interesting thing&quot; they were publishing about, was that models trained under the GPT-2 ML architecture were uniquely-good at transfer learning, and so <i>any</i> pre-trained GPT-2 model of sufficient size, would be extremely useful as a &quot;seed&quot; for doing your own model training on top of.<p>They built one such model, but that model was not, itself, &quot;GPT-2.&quot;<p>Keep in mind, the training data for that model is open; you can download it yourself and reproduce the offered base-model from it if you like. That&#x27;s because GPT-2 (the architecture) was formal academic computer science: journal papers and all. The particular pre-trained model, and its input training data, were just published as experimental data.<p>It is under <i>that</i> lens, that I call GPT-3 &quot;GPT-2++.&quot; It&#x27;s a different <i>model</i>, but it&#x27;s the same <i>science</i>. The model was never OpenAI&#x27;s &quot;product.&quot; The science itself was&#x2F;is.<p>Certainly, the SaaS pre-trained model named &quot;GPT-3&quot; is qualitatively different than the downloadable pre-trained base-model people refer to as &quot;GPT-2.&quot; But so are all the various trained models people have built by training GPT-2 <i>the architecture</i> with their own inputs. The whole class of things trained on that architecture are fundamentally all &quot;GPT-2 models.&quot; And so &quot;GPT-3&quot; is just one such &quot;GPT-2 model.&quot; Just a really big, surprisingly-useful one.')