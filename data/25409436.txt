Item(by='kevingadd', descendants=None, kids=None, score=None, time=1607888418, title=None, item_type='comment', url=None, parent=25409212, text='In the first place, using the hardware encoder is only feasible if the output is up to your quality&#x2F;size standards and is compatible with the decoders that are going to consume your content. If your goal is to quickly render near-lossless mp4&#x2F;mkv files for uploading to youtube, any regular old hardware encoder is probably fine. If your goal is to render out 6000kbps footage to store on your own CDN, the quality per bit becomes EXTREMELY IMPORTANT and suddenly it may not be feasible to use a particular hardware encoder.<p>FWIW, NVIDIA has made significant improvements to quality for their hardware encoders in each of their last 3 generations, and you definitely saw reviewers and creatives talking about that in particular when it came to purchasing decisions.<p>Apple&#x27;s encoder is probably quite good at least, but I don&#x27;t think it&#x27;s meaningful to consider it for most benchmarks. The scenarios where you both are willing to use the hardware encoder <i>and</i> care about how fast it is are relatively few and far between - if you&#x27;re just doing a zoom call all that matters is whether it can pump out 60fps and how good it looks, not whether it uses 3% cpu instead of 5%. I&#x27;d rather see quality&#x2F;bitrate comparisons of their encoder with x264, not benchmarks.')