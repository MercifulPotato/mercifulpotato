Item(by='gamozolabs', descendants=None, kids=None, score=None, time=1607378251, title=None, item_type='comment', url=None, parent=25337256, text='Oooh, wasn&#x27;t really expecting this to make it to HN cause it was meant to be more of an announcement than a description.<p>But yes, I&#x27;ve done about 7 or 8 operating systems for fuzzing in the past and it&#x27;s a massive performance (and cleanliness) cleanup. This one is going to be like an operating system I wrote 2-3 years ago for my vectorized emulation work.<p>To answer your QEMU questions, the goal is to effectively build QEMU with MUSL (just to make it static so I don&#x27;t need a dynamic loader), and modify MUSL to turn all syscalls to `call` instructions. This means a &quot;syscall&quot; is just a call to another area, which will by my Rust Linux emulator. I&#x27;ll implement the bare minimum syscalls (and enum variants to those syscalls) to get QEMU to work, nothing more. The goal is not to run Linux applications, but run a QEMU+MUSL combination which may be modified lightly if it means a lower emulation burden (eg. getting rid of threading in QEMU [if possible] so we can avoid fork())<p>The main point of this isn&#x27;t performance, it&#x27;s determinism, but that is a side effect. A normal syscall instruction involves a context switch to the kernel, potentially cr3 swaps depending on CPU mitigation configuration, and the same to return back. This can easily be hundreds of cycles. A `call` instruction to something that handles the syscall is on the order of 1-4 cycles.<p>While for syscalls this isn&#x27;t a huge deal, it&#x27;s even more emphasized when it comes to KVM hypercalls. Transitions to a hypervisor are very expensive, and in this case, the kernel, the hypervisor, and QEMU (eg. device emulation) will all be running at the same privilege level and there won&#x27;t be a weird QEMU -&gt; OS -&gt; KVM -&gt; other guest OS device -&gt; KVM -&gt; OS -&gt; QEMU transition every device interaction.<p>But then again, it&#x27;s mainly for determinism. By emulating Linux deterministically (eg. not providing entropy through times or other syscall returns), we can ensure that QEMU has no source of external entropy, and thus, will always do the same thing. Even if it uses a random-seeded hash table, the seed would be derived from syscalls, and thus, will be the same every time. This determinism means the guest always will do the same thing, to the instruction. Interrupts happen on the same instructions, context switches do, etc. This means any bug, regardless of how complex, will reproduce every time.<p>All of this syscall emulation + determinism I have also done before, in a tool called tkofuzz that I wrote for Microsoft. That used Linux emulation + Bochs, and it was written in userspace. This has proven incredibly successful and it&#x27;s what most researchers are using at Microsoft now. That being said, Bochs is about 100x slower than native execution, and now that people have gotten a good hold of snapshot fuzzing (there&#x27;s a steep learning curve), it&#x27;s time to get a more performant implementation. With QEMU with get this with a JIT, which at least gets us a 2-5x improvement over Bochs while still &quot;emulating&quot;, but even more value could be found if we get the KVM emulation working and can use a hypervisior. That being said, I do plan to support a &quot;mode&quot; where guests which do not touch devices (or more specifically, snapshots which are taken after device I&#x2F;O has occurred) will be able to run without QEMU at all. We&#x27;re really only using QEMU for device emulation + interrupt control, thus, if you take a snapshot to a function that just parses everything in one thread, without process IPC or device access (it&#x27;s rare, when you &quot;read&quot; from a disk, you&#x27;re likely just hitting OS RAM caches, and thus not devices), we can cut out all the &quot;bloat&quot; of QEMU and run in a very very thin hypervisor instead.<p>In fuzzing it&#x27;s critical to have ways to quickly map and unmap memory as most fuzz cases last for hundreds of microseconds. This means after a few hundred microseconds, I want to restore all memory back to the state &quot;before I handled user input&quot; and continue again. This is extremely slow in every conventional operating system, and there&#x27;s really no way around it. It&#x27;s of course possible to make a driver or use CRIU, but these are still not exactly the solution that is needed here. I&#x27;d rather just make an OS that trivially runs in KVM&#x2F;Hyper-V&#x2F;Xen, and thus can run in a VM to get the cross-platform support, rather than writing a driver for every OS I plan to use this on.<p>Stay cute,\n~gamozo')