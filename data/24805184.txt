Item(by='throwawaygh', descendants=None, kids=None, score=None, time=1602883362, title=None, item_type='comment', url=None, parent=24805097, text='<i>&gt;&gt;&gt; No primary moderation action should be made based on human input</i><p><i>&gt;&gt; 1. That means no HN.</i><p><i>&gt; No, it means less 230 protection for HN.</i><p>I&#x27;d be fascinated to hear what dang thinks about HN&#x27;s future existence if this hypothetical law where &quot;No primary moderation action should be made based on human input&quot; applied to HN.<p>It seems impossible to (a) run a healthy forum or (b) avoid lawsuits or even jail. E.g., can you link me to a github repo that automatically catches 100% of libel? Or even 100% of child porn (or I guess actual porn as a proxy for that problem)? Removing libel and other illegal content without &quot;primary moderation action&quot;s that are based on &quot;human input&quot; is not currently possible.<p>(BTW: that&#x27;s NOT what Hawley&#x27;s bill does! It allows human moderation, you just have to keep the political appointees happy.)<p><i>&gt;&gt; What is the difference between misinformation and libel?</i><p><i>&gt; Actual malice? If the standard works for newspapers, why can&#x27;t it work for social media companies?</i><p>Because newpapers have a few journalists. Not hundreds of millions of users.<p>This has to be done arithmetically or it&#x27;s financially reckless to allow free-form comments at all. If it&#x27;s so easy to algorithmically identify libel with 100.00% accuracy, <i>go do it!</i><p>Given that there are regularly court cases that hinge on whether some statement raised to the level of libel -- cases that even get appealed and where highly trained judges disagree -- I&#x27;m willing to bet the problem is AGI-complete. And then some.<p><i>&gt; The government has a clear processes to designate foreign and domestic terrorist organizations. [0] Let the actual politicians engage in that political fight. Social media companies can use the result.</i><p><i>&gt; Content should not be removed or suppressed based on any political preference or designation</i><p>So politicians get to define what terrorism means and companies should suck it up and implement whatever the politicians in power decide.<p>So, if some powerful GOP senator designates BLM a terrorist organization, and social media companies then remove all BLM content, is that not &quot;removing or suppressing based on political preference&quot;? What about pro-2A militias? What about QAnon?<p>By the way, what about &quot;illicit content&quot;? If some hard core right-winger takes over Twitter tomorrow, can they ban pictures of homosexuals kissing as &quot;illicit content&quot;?<p>Hawley -- whose bill doesn&#x27;t even do what you suggest -- is just shifting power over content moderation decisions from companies to political appointees. That&#x27;s all. It&#x27;s not neutral, it is based on human input, and it&#x27;s primarily just a shift in decision making power.<p>Dressing this up as &quot;neutral&quot; is obvious bullshit. Hawley wants Twitter to understand that his political party is their ultimate master when they choose which speech to amplify on their platform. This is his explicit and openly stated goal. It is about power, not neutrality.<p>But anyways, this argument is easy to resolve in your favor. You propose not Hawley&#x27;s bill, but a hypothetical different one where human input can&#x27;t be a primary consideration. So, you&#x27;re claiming that a formal specification of the political neturality of an NLP classifier exists. I&#x27;ve build a lot of classifiers, and I don&#x27;t believe you. Show me the code.')