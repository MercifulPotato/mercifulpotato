Item(by='syllogism', descendants=None, kids=None, score=None, time=1612191832, title=None, item_type='comment', url=None, parent=25989177, text='Yes you can easily train with xlnet instead of roberta-base --- just write the different model name in the config file (or pass a different string value, if doing it from Python). You can find an example config file here: <a href="https:&#x2F;&#x2F;github.com&#x2F;explosion&#x2F;projects&#x2F;blob&#x2F;v3&#x2F;benchmarks&#x2F;ner_conll03&#x2F;configs&#x2F;transformer.cfg" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;explosion&#x2F;projects&#x2F;blob&#x2F;v3&#x2F;benchmarks&#x2F;ner...</a><p>I actually didn&#x27;t see a performance improvement when using XLNet over roberta-base. I always wondered about this; ages ago I looked into it and I wasn&#x27;t sure that the preprocessing details in the transformers version were entirely correct.<p>Given very similar accuracies from XLNet and RoBERTa, I preferred RoBERTa for the following reasons:<p>* I&#x27;ve never been able to understand the XLNet paper :(. I spent some time trying when it was released, but I just didn&#x27;t really get it, not anything close to the level where I&#x27;d be able to implement it, anyway.<p>* Standardising on BERT architecture has some advantages. If we mostly use BERT, we have a better chance of using faster implementations. Mostly nobody is training new XLNet models, whereas many new BERT models are being trained.')