Item(by='gvinciguerra', descendants=None, kids=None, score=None, time=1611572861, title=None, item_type='comment', url=None, parent=25900023, text='Hi @etaioinshrdlu!<p>The worst-case bounds are discussed in *Section 2.2* and *Theorem 1*. Essentially, we have the following bounds:<p>Query: <i>O(log_c(m) log_2(ε&#x2F;B))</i> I&#x2F;Os\nSpace of the index: <i>O(m)</i><p>where:\n<i>n</i> = number of input keys\n<i>B</i> = disk page size\n<i>ε</i> = user-given maximum error of the piecewise linear approximation (determines how many keys you need to search at each level)\n<i>m</i> = number of segments in the piecewise linear approximation\n<i>c</i> = fan out of the data structure (differently from standard B-trees it is not fixed, and it is potentially large)<p>Intuitively, the query complexity comes from the fact that the PGM-index has <i>O(log_c(m))</i> levels, and at each level you do a binary search that costs <i>O(log_2(ε&#x2F;B))</i> I&#x2F;Os.<p>Note that <i>m</i> and <i>c</i> depend on the &quot;linearity&quot; of the given input data. For example, if the input data can be approximated by a few segments, i.e. if m=O(1), and you choose ε=Θ(B), then the PGM-index takes O(1) space and answer queries in O(1) I&#x2F;Os!<p>In general, you can remove the dependence from <i>m</i> and <i>c</i> if you can prove a lower bound on the length of a segment (i.e. the number of keys it &quot;covers&quot;), irrespective of the input data. We proved that the length of a single segment is at least <i>2ε</i> (thus <i>c≥2ε</i>), or equivalently, that the number of segments <i>m</i> is upper bounded by <i>n&#x2F;(2ε)</i> [Lemma 2, the proof is very straightforward].\nAgain, if you choose <i>ε=Θ(B)</i>, then you have the following (rather pessimistic) worst-case bounds:<p>Query: <i>O(log_B(n))</i> I&#x2F;Os\nSpace of the index: <i>O(n&#x2F;B)</i><p>Basically, these bounds tell you that the PGM-index is *never* worse in time and in space complexity than a B-tree!<p>---<p>However, in our experiments, the performance of the PGM-index was better than what the above bounds show, and this motivated us to study what happens when you make some (general) assumptions on the input data. The results of this study are in the ICML20 paper &quot;Why are learned indexes so effective?&quot; (<a href="http:&#x2F;&#x2F;pages.di.unipi.it&#x2F;vinciguerra&#x2F;publication&#x2F;learned-indexes-effectiveness&#x2F;" rel="nofollow">http:&#x2F;&#x2F;pages.di.unipi.it&#x2F;vinciguerra&#x2F;publication&#x2F;learned-ind...</a>).<p>We found that, if you assume that the gaps between input sorted keys are taken from a distribution with finite mean and variance, then you can prove (Corollary 2 of the ICML20 paper) that the space of the PGM-index is actually <i>O(n&#x2F;B^2)</i> whp  (versus <i>Θ(n&#x2F;B)</i> of classic B-trees).<p>Note that the result applies to *any* distribution, as long as the mean and variance of the RVs modelling the gaps are finite. Indeed, we specialised our main result to some well-known distributions, such as Uniform, Lognormal, Pareto, Exponential, and Gamma (Corollary 1 of the paper).')