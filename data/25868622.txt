Item(by='fossuser', descendants=None, kids=[25868656], score=None, time=1611293116, title=None, item_type='comment', url=None, parent=25868477, text='AGI = Artificial General Intelligence, watch this for the main idea around the goal alignment problem: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=EUjc1WuyPT8" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=EUjc1WuyPT8</a><p>They&#x27;re explicitly <i>not</i> political, lesswrong is a website&#x2F;community and rationality is about trying to think better by being aware of normal cognitive biases and correcting for them. Also trying to make better predictions and understand things better by applying Bayes&#x27; theorem when possible to account for new evidence: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bayes%27_theorem" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bayes%27_theorem</a> (and being willing to change your mind when the evidence changes).<p>It&#x27;s about trying to understand and accept what&#x27;s true no matter what political tribe it could potentially align with. See: <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;rationality" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;rationality</a><p>For more reading about AGI:<p>Books:<p>- Superintelligence (I find his writing style somewhat tedious, but this is one of the original sources for a lot of the ideas): <a href="https:&#x2F;&#x2F;www.amazon.com&#x2F;Superintelligence-Dangers-Strategies-Nick-Bostrom&#x2F;dp&#x2F;1501227742" rel="nofollow">https:&#x2F;&#x2F;www.amazon.com&#x2F;Superintelligence-Dangers-Strategies-...</a><p>- Human Compatible: <a href="https:&#x2F;&#x2F;www.amazon.com&#x2F;Human-Compatible-Artificial-Intelligence-Problem&#x2F;dp&#x2F;0525558632&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.amazon.com&#x2F;Human-Compatible-Artificial-Intellige...</a><p>- Life 3.0, A lot of the same ideas, but the other extreme of writing style from superintelligence makes it more accessible: <a href="https:&#x2F;&#x2F;www.amazon.com&#x2F;Life-3-0-Being-Artificial-Intelligence&#x2F;dp&#x2F;1101970316&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.amazon.com&#x2F;Life-3-0-Being-Artificial-Intelligenc...</a><p>Blog Posts:<p>- <a href="https:&#x2F;&#x2F;intelligence.org&#x2F;2017&#x2F;10&#x2F;13&#x2F;fire-alarm&#x2F;" rel="nofollow">https:&#x2F;&#x2F;intelligence.org&#x2F;2017&#x2F;10&#x2F;13&#x2F;fire-alarm&#x2F;</a><p>- <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;tag&#x2F;artificial-general-intelligence" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;tag&#x2F;artificial-general-intelligenc...</a><p>- <a href="https:&#x2F;&#x2F;www.alexirpan.com&#x2F;2020&#x2F;08&#x2F;18&#x2F;ai-timelines.html" rel="nofollow">https:&#x2F;&#x2F;www.alexirpan.com&#x2F;2020&#x2F;08&#x2F;18&#x2F;ai-timelines.html</a><p>The reason the groups overlap a lot with AGI is that Eliezer Yudkowsky started less wrong and founded MIRI (the machine intelligence research institute). He&#x27;s also formalized a lot of the thinking around the goal alignment problem and the existential risk of discovering how to create an AGI that can improve itself without first figuring out how to align it to human goals.<p>For an example of why this is hard: <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;4ARaTpNX62uaL86j6&#x2F;the-hidden-complexity-of-wishes" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;4ARaTpNX62uaL86j6&#x2F;the-hidden...</a> and probably the most famous example is the paperclip maximizer: <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;tag&#x2F;paperclip-maximizer" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;tag&#x2F;paperclip-maximizer</a>')