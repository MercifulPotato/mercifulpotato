Item(by='bob1029', descendants=None, kids=[25425885], score=None, time=1607994188, title=None, item_type='comment', url=None, parent=25425218, text='I sometimes wonder how many of us are running infrastructure across multiple nodes in support of an application that can practically be ran on a single instance.<p>For us, having production go down for 2~5 minutes while we spin up a prior VM snapshot is totally acceptable for our customers. I have a hard time believing this RTO does not exceed what would be meaningfully required for most businesses (i.e. actual $$$ impact vs someone&#x27;s paranoid fantasies).<p>I totally concur on your 10x point. If you find your engineering staff arguing over containerization technologies or multi-cloud event-driven virtual actor architectures, you are probably wasting a lot of time. I would state that simply making the arbitrary decision to use containers is a massive mistake. All that overhead and complexity for what? You better have a damn good reason. Maintaining a typical Dev&#x2F;QA&#x2F;Staging&#x2F;Prod stack is not sufficient justification. Unless you seriously fucked something up, your software should be able to be cloned+built+deployed+started in a few lines of powershell, et. al. What is stopping you from running this script on 4 servers or writing a little tool to do it from a web interface every time you click a button? Oh right, can&#x27;t write your own CI&#x2F;CD tools when there are so many on the market. So it goes. Down the rabbit hole and to the right.<p>For those who can handle a 2-5 minute RTO, and have business application performance requirements that can be addressed by a single 32+ core server, there is no reason you should be screwing around with anything beyond your basic language+framework tooling, SQLite, source control, and project management tools. This is an engineering paradise if your constraints allow for it. I would never squander this opportunity with shiny bullshit.')