Item(by='mjb', descendants=None, kids=None, score=None, time=1604417431, title=None, item_type='comment', url=None, parent=24979542, text='&gt; While it increases the data locality, I have seen a few software following this sharding model (notably scylla) that work really bad once the load is not evenly distributed across all shards<p>Serial access to hot keys is a hard thing to design around, and you&#x27;re right that sharding doesn&#x27;t solve that problem. Worse, it exposes other keys that just happen to share the shard (or the core) to poor performance.<p>There are a couple of well-understood solutions to this problem. The obvious one is to dynamically re-balance shards based on heat, either moving some keys or split&#x2F;merge. This is the same tactic that many distributed databases use, and while it&#x27;s complex to do, it&#x27;s easier to do locally than distributed. Another option is stochastic re-balancing, like the Stochastic Fairness Queuing (<a href="https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;document&#x2F;91316" rel="nofollow">https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;document&#x2F;91316</a>) model used in networking. Here, shards are randomly re-shuffled occasionally. Doesn&#x27;t fix the noisy neighbor problem, but does mean that the noisyness moves around. That might seem silly, but it&#x27;s pretty much what&#x27;s going to happen under the covers of the non-sharded version of the code when the scheduler gets involved, only easier to reason about.<p>&gt; When that happens it can be a huge waste of resources and can give lower performance (depending on the type of load)<p>Lower apparent performance for neighbors of the heavy hitter, sure. Under which other circumstances does it reduce performance?<p>&gt; Imho unless you are absolutely sure about the type of load, leave the sharding to dividing data between servers, or have some mechanism that can shift to sharing the load between threads if the system imbalance is too great<p>I&#x27;m a bit puzzled by this. Distributed systems have exactly the same problem, and solving that problem is much harder there because the cost of contention is higher, data movement is more expensive, and you have to deal with a lot more failure cases.<p>The statistics may be better for distributed systems because a hot tenant has to be a lot hotter to make hot box than a hot core. But that&#x27;s a very specific kind of bet, and if you end up with a tenant that does cause a hot box you have an even harder problem to solve.<p>Dynamo (<a href="https:&#x2F;&#x2F;www.allthingsdistributed.com&#x2F;files&#x2F;amazon-dynamo-sosp2007.pdf" rel="nofollow">https:&#x2F;&#x2F;www.allthingsdistributed.com&#x2F;files&#x2F;amazon-dynamo-sos...</a>) solves this by moving the key space around, as do many similar kinds of systems. It&#x27;s not easy, though, and filled with caveats. If you&#x27;re scared of sharding, distributed sharding should be scarier than on-box sharding.')