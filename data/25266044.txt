Item(by='dragontamer', descendants=None, kids=[25267675], score=None, time=1606836990, title=None, item_type='comment', url=None, parent=25263331, text='C++&#x27;s &quot;seq_cst&quot; model is simple. If you&#x27;re having any issues understanding anything at all, just stick with seq_cst.<p>If you want slightly better performance on some processors, you need to dip down into acquire-release. This 2nd memory model is faster because of the concept of half-barriers.<p>Lets say you have:<p><pre><code>    a();\n    b();\n    acquire_barrier(); &#x2F;&#x2F; Half barrier\n    c();\n    d();\n    e();\n    release_barrier(); &#x2F;&#x2F; Half barrier\n    f(); \n    g();\n</code></pre>\nThe compiler, CPU, and cache is ALLOWED to rearrange the code into the following:<p><pre><code>    acquire_barrier(); &#x2F;&#x2F; Optimizer moved a() and b() from outside the barrier to inside the barrier\n    a();\n    b();\n    d();\n    c();\n    e();\n    g();\n    f();\n    release_barrier(); &#x2F;&#x2F; Optimizer moved g() and f() from outside the barrier to inside the barrier\n</code></pre>\nYou&#x27;re allowed to move optimizations &quot;inside&quot; towards the barrier, but you are not allowed to rearrange code &quot;outside&quot; of the half-barrier region. Because more optimizations are available (for the compiler, the CPU, or the caches), half-barriers execute slightly faster than full sequential consistency.<p>----------<p>Now that we&#x27;ve talked about things in the abstract, lets think about &quot;actual&quot; code. Lets say we have:<p><pre><code>    int i = 0; &#x2F;&#x2F; a();\n    i++; &#x2F;&#x2F; b();\n\n    full_barrier(); &#x2F;&#x2F; seq_cst barrier\n\n    i+=2; &#x2F;&#x2F; c();\n    i+=3; &#x2F;&#x2F; d();\n    i+=4; &#x2F;&#x2F; e();\n\n    full_barrier(); &#x2F;&#x2F; seq_cst barrier\n\n    i+=5; &#x2F;&#x2F; f();\n    i+=6; &#x2F;&#x2F; g();\n</code></pre>\nAs the optimizer, you&#x27;re only allowed to optimize to...<p><pre><code>    int i = 1; &#x2F;&#x2F; a() and b() rearranged to the same line\n    full_barrier(); &#x2F;&#x2F; Not allowed to optimize past this line\n    i+= 9; &#x2F;&#x2F; c, d, and e rearranged\n    full_barrier();\n    i+= 11; &#x2F;&#x2F; f, g rearranged.\n</code></pre>\nNow lets do the same with half barriers:<p><pre><code>    int i = 0; &#x2F;&#x2F; a();\n    i++; &#x2F;&#x2F; b();\n\n    acquire_barrier(); &#x2F;&#x2F; acquire\n\n    i+=2; &#x2F;&#x2F; c();\n    i+=3; &#x2F;&#x2F; d();\n    i+=4; &#x2F;&#x2F; e();\n\n    release_barrier(); &#x2F;&#x2F; release\n\n    i+=5; &#x2F;&#x2F; f();\n    i+=6; &#x2F;&#x2F; g();\n</code></pre>\nBecause all code can be rearranged to the &quot;inside&quot; of the barrier, you can simply write:<p><pre><code>   i = 21;\n</code></pre>\nTherefore, half-barriers are faster.<p>----------<p>Now instead of the compiler rearranging code: imagine the L1 cache is rearranging writes to memory. With full barriers, the L1 cache has to write:<p><pre><code>    i = 1;\n    full_barrier(); &#x2F;&#x2F; Ensure all other cores see that i is now = 1;\n\n    i = 10; &#x2F;&#x2F; L1 cache allows CPU to do +2, +3, and +4 operations, but L1 &quot;merges them together&quot; and other cores do NOT see the +2, +3, or +4 operations\n\n    full_barrier(); &#x2F;&#x2F; L1 cache communicates to other cores that i = 10 now;\n\n    i = 21; &#x2F;&#x2F; L1 cache allows CPU to do +5 and +6 operations\n\n   &#x2F;&#x2F; Without a barrier, L1 cache doesn&#x27;t need to tell anyone that i is 21 now. No communication is guaranteed.\n</code></pre>\n----------<p>Similarly, with half-barriers instead, the L1 cache&#x27;s communication to other cores only has to be:<p><pre><code>    i = 21; &#x2F;&#x2F; L1 cache can &quot;lazily&quot; inform other cores, allowing the CPU to perform i+=1, i+=2... i+=6.\n</code></pre>\nSo for CPUs that implement half-barriers (like ARM), the L1 cache can communicate ever so slightly more efficiently, if the programmer specifies these barriers.<p>----------<p>Finally, you have &quot;weak ordered atomics&quot;, which have no barriers involved at all. While the atomics are guaranteed to execute atomically, their order is completely unspecified.<p>There&#x27;s also consume &#x2F; release barriers, which no one understands and no compiler implements. So ignore those. :-) They&#x27;re trying to make consume&#x2F;release easier to understand in a future standard... and I don&#x27;t think they got all the &quot;bugs&quot; out of the consume&#x2F;release standard yet.<p>-------<p>EDIT: Now that I think of it, acquire_barriers &#x2F; release_barriers are often baked into a load&#x2F;store operation and are &quot;relative&quot; to a variable. So the above discussion is still inaccurate. Nonetheless, I think its a simplified discussion to kinda explain why these barriers exist and why programmers were driven to make a &quot;more efficient barrier&quot; mechanic.')