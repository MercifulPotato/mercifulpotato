Item(by='radiowave', descendants=None, kids=[25165121], score=None, time=1605902384, title=None, item_type='comment', url=None, parent=25163460, text='&gt; Maybe a small recordsize can have some benefits when overwriting parts of the files...mmmhhh...?<p>Right. People who&#x27;ve done more testing than me reckon on 16KB being a good record size for transaction-processing database work, where tables are seeing lots of small inserts and updates. (You might think matching the database&#x27;s block size would be ideal, e.g. Postgres writes 8KB at a time, but the rationale here is that you tend to get better compression at 16KB recordsize than 8KB, and the benefit from this outweights the write-amplification.)<p>But if database update performance isn&#x27;t a big deal for you then you can probably just ignore this.<p>I&#x27;ve not done any testing of my own at the 1MB size, but I don&#x27;t think I&#x27;d be inclined to try it unless I was fairly confident that there weren&#x27;t going to be many small writes to big files.<p>In short: use the large recordsize where you think you&#x27;ve got a good case for it, and likewise with a small record size. Otherwise, just stick with the default.')