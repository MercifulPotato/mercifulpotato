Item(by='anarazel', descendants=None, kids=None, score=None, time=1603818881, title=None, item_type='comment', url=None, parent=24907039, text='&gt; Think you&#x27;re mixing up parallelism and concurrency. Other databases with connections that support pipelining can easily get to high concurrency even with a single core. The key is the cpu can kick off 100 queries to a single connection and just wake up when a response is available.<p>Which RDBMS implement a protocol with tagged queries? I don&#x27;t think that&#x27;s common. From my, quite fallible, memory, the DB protocols supporting pipelining all just do in-order query execution. As I mentioned before, postgres <i>does</i> support that.<p>The TCP connection piece of a database connection isn&#x27;t really expensive - that&#x27;s the transaction and query execution state. These days (used to be different) it&#x27;s not that hard to write an application handling 10s-100s of thousands of TCP connections, as long as most of those aren&#x27;t continually sending data.<p>Obviously that&#x27;s where the process-per-connection limitations for postgres come in - having that several 100k processes isn&#x27;t really a good option. Not even just because of the process overhead, but even just the context switch and process scheduler overheads become more and more debilitating.<p>But even if postgres&#x27; connection model were switched to many-connections-per-process&#x2F;thread - you still need to have the per-connection state somewhere; obviously transactional semantics need to continue to work. And the per-connection transaction state is where the snapshot scalability limitation the article is talking about was. So the snapshot scalability issue needed to be fixed before a connection model switch really made sense.')