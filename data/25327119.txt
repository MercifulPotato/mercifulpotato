Item(by='xiphias2', descendants=None, kids=None, score=None, time=1607292786, title=None, item_type='comment', url=None, parent=25325624, text='Actually self attention is a generalization of convolution:<p><a href="https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id=HJlnC1rKPB" rel="nofollow">https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id=HJlnC1rKPB</a><p>,,This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention\nlayer with sufficient number of heads is at least as expressive as any convolutional\nlayer. Our numerical experiments then show that self-attention layers attend to\npixel-grid patterns similarly to CNN layers, corroborating our analysis.&#x27;&#x27;')