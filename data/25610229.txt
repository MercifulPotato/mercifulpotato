Item(by='zackmorris', descendants=None, kids=None, score=None, time=1609559561, title=None, item_type='comment', url=None, parent=25605853, text='Ya I hear you, I started with C++ and dabbled in 68000 assembly back in the early 90s. The biggest problems back then was the main loop and that a program could only get to a million lines before it was too unstable and crashed. Even just the protected memory of Java, or protected memory in general, felt like a distant dream.<p>Now we have event-based apps but we&#x27;re also stuck in the purgatory of async hell. Hundreds of classes got replaced with hundreds of factories and having to endlessly learn DSLs. Compilers mutated into handwritten unit tests. I feel a heaviness in my chest just writing this, because one thought leads to another and it&#x27;s hard to articulate the root of the discontent. I still believe that the web way is better than bare metal, but I&#x27;m saddened to see it reinventing the bad habits that we abandoned 25 years ago.<p>For me, what&#x27;s really going on is that computers have not gotten appreciably faster since about the time that PowerPC iMacs were running OS X, roughly the year 2000. Before that, computers were getting 100 times faster every decade, and then it just kinda.. stopped. Only video games kept going, the only tradeoff being that we have to use someone else&#x27;s 3D library rather than just writing ray tracers in a few pages of code (if only we had general-purpose multicore chips).<p>And that made programmers desperate, because they were still focusing on performance instead of stepping back and seeing the high-level abstractions that were largely understood by the 1980s. Everyone is so used to being compute-bound that we can&#x27;t even think about solutions outside of that reality.<p>My &quot;idea&quot; to fix all this, bluntly, is to forget about improving single-threaded performance and start giving people the raw computing power they need to get back to work again. To keep up with Moore&#x27;s Law, that&#x27;s a computer with 10,000 parallel threads running at least 1 GHz, for $1000. Or 100 times the cores every decade. My initial mention of 1000 cores was perhaps conservative.<p>Pretty much all of the problems we deal with today are embarrassingly parallel. A synchronous blocking PHP page is, when served to thousands of users. DSP is. Neural nets, genetic algorithms, stocks, Bitcoin..<p>So our desktop machines should really be thousands of Docker containers with a total capacity of like 1000000%. No program would ever block another program, or get out of its sandbox. Programs would sometimes run across the internet. I picture it kind of like this big Minecraft Disneyland where you&#x27;re in VR but processing stuff in the background and forgetting it&#x27;s there. Maybe you&#x27;d devote 50% to an AI agent like J.A.R.V.I.S. that sits around all day backing up its best self and evolving its subprocesses to be even better. Not being compute-bound is like being able to throw processing power at problems declaratively and never having to solve anything menial again. I&#x27;ve been daydreaming about all this since like 1999 hahaha.<p>The math is all there, I&#x27;ve written about it at length in previous comments. You basically take an old processor like MIPS that was about as optimized for single-threaded performance as one can get, without getting mired in the evolutionary dead end of long pipelines and huge caches. That or the PowerPC 601 or DEC Alpha had on the order of 1-3 million transistors. Looks like the Apple M1 has 16 billion. So the raw numbers do suggest 10,000 of last-century&#x27;s best cores. Then spend another 10 billion transistors for about 1-10 GB of RAM, or 1 MB of ram in-core.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Transistor_count" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Transistor_count</a><p>Yes, memory routing is a pain, but you just use content-addressable memory and treat the interconnect just like any other network on the internet. The cores use caching, compression and copy-on-write to combine the best aspects of Erlang, Lisp, and Clojure. We&#x27;d write code in a Javascript-like language that&#x27;s identical to the ideas from Immer and Immutable.js, but natively. Since everything is read-only, it statically compiles ahead to the best of its ability, running through a monad only when processing mutable state, and then going back to static. When you&#x27;re not compute-bound, the static stuff processes instantly. It basically inverts the logic, so the only slow part of your code is the IO. My terminology isn&#x27;t exact here, but it would basically transpile a subset of Javascript to Lisp and then run the embarrassingly parallel stuff 10,000 times faster than we&#x27;re used to, rivaling the speed of video cards.<p>To do this as a hobby project, it might be fun to see how many early ARM cores and small RAMs could fit on a chip with a billion transistors. Then see how hard it is to add the content-addressable networking to the OS. Then finally get 1000 Docker containers running with, say, Debian. I used to daydream about doing it on an FPGA, but haven&#x27;t kept up as closely as I&#x27;d like. Also I feel like there is industry pressure to keep FPGAs down, because they haven&#x27;t kept up with Moore&#x27;s law either, and never went fully open source like they should have.<p>I feel kind of weary about all this because I&#x27;ve been thinking about it for so long, and have a lot of regrets about not using my degree more. I&#x27;m still just writing CRUD apps like everyone else. It&#x27;s so tedious, and takes so much code to do so little visible work, that I&#x27;ve lost seasons.')