Item(by='leecarraher', descendants=None, kids=[25641297], score=None, time=1609779858, title=None, item_type='comment', url=None, parent=25632592, text='I suspect we will not see a revolution in deep learning from GNN without a corresponding hardware(FPGA?) or optimization method(HSIC? HTMs?) advance.<p>GNNs trained by backprop are making many of the same mistakes that LSTMs did: solve one important problem(exploding&#x2F;vanishing gradients), but introduce a bunch of hyperparameters that bring along their own set of problems. GRUs are successful in my opinion, because they remove some of those tunable parameters.<p>Of course as the post suggested, not being able to tune something, often loses out against some more tuned and curated solution. GNNs being the newest version, having tons of parameters to tweek. In the end do we get a better, ultimately more generalizable solution? Or do we just get more hyperparameters to tune, and spend more time and money for a modest, unrepeatable gain.')