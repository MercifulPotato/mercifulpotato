Item(by='Veedrac', descendants=None, kids=None, score=None, time=1604931931, title=None, item_type='comment', url=None, parent=25033552, text='&gt; Rather, it&#x27;s Inductive Programming and more specifically Inductive Logic Programming (ILP), which is learning programs from examples, i.e. &quot;incomplete specifications&quot;. I&#x27;m not familiar with the General Program Synthesis Benchmark Suite, but the problem you list (test three strings are ordered by length) is trivial for ILP approaches.<p>The General Program Synthesis Benchmark Suite works from input-output examples, not “complete, formal specifications”.<p>How would you tackle this with ILP?<p>&gt; However, I have to say that even so, if something is a difficult problem for program synthesis approaches, then it&#x27;s very unlikely that neural networks will do any better at it. For instance, do you know how well deep neural nets perform on this benchmark?<p>I&#x27;m not aware of any serious at-scale attempts. Your option is basically to try few-shot with GPT-3.<p>OTOH, learning these trivial programs from 100 examples is a largely artificial framing used to support a field which hadn&#x27;t worked its way up to meaningful problems, and in the more general sense, large networks are promising; eg. the GitHub-trained GPT:<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=y5-wzgIySb4" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=y5-wzgIySb4</a><p>or any of the GPT-3 programming demos:<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;sharifshameem&#x2F;status&#x2F;1284103765218299904" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;sharifshameem&#x2F;status&#x2F;1284103765218299904</a>\n<a href="https:&#x2F;&#x2F;twitter.com&#x2F;sharifshameem&#x2F;status&#x2F;1284815412949991425" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;sharifshameem&#x2F;status&#x2F;1284815412949991425</a>\n<a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;commandline&#x2F;comments&#x2F;jl8jyr&#x2F;the_nlc2cmd_challenge_site_has_a_gpt3powered&#x2F;gaop9ji&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;commandline&#x2F;comments&#x2F;jl8jyr&#x2F;the_nlc...</a><p>&gt; These approaches are still state of the art for their respective tasks and there is no other approach that has been shown to do any better, including deep neural networks. In what sense are they &quot;no longer more than briefly and tangentially relevant&quot; as you say?<p>“if you&#x27;re interested in learning <i>AI</i>”<p>These techniques were invented from the field of AI, but that does not mean they remain in the field of AI.<p>&gt; You clearly have a strong opinion on GOFAI and the AI winter of the &#x27;80s, but what knowledge does this opinion come from? Can you say?<p>I can argue why ML approaches are good and promising and point at that. I can argue why ML approaches make conceptual sense whereas GOFAI does not, though I don&#x27;t see us resolving that short-term so I&#x27;d rather not. But what I can&#x27;t so easily do is point to the non-existence of GOFAI AI successes. It&#x27;s just not there.<p>&gt; the ability for reasoning (despite big claims to the contrary)<p>The nebulousness of the term ‘reasoning’ is pulling a lot of weight here. It&#x27;s clearly doing sophisticated computations of some sort, beyond brute memorization.<p>&gt; or arithmetic (ditto)<p><a href="http:&#x2F;&#x2F;gptprompts.wikidot.com&#x2F;logic:math#toc6" rel="nofollow">http:&#x2F;&#x2F;gptprompts.wikidot.com&#x2F;logic:math#toc6</a><p>There are more examples too, this is just addressing the one point people get wrong most often. BPEs are an interim performance hack, not an indictment on the approach in general.<p>&gt; or generation of novel programs<p>Is clearly false.<p>&gt;  For instance, the append() example you show above is clearly memorised: you haven&#x27;t given the model any examples of append(), so it can&#x27;t possibly learn its definition from examples.<p>This is true, but it&#x27;s mostly just an artifact of me having to prompt it through FitnessAI. Unlike smaller models, few-shot learning works, it just takes more space than I have to prompt with.<p>See the GitHub-trained example for something that integrates with more arbitrary code. There are many other examples, like the database prompt below (all bold is human input), or see some of the examples I linked above.<p><a href="https:&#x2F;&#x2F;www.gwern.net&#x2F;GPT-3#the-database-prompt" rel="nofollow">https:&#x2F;&#x2F;www.gwern.net&#x2F;GPT-3#the-database-prompt</a><p>Or I can ask<p>Q: “If z(str) = str + &quot; &quot; + str + &quot; z&quot; (for example, z(&quot;dumbell&quot;) = &quot;dumbell dumbell z&quot;), and g(str) = &quot;k &quot; + str + &quot; j&quot; then what is g(&quot;run&quot;)?”<p>A: “g(&quot;run&quot;) = &quot;k run j&quot;”<p>&gt; btw, why do you need to give it the list &quot;a&quot;? What happens if this is ommitted from the prompt?<p>That example was from me trying to emulate an example I saw on Twitter I&#x27;ve since lost, which was a similar thing but multi-step, where each step GPT-3 returned all three lists, modified or queried per the given commands.<p>Omitting `a`, I get<p>Q: “b = [&quot;lifting&quot;, &quot;curls&quot;, &quot;squats&quot;], c = [&quot;running&quot;, &quot;jogging&quot;], so what is b after b.append(&quot;pushups&quot;)?”<p>A: “lifting,curls,squats,pushups”<p>I had to change the prompt a bit because initially the result was truncated (FitnessAI is not made for this), or said “b.append(&quot;pushups&quot;) will add the string &quot;pushups&quot; to the end of b.”, which is correct but not what I wanted.<p>Few-shot would fix formatting inconsistencies; right now the model is just guessing.')