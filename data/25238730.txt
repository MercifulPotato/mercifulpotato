Item(by='jbeam', descendants=None, kids=[25238863], score=None, time=1606583131, title=None, item_type='comment', url=None, parent=25237356, text='Very interesting article that cuts to the core of many of the issues with pricing insurance products. While the phrase is hardly limited to the actuarial world, &quot;all models are wrong, but some are useful&quot; is definitely an extension of this.<p>The fluid nature of probability is the center of the insurance universe. Probability is always a moving target in the insurance world. Indeed, if it weren&#x27;t, there wouldn&#x27;t be much of a need for actuaries. Much of actuarial training revolves around the idea of credibility -- how credible is your sample set, what alterations should you make to old data to make it relevant to today, and what data should you add to it as a complement in order to relieve the model of the biases inherent in your sample size. This is inherently Bayesian in it&#x27;s approach.<p>Where it truly gets interesting is that insurance companies are very cognizant of tail risk -- the 1-in-100, 1-in-250, 1-in-500 events that can cause insurer insolvency if not properly accounted for. You can survive a miscalculated loss trend within reasonable bounds, but if you haven&#x27;t thought about the potential Cat 5 hurricane that hits Miami-Dade then you are going to have some very unhappy investors. When it comes to these types of events, you mostly need to be in the right ballpark. The order of magnitude matters more than the exact number -- albeit the exact number matters quite a bit for regulatory reasons. This type of calculation for property lines has largely been outsourced to the stochastic models developed by companies such as AIR and RMS. A sudden change in their models, which I think is likely after this record breaking hurricane season, can inflict capital pressure on the industry almost instantly.[1]<p>There are some actuarial papers from around 50 years ago that discuss information entropy as another way to approach the issue of constructing probability models, but they never really caught on. It seems that is likely due to the lack of widespread computing power. I&#x27;m hoping these ideas can gain some steam now that we can construct some of these distributions from Python and R.<p>[1] There is a fantastic article by Michael Lewis that describes this issue at great detail: <a href="https:&#x2F;&#x2F;www.nytimes.com&#x2F;2007&#x2F;08&#x2F;26&#x2F;magazine&#x2F;26neworleans-t.html" rel="nofollow">https:&#x2F;&#x2F;www.nytimes.com&#x2F;2007&#x2F;08&#x2F;26&#x2F;magazine&#x2F;26neworleans-t.h...</a>')