Item(by='kevinstumpf', descendants=None, kids=None, score=None, time=1603343007, title=None, item_type='comment', url=None, parent=24849507, text='Kevin here, Tecton&#x27;s CTO. Great question:<p>With Tecton, transformations are an optional component of the system. Similar to Feast, you can bypass the Transform component to ingest data directly from external pipelines. You typically do this when you have existing data pipelines and you want to make the values available in a Feature Store.\nHowever, if you don&#x27;t have an existing stream &#x2F; batch data pipeline infrastructure that your data scientists &#x2F; data engineers can easily contribute to, a Feature Store&#x27;s Transform component is an easy way for them to be fully self-sufficient. Tecton makes it easy to express feature transformations using Spark&#x27;s native DataFrame API, Python, SQL or Tecton&#x27;s DSL.<p>Besides the self-sufficiency, there are a few other advantages you get from having a feature store manage your feature transformations:<p>- Feature Versioning: If you change a feature transformation, the Feature Store will know to increment the version of that feature and ensure that you don&#x27;t accidentally mix features that were computed using two different implementations<p>- End-to-end lineage tracking and reproducibility: If a feature store manages your transformation, it can tie exact feature definitions all the way through a training data set and a model that&#x27;s used in production. So, if years later you want to reproduce a model of a certain time in the past, a Feature Store that supports transformations would be able to recreate that model as long as the raw data still exists<p>- Trust: It&#x27;s more likely that a data scientist will trust and then reuse another user&#x27;s feature, if they can peek under the hood and see how the feature is actually calculated<p>- On-Demand Features: These transformations cannot be executed by existing data processing pipelines because they have to be computed in real-time when the prediction is made â€” which happens in the operational environment.<p>In reality, you will frequently see multi-stage data processing workflows in an organization: You will have a lot of data cleaning and preprocessing happening in an organization&#x27;s standard and ML-independent data processing infrastructure. Afterwards, a Feature Store will pick up and transform that preprocessed data and turn it into feature values.')