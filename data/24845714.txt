Item(by='teorema', descendants=None, kids=None, score=None, time=1603267373, title=None, item_type='comment', url=None, parent=24844897, text='It&#x27;s too bad you&#x27;re being downvoted. I had a similar reaction, and I think you&#x27;re on to something important.<p>Many adversarial cases are good examples of this: a DL model being completely thrown off by something very incidental, that a human would instantly recognize as not being within a class. Not just something a human would instantly recognize as not being within a class, but something a human would be perplexed by as an adversarial case.<p>The point isn&#x27;t that humans are better or worse, it&#x27;s that the models do often seem to be overfitting, but overfitting in a way that isn&#x27;t evident until the inputs are generalized beyond whatever is in the development samples. Put another way, they might be learning something about your development datasets more so than the actual features of interest, which is the whole idea of overfitting. It&#x27;s just that what it means to &quot;generalize&quot; is much broader.<p>It&#x27;s a really interesting piece but I think there&#x27;s lots more to the story.')