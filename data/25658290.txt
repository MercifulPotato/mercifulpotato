Item(by='calebkaiser', descendants=None, kids=None, score=None, time=1609945156, title=None, item_type='comment', url=None, parent=25657343, text='Not for the kind of inference running here, I&#x27;d imagine.<p>There are few key reasons why most realtime inference is done on the cloud:<p>- Scale. Deep learning models especially tend to have poor latency, especially as they grow in size. As a result, you need to scale up replicas to meet demand at a way lower level of traffic than you do for a normal web app. At one point, AI Dungeon needed over 700 servers to support just thousands of concurrent players.<p>- Cost. Related to the above, GPUs are really expensive to buy. A g4dn.xlarge instance (the most popular AWS EC2 instance for GPU inference) is $0.526&#x2F;hour on demand. To hit $3,000 per month in spend, you&#x27;d need to be running ~8 of them 24&#x2F;7. Prices vary with purchasing GPUs, but you could expect 8 NVIDIA T4&#x27;s to run around $20,000 at minimum, plus the cost of other components and maintainence. To be clear, that&#x27;s very conservative--it&#x27;s unlikely you&#x27;ll get consistent traffic. What&#x27;s more likely is you&#x27;ll have some periods of very little traffic where you need one or two GPUs, and other high load periods where you&#x27;ll need 10+.<p>3. Less universal of an issue, but the cloud gives you much better access to chips at lower switching costs. If NVIDIA releases a new GPU that&#x27;s even better for inference,  switching to it (once its available on your cloud) will be a tweak in your YAML. If you ever switch to ASICs like AWS&#x27;s Inferentia or GCP&#x27;s TPUs, which in many cases give way better performance and economics than GPUs, you&#x27;ll also naturally have to be on their cloud.<p>However, there is a lot that can be done to lower the cost of inference even in the cloud. I listed some things in a comment higher up, but basically, there are some assumptions you can make with inference that allow you to optimize pretty hard on instance price and autoscaling behavior.')