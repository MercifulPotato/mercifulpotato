Item(by='scalablenotions', descendants=None, kids=None, score=None, time=1607297064, title=None, item_type='comment', url=None, parent=25325294, text='&gt; humans can&#x27;t explain or understand how we drive, speak, translate, play chess, etc, so why should we expect to understand how models that do these work?<p>This implies there&#x27;s no point in pursuing explainability, but many domains involve inferences where the significant predictors are much easier to abstract at a useful level.<p>For example, if a DLNN could make suggestions as to how to tune a greenhouse given certain yield objectives, then it&#x27;s reasonable to pursue heuristic techniques aiming to explain what about the parameters most significantly led to the given suggestions.')