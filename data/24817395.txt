Item(by='pumpkinpal', descendants=None, kids=[24821043], score=None, time=1603024815, title=None, item_type='comment', url=None, parent=24809167, text='Its funny that you mentioned games as an example of parallel computation because I&#x27;d argue they&#x27;re some of the hardest programs to parallelize effectively since they don&#x27;t generally involve that much bulk-processing of read-only data.<p>Parellism in C++ is most often used for scientific applications and other forms of mass number crunching. It&#x27;s really easy to just throw a &quot;#pragma omp parallel for&quot; on a loop and call it a day but of course that would also apply to C and Fortran and is somewhat limited. Parallelism libraries like Intel TBB which I&#x27;m most familiar with are very easy to use and performant. I think there&#x27;s a large problem in the reluctance of educators to use libraries to teach parallelism and people always dive straight into locks, threads and atomics which are really not the way to approach parallel computing if you&#x27;re looking to do parallel computing and not looking to implement parallel primitives yourself (i.e DIY tasking-system or lock-free queue)<p>Focusing on TBB, it facilitates efficient parallelism by providing high-level canned algorithms such as parallel_invoke, parallel_reduce, parallel_for and parallel_do which anybody who claims to know C++ should be able to use easily. It also provides a task-graph which is great for more complex processing pipelines (things like join&#x2F;split, fan-in&#x2F;out and queueing). If you need more low level control you operate at the task level and TBB provides customization points for that. There&#x27;s other libraries out there which provide similar functionality and even the STL in C++17 provides basic parallel algorithms such as transform (equivalent of map in other langs), reduce and many others.')