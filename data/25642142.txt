Item(by='jedbrown', descendants=None, kids=[25645203], score=None, time=1609821161, title=None, item_type='comment', url=None, parent=25641566, text='Unless your data is at rest, you&#x27;re paying for bandwidth, not capacity. SRAM &gt; HBM &gt; DRAM &gt; NVMe when you&#x27;re buying bandwidth. Seriously, when we have a low-latency network, the most cost-effective way to run some simulations and graph algorithms is out of L3 on EPYC (where mid-tier parts have over 1 TB&#x2F;s at a fraction of the energy and cost of an A100&#x2F;MI100).<p>Your NVMe quotes 3.2 GB&#x2F;s, versus 25 GB&#x2F;s (theoretical) for a stick of DDR4-3200 (which is in the same price range). A standard 2-socket EPYC server can give you 300 GB&#x2F;s from DRAM, but you&#x27;d need several servers servers packed with NVMe to provide that bandwidth.<p>The rationale for persistent storage (like NVMe) as an algorithmic device is either (a) you need it to be persistent or (b) you have physical or algorithmic constraints that prevent you from using more parallelism and you&#x27;re willing to pay 5-10x for the privilege of executing from NVMe.')