Item(by='1vuio0pswjnm7', descendants=None, kids=None, score=None, time=1611688587, title=None, item_type='comment', url=None, parent=25918322, text='James Mickens on Javascript:<p><a href="https:&#x2F;&#x2F;vimeo.com&#x2F;111122950" rel="nofollow">https:&#x2F;&#x2F;vimeo.com&#x2F;111122950</a><p>Here&#x27;s a stupid, simple Vimeo downloader instead of a using the massive, slow starting youtube-dl<p><pre><code>  #!&#x2F;bin&#x2F;sh\n  # usage: curl https:&#x2F;&#x2F;vimeo.com&#x2F;123456789 | $0 \n  x=$(curl -s `grep -m1 -o https:&#x2F;&#x2F;player.vimeo.com&#x2F;video&#x2F;[^\\&quot;?]*|sed &#x27;s&gt;$&gt;&#x2F;config&gt;&#x27;`|grep -o https:&#x2F;&#x2F;[^\\&quot;]*mp4|sed -n \\$p)\n  y=${x%&#x2F;*};y=${y##*&#x2F;};exec curl -so $y.mp4 $x \n\n</code></pre>\nMore from Mickens:<p><a href="https:&#x2F;&#x2F;www.usenix.org&#x2F;legacy&#x2F;events&#x2F;webapps10&#x2F;tech&#x2F;full_papers&#x2F;Mickens.pdf" rel="nofollow">https:&#x2F;&#x2F;www.usenix.org&#x2F;legacy&#x2F;events&#x2F;webapps10&#x2F;tech&#x2F;full_pap...</a><p><a href="https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;1403_02-08_mickens.pdf" rel="nofollow">https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;1403_02-08_mickens.pdf</a><p>Unlike Mickens, I cannot save the world, and I am not telling anyone else what to do or not to do, but I made the web fast for myself.  Hence I am very skeptical of claims that &quot;the web is slow&quot;.  Web servers, the network and computers are plenty fast and still getting faster.  I do not define &quot;the web&quot; as certain popular browsers, CSS, Javascript, etc. or whatever web developer tell me it is.  Those are someone else&#x27;s follies.  I define it as hyperlinks (thus, a &quot;web&quot;)  and backwards compatible HTML.  Stuff that is reliable and always works.  To &quot;make the web fast&quot;, I follow some simple rules.  I only load resources from one domain, I forgo graphics, and I do not use big, complex, graphical, &quot;modern&quot; web browsers to make HTTP requests.<p>I do not even use wget or curl (only in examples on HN).  I generate the HTTP myself using software I wrote in C for that purpose and send using TCP clients others have written over the years.  There are so many of them.  With a &quot;modern&quot; SSL-enabled forward proxy or stunnel, they all work with today&#x27;s websites.  &quot;Small programs that do one thing well&quot;, as the meme goes.<p>Obviously, I still need the ever-changing, privacy-leaking, security risk-creating, eye-straining, power-consuming, time-wasting, bloated, omnibus browsers for any sort of serious transaction done with web, e.g., commerce, financial, etc.  However that is a small fraction of web use in my case.<p>For me, using the web primarily comprises searching, reading and downloading.  I never need Javascript for those tasks.  I can do those them faster without the popular broswers than I can with them.  The less interaction the better.  I use automation where I can because IMO that is what computers were made for.  &quot;The right tool for the job&quot;, as the meme goes.<p>To think how much time and energy (kwH) has been devoted to trying to make Javascript faster as a way to make websites faster is, well, I won&#x27;t think about it.  Those working in the &quot;software industry&quot; and now &quot;tech&quot; are highly adept at creating the problems they are trying solve.  Unfortunately today as we try to rely on software and the web for important things, we all have to suffer through that process with them.<p>By not using the popular browsers for a majority of web use, I have minimised the suffering of one user: me.  The web is fast.')