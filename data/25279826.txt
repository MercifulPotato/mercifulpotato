Item(by='hinkley', descendants=None, kids=None, score=None, time=1606934825, title=None, item_type='comment', url=None, parent=25277848, text='There&#x27;s a phenomenon I&#x27;ve witnessed in dev teams that  I always thought was a separate issue but I&#x27;m starting to question that conclusion.<p>Like with Covid, like with SLAs, it&#x27;s not enough to think of ratios of success or failure. You have to also look at frequency and duration.<p>A flaky test might fail a build 1 time in 200, but as your team gets bigger, build frequency rises, your tests grow, and eventually you&#x27;re getting failed builds frequently enough that people start to see them as a regular occurrence, and that negatively affects their opinions about the whole experience. I&#x27;ve seen people bash the system when failures happen weekly, I&#x27;ve seen others &#x27;turn&#x27; after a couple of statistical clusters and then fall to confirmation bias long afterward.<p>Google has so many irons in the fire that I think we&#x27;ve reached that same threshold for a lot of people. Shutting down the worst 1% of your projects a year sounds like a completely reasonable business plan. Until you have 1000 projects, and now you&#x27;re shutting one down every five weeks on average. People will talk.<p>And if there&#x27;s no transparency in that process, how do I know that my favorite tool isn&#x27;t next, or on the list for next year? Odds are low, but not zero.')