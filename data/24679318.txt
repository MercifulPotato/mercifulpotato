Item(by='dragontamer', descendants=None, kids=[24684734], score=None, time=1601821911, title=None, item_type='comment', url=None, parent=24676601, text='Surprisingly: no.<p>1. Cache-oblivious data-structures work no matter the cache-size. They aren&#x27;t 100% optimized to a particular cache size, but they are proven to be &quot;efficient&quot; across many different cache sizes.<p>2. For an example of #1, matrix multiplication is one of the most optimized functions of modern computing. One major step is to transpose the matrix, so that the &quot;vertical&quot; movement turns into a &quot;horizontal&quot; movement. The horizontal movement, which is &quot;address + 1&quot; allows for the cache to work with your loop, while a vertical movement across the matrix is &quot;address + width&quot;, and not cache-efficient.<p>------------<p>3. I think it is safe to start making some degree of cache-assumptions on modern systems. All caches today have 64-byte cache lines (or greater). That&#x27;s ARM, Intel, and AMD CPUs all use 64-byte cache lines. AMD GCN GPUs use 64-byte cache lines. AMD RDNA GPUs use 128-byte cache lines (and Intel is rumored to &quot;pair up&quot; cache lines into 128-byte segments at L3).<p>This means that every memory operation to main-memory reads or writes at least 64-bytes of data at a time, on all modern systems. DDR4 RAM itself has Burst-Length 8, meaning 64-bytes is literally the smallest unit of data that can be addressed, so it isn&#x27;t surprising that all CPUs &#x2F; GPUs are working with such a large number these days.<p>-----------<p>Given the 64-byte cache line (or 128-byte, for Intel&#x27;s L3 cache or AMD RDNA GPUs), &quot;unrolling&quot; your linked lists, or using B-trees of larger sizes (instead of the cache-inefficient binary tree), or d-heaps (instead of binary heaps) plays to the benefit of caches helps out a lot.<p>You don&#x27;t need to know the specific size of the cache line to know that d-heaps are more efficient than binary-heaps on modern, cache-heavy systems. You just need to know that sequential-memory access stays within the L1 cache.')