Item(by='sdenton4', descendants=None, kids=[25328126], score=None, time=1607291694, title=None, item_type='comment', url=None, parent=25326643, text='These things matter a lot in practice. Imagine a giant million dimensional loss surface, where each point is a set of weights for the model. Then the gradient is pushing us around on this surface, trying to find a &#x27;minimum.&#x27; Current understanding (for a while, actually) is that you never really hit minima so much as giant mostly-flat regions where further improvement maybe takes a million years. The loss surfaces for models with skip connections seem to be much, much nicer.<p><a href="https:&#x2F;&#x2F;papers.nips.cc&#x2F;paper&#x2F;2018&#x2F;file&#x2F;a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf" rel="nofollow">https:&#x2F;&#x2F;papers.nips.cc&#x2F;paper&#x2F;2018&#x2F;file&#x2F;a41b3bb3e6b050b6c9067...</a><p>In effect, there&#x27;s a big gap between an existence proof and actually workable models, and the tricks of the trade do quite a lot to close the gap. (And there are almost certainly more tricks that we&#x27;re still not aware of! I&#x27;m still amazed at how late in the game batch normalization was discovered.)<p>OTOH, so long as you&#x27;re using the basic tricks of the trade, IME architecture doesn&#x27;t really matter much. Our recent kaggle competition for birdsong identification was a great example of this: pretty much everyone reported that the difference between five or so &#x27;best practices&#x27; feature extraction architectures (various permutations of resnet&#x2F;efficientnet) was negligible.')