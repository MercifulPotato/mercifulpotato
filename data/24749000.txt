Item(by='tylerhou', descendants=None, kids=[24749108], score=None, time=1602449519, title=None, item_type='comment', url=None, parent=24748835, text='The other time that you saw it was also probably me. It&#x27;s from this talk, which is about how a large amount of generated protocol buffer code at Google led to a quadratic increase in compile times: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;rHIkrotSwcc?t=720" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;rHIkrotSwcc?t=720</a>.<p>TL;DW: The reasoning is because if you use a distributed build system, then your compile time is gated by the file with the longest compile time (tail latency). The more files you have, the greater chance that one of them takes a long time. When you generate source files, you tend to produce more files than if you didn&#x27;t.<p>Most users don&#x27;t use a distributed build system to compile the kernel, so on further thought, in that case compile times probably scale closer to linear with the number of translation units. But wasted cycles are still wasted cycles, and regardless of how exactly compile times scale, you should still consider the cost of longer compile times when you duplicate code.<p>With regards to link time optimization: sophisticated analyses take superlinear complexity: <a href="https:&#x2F;&#x2F;cs.stackexchange.com&#x2F;questions&#x2F;22435&#x2F;time-complexity-of-a-compiler" rel="nofollow">https:&#x2F;&#x2F;cs.stackexchange.com&#x2F;questions&#x2F;22435&#x2F;time-complexity...</a>.<p>Disclaimer: I work at Google.')