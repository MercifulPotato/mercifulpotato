Item(by='hnarn', descendants=None, kids=[25025400, 25025408], score=None, time=1604837958, title=None, item_type='comment', url=None, parent=25006829, text='&gt; perl -MList::Util=uniq -e &#x27;print uniq &lt;&gt;&#x27;<p>This one is interesting, because I&#x27;ve come across this issue many times where &quot;uniq&quot; will not work the way you perhaps expect it to (if you, like me, don&#x27;t read &quot;man&quot; first), and will give back unique values from lines later in the file multiple times. Normally I&#x27;ve found that the way to work around this is to sort the output first before passing it to uniq, so I was curious if the perl way was faster or not.<p>I wrote 10M random numbers between 1-3 to a text file and tried this benchmark:<p>&gt; time cat hn.txt | sort -n | uniq<p>real 0m3,929s<p>&gt; time cat hn.txt | perl -MList::Util=uniq -e &#x27;print uniq &lt;&gt;&#x27;<p>real 0m2,205s<p>Almost twice as fast. Lesson learned I guess, don&#x27;t knock on Perl for CLI wrangling :)<p>I can share one of my frequently used ones as well: if you have a log file with an initial timestamp that is UNIX epoch, you can pipe the text to this perl command to convert it to localtime:<p>&gt; cat logfile | perl -pe &#x27;s&#x2F;(\\d+)&#x2F;localtime($1)&#x2F;e&#x27;<p>(I&#x27;m sure you could do this with something other than Perl as well, but it does the job well)')