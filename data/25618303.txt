Item(by='luizfelberti', descendants=None, kids=[25619348, 25619331, 25625104], score=None, time=1609635495, title=None, item_type='comment', url=None, parent=25616994, text='I think ternary computers are an interesting thought excercise, but I&#x27;ve heard on multiple occasions people making serious comments about &quot;ternary computers would be a revolution in computing&#x2F;efficiency&#x2F;&lt;something&gt;&quot;, and it always sounded like crackpot speculation to me.<p>I&#x27;d be interested in hearing a sophisticated counter-argument if anybody has a good one. Here&#x27;s my take on this:<p>1. Binary is that smallest base in which we can encode Shannon information, and thus yields to &quot;bits&quot; a more elegant foundation for computation. Choosing any other base sounds like it would be a very arbitrary choice, and to the best of my knowledge, the only other canonical base for computation that <i>inherently</i> makes sense (as in, not arbitrary) aside from the `bit` is the `nat`[0], but since it uses Euler&#x27;s constant (a transcendental number) it would be (ironically) very unnatural to express &quot;computation&quot; in it.<p>2. By theorem, all bases are equivalent in power with respect to their capabilities of encoding information and processing&#x2F;computing said information, and binary logic can be used to bootstrap any other multi-valued logic system desirable (goes for ternary logic and 64-bit integer computation alike). Thus, the decision to make a base-3 falls back pretty much entirely on circuit efficiency ...<p>3. The claims of computational efficiency, as far as I&#x27;ve been able to observe, are fallaciously constructed by studying their performance characteristics in abstract computer models that are completely detached from computational thermodynamics:<p>3a. Turing-machines, for example, fail to adequately model real-world constraints due to information locality being one-dimensional (while the real world is 3D). Von-Neumann computers (and other RAM based machine models) are IMO the largest infractor of this â€” there is no such thing as &quot;RAM&quot; in the real world: you can&#x27;t access any address space irrespective of locality in O(1), such a computer would violate relativistic physics and thermodynamical constraints, RAM can only exist by making all memory access equally inneficient and mitigating that as much as possible with L3 caches and what not. Semiconductor engineers and chip designers inherently understand this and are aware of die-locality, most computer scientists and software engineers are completely oblivious to this and get to ignore it most of the time [1]<p>3b. This means that even though anyone can theorize an abstract machine where every discrete &quot;tick&quot; of time will perform a ternary operation in constant time, that doesn&#x27;t mean such a machine can be efficiently reified if you move it out of the realm of abstraction. We could just as easily theorize a computer that uses base-1T (one trillion), and assume that all &quot;trilliernary&quot; operations execute instantly in discrete ticks of O(1), and voila! We have a computation device that can rival all of Google&#x27;s data-centers!<p>3c. In the real world however, there are thermodynamic constraints on how information can &quot;flow&quot; across spacetime, and I have seen absolutely zero evidence that a ternary, trilliernary, or whatevernary logic gate can be constructed without introducing many new voltage levels or other circuit fuckery to counteract this, in a way that wouldn&#x27;t completely wreck any purported efficiency. It&#x27;s either that, or we&#x27;d implement whatevernary logic using binary gates (which is exactly what 64-bit integer arithmetic already is!)<p>4. Bottom line and TLDR is this: You can&#x27;t get more computation[2] for free. Welcome to the real world.<p>---<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nat_(unit)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nat_(unit)</a><p>[1] This is especially painful to watch when they are forced to deal with it, and these optimistic-assumptions clash with reality, as evidenced by NUMA bottlenecks wrt RAM&#x2F;RDMA&#x2F;GPUs&#x2F;Disk access, a collective failure of most engineers to understand the CAP theorem or other concepts related to networking, and so on...<p>[2] And by &quot;computation&quot; I mean it in the thermodynamical, relativistically-physical, information-theoretical kind of way. Physics is a thing we forget about way too often in computer science...')