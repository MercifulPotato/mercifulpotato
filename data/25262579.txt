Item(by='root_axis', descendants=None, kids=None, score=None, time=1606799889, title=None, item_type='comment', url=None, parent=25258542, text='&gt; <i>Which gets into the concept of whether the ML model has actually learned some deeper conceptual ideas than we have, some deeper truth about how this works.</i><p>Well I think that the results speak for themselves; ultimately the question you raise is one of semantics. ML models don&#x27;t think in terms of &quot;conceptual ideas&quot; like humans do, these models simply perform at such a massive statistical scale that they can identify patterns far beyond any human conception. Clearly, the model embodies some verifiably reliable information about the way the world works, but this is &quot;just&quot; a trick of statistics not anything resembling actual &quot;understanding&quot; in the way the word is typically used when referring to human understanding.')