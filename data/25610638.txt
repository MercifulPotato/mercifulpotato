Item(by='shishy', descendants=None, kids=None, score=None, time=1609565164, title=None, item_type='comment', url=None, parent=25605473, text='I&#x27;m not sure that&#x27;s actually something we want within the institution. IMO the most important thing is improving the overall _reliability_ of research.<p>On this note, and admittedly a bit of a (relevant) plug: I work at a startup called scite that&#x27;s trying to improve this -- <a href="https:&#x2F;&#x2F;scite.ai" rel="nofollow">https:&#x2F;&#x2F;scite.ai</a><p>Citations are the primary mechanism by which scientific papers &quot;talk&quot; to one another, and one of the systemic issues we see in the status quo is a sort of numerical reductionism where a lot of emphasis is placed on &quot;how many times someone is cited&quot; without any indication of whether those citation counts are from papers that support or dispute someone&#x27;s findings.<p>One of the things we do at scite is let you see, for a paper, how it has been cited (i.e. not just a count, but the surrounding textual context around the citation, and a classification from our model as to whether the citing work provides supporting or disputing evidence for the cited paper, or just mentions it).<p>That information is also aggregated to the author level, or journal, and so on.<p>The hope is that by providing (and improving) this service so that researchers can see how papers are cited, we&#x27;re able to promote more reliable science (in addition to letting someone explore new subject areas &#x2F; doing lit reviews much faster, and other things).<p>I just wanted to share that because a lot of the other comments were more abstract and it might help to include something more tangible that&#x27;s being actively worked on now. There&#x27;s a lot of interesting work in general in this space.<p>If you have any thoughts &#x2F; feedback &#x2F; questions, feel free to reply here (I check occassionally) or write to me at the email in my profile.')