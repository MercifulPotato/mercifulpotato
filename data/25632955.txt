Item(by='derefr', descendants=None, kids=None, score=None, time=1609775933, title=None, item_type='comment', url=None, parent=25632714, text='&gt; I think we should push for a metric where &quot;up&quot; means 100% of people that want to use the service are able to use the service.<p>I mean, that’s nice to say, but how do you measure&#x2F;prove it?<p>Certainly, having the SLAed party check <i>themselves</i> is silly. But what are the other options? If it was up to the customer, customers could make up faults to get free service. (Since it’d be up to the customer to prove, and customers are generally less technical than vendors, you’d have to expect&#x2F;accept very non-technical — and thus non-evidentiary! — forms of “proof”, e.g. “I dunno, we weren’t able to reach it today.” Things that could have just as well been their own ISP, or even operator error on their side.)<p>IMHO, contractual SLAs <i>should</i> be based on the checks of some agreed-upon neutral-third-party auditor (e.g. any of the many status&#x2F;uptime monitoring services.) If the third party says the service is up, it’s up in SLA terms; if the third party says the service is down, it’s down in SLA terms.<p>(And, of course, if the third party <i>themselves</i> go down, or experience connectivity issues that cause them to see false correlated failures among many services, that <i>should</i> be explicitly written into the SLA as a condition where the customer isn’t going to get a remedial award against the SLA, even if the SLAed service <i>does</i> go down during that time. If the Internet backbone falls over, that’s the equivalent of what insurance providers call an “act of God.”)<p>But in a neutral-third-party observer setup, you aren’t going to get 100% coverage for customer-seen problems. An uptime service isn’t going to see the service the way <i>every single</i> customer does. Only the way one particular customer would. So it’s not going to notice these spurious some-customers-see-it-some-don’t faults.<p>So, again: what kind of input <i>would</i> feed this hypothetical “100% of customers are being served successfully” metric?<p>ETA: maybe you could get <i>closer</i> to this ideal by ensuring that the monitoring service 1. is effectively running a full integration test suite, not just hitting trivial APIs; and 2. if gradual-rollout experiments ala “hash the user’s ID to land them in an experiment hash-ring position, and assign feature flags to sections of the hash ring” are in use by the SLAed service, then the monitoring service should be given N different “probe users” that together cover the complete hash-ring of possible generated-feature-flag combinations. Or given special keys that get randomly assigned a different combination of feature-flags every time they’re used.')