Item(by='hezag', descendants=None, kids=[25312192, 25313464, 25314497], score=None, time=1607147803, title=None, item_type='comment', url=None, parent=25311402, text='I did not read the paper (just like most people here), but by the title — “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” — it does not look like the CO2 emissions thing is the main topic of this research.<p>BTW, &quot;Stochastic Parrots&quot; is a very descriptive name for the problem<p>&gt; Moreover, because the training datasets are so large, it’s hard to audit them to check for these embedded biases. “A methodology that relies on datasets too large to document is therefore inherently risky,” the researchers conclude. “While documentation allows for potential accountability, [...] undocumented training data perpetuates harm without recourse.”<p>Since these models are being applied in a lot of fields that directly affects the life of millions of people, this is a very important and underdiscussed problem.<p>I really want to read the paper.')