Item(by='glangdale', descendants=None, kids=None, score=None, time=1606283509, title=None, item_type='comment', url=None, parent=25203929, text='I&#x27;ve done some very, very preliminary work to pave the way towards iteration and recursion in superoptimizers, but you are correct that &quot;straight-line over tiny problems&quot; is mostly where it&#x27;s at.<p>Even multiplication ops are a real bear for SMT. I can get them to work, but they are <i>way</i> more expensive than other arithmetic.<p>From what I understand, the whole &#x27;approximate with uninterpreted functions&#x27; stuff can be done under the hood by SMT solvers during bit-blasting - from what I understand, more expensive operations can be expanded lazily (and until then treated as UFs) in the hope that useful properties emerge. Logics that have floating point theories do exist in the SMT benchmarks but like a big coward I haven&#x27;t played with them.<p>You can&#x27;t <i>entirely</i> ignore cost issues in a superoptimizer - the difference between, say, a latency = 3 instruction like a multiply and a latency = 1 operation might be decisive. But (a) there are ways to model this in your optimization process and (b) fine differences of cost aren&#x27;t the primary issue when superoptimizing anyhow. The later point is decisive - I have a benchmark (on bytewise set membership) in my superoptimizer where I example 25 million 4-instruction specializations and there are only 7 solutions. I think I can spend a few extra cycles evaluating with of those 7 will be faster under a range of transforms, etc etc - the expensive part was finding 7&#x2F;25M solutions to begin with (or, as is often the case, finding 0&#x2F;25M solutions).')