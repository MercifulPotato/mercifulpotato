Item(by='Jugurtha', descendants=None, kids=[25920913], score=None, time=1611693263, title=None, item_type='comment', url=None, parent=25920264, text='&gt;<i>All learning amounts to remembering. A machine learning model is basically remembered weights. Everything we&#x27;ve learned is what we remember as solidified in our neurons, etc.</i><p>That&#x27;s &quot;not even wrong&quot;, as Pauli would say. Your paragraph suffers from using a shaky non inversible analogy:<p>Machine learning often uses an analogy for the brain, neurons, activation functions, etc. Some accuracy about the real world is sacrificed in that analogy for the sake of being useful and to have <i>something</i> to reason with and shared taxonomy. We accept that loss for the sake of being productive and for lack of actual equivalents.<p>What your first paragraph did is use that analogy of the brain used in machine learning, that is shaky to begin with, and use it to reason about the biological brain as if we did not have the actual thing.<p>In other words, we had a biological brain that we clumsily modeled to get work done in ML, and the paragraph used that model to reason about the brain itself. Similar to how you translate from French --&gt; English --&gt; French and get a different output than the input.<p>Remembering certainly plays a role in learning, though it is but one component. For it to <i>be</i> what learning <i>is</i>, everything has to be exactly the same with every instance.<p>To use the analogy, a machine learning model returns predictions&#x2F;output for instances it has not necessarily seen. Our brain produces outputs based on situations that had not yet happened, at least not in the &#x27;anisotropic time&#x27; universe, before that.<p>What do you think?')