Item(by='kernelsanderz', descendants=None, kids=None, score=None, time=1601704766, title=None, item_type='comment', url=None, parent=24659613, text='I was originally appalled at the software limiting. But according to Tim Dettmers who has a solid record of predicting and comparing NVIDIA cards for deep learning performance, it&#x27;s not really a big deal.<p>You can read his analysis here: <a href="https:&#x2F;&#x2F;timdettmers.com&#x2F;2020&#x2F;09&#x2F;07&#x2F;which-gpu-for-deep-learning&#x2F;" rel="nofollow">https:&#x2F;&#x2F;timdettmers.com&#x2F;2020&#x2F;09&#x2F;07&#x2F;which-gpu-for-deep-learni...</a><p>and his tweet about this here: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;Tim_Dettmers&#x2F;status&#x2F;1311354118514982912" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;Tim_Dettmers&#x2F;status&#x2F;1311354118514982912</a><p>Essentially from my understanding it&#x27;s memory bandwidth which is the real critical path on performance in most cases. The previous generation of Turing cards had more compute than was necessary so they were an underutilized resource.<p>Also, this Puget benchmark is using an older version of the CUDA drivers. I believe performance is much better in CUDA 11.1.<p>This new benchmark which is running on the latest CUDA seems to confirm Tim&#x27;s numbers: <a href="https:&#x2F;&#x2F;www.evolution.ai&#x2F;post&#x2F;benchmarking-deep-learning-workloads-with-tensorflow-on-the-nvidia-geforce-rtx-3090" rel="nofollow">https:&#x2F;&#x2F;www.evolution.ai&#x2F;post&#x2F;benchmarking-deep-learning-wor...</a>')