Item(by='t-writescode', descendants=None, kids=None, score=None, time=1609889630, title=None, item_type='comment', url=None, parent=25650905, text='I have worked for 3 separate companies in my decade of software development where I have had to be on-call. One of these companies was an organization at a very large and prominent software company.<p>The different on-call rotations worked out thusly:<p>1. on-call was 1 month long. Response times had to be very short. During business hours, there was a large queue of long-tail work that needed to be resolved that was outside my normal work. Most of the employees here were in their 20s and 30s, probably.<p>2. Small company. Probably 30 devs total. I was on a team of 1, 2 and eventually 3 people. on-call was 24&#x2F;7 for my team. Response time was about an hour. I was the youngest employee and most employees here were in their 40s or beyond.<p>3. Smallish company. &lt; 500 employees. Dev team size of 6ish. On-call is a week-long venture. Turn around time is very short, I think 30 minutes? On-call is a dedicated period. Most issues can be resolved during business hours; but, emergencies are handled at all times.<p>For [2] and [3], there were unwritten patterns around how much you really needed to be at work once your shift was over if on-call was particularly bad.<p>At [1], the on-call was particularly long and harsh for a couple reasons. In the early days, I heard that the on-call was absolutely horrible. Logs were non-existent, errors were terrible and required a great deal of work. But, it caused developers to feel the pain of not logging properly, not handling errors correctly, and not monitoring usefully. Over time, those issues were resolved, the team has incredible logging and incredible tooling, knowing that they&#x27;re going to be the ones that have to fix it this time.<p>At [2], the constant trouble of code prior to my time there caused the developers of the old code to make it more stable. The services eventually became auto-resolving, we had a network operations center (with appropriate work hours that covered the whole day) that had playbooks for all the remaining normal issues; and, the bad stuff made it to us. On-call 24&#x2F;7 meant I might get called once every couple weeks or less by the end of my tenure there. I lived a normal life.<p>At [3], we&#x27;re still learning and the code is in constant churn. Issues come up and we attempt to fix the root cause on most of the issues. Our logging has gradually improved and our monitoring has been improving and they&#x27;re tweaked to find real issues.<p>--<p>My thoughts:<p>I think on-call is an important experience for developers. Developers should be first responders for their code when it hits production for the first day or two to catch any possible issue.<p>Developers should know the pain of deploying their change at noon or on a Friday at 5pm, or at 11pm on a Wednesday, so that they accept responsibility and importance if it breaks at those times, and those actions should be above and beyond their on-call rotation.<p>If the work of the on-call is especially intense, it should be a separate role that the developers take, with a rotation so that that&#x27;s all that specific developer is working on.<p>Developers should write code and review code with debugging and tracing and monitoring and self-correction in mind, to reduce on-call pain - and one of the best ways to do that is to make them feel it, themselves.<p>If your code-base is having as many issues as you suggest, there are probably some common areas and pitfalls that the code has, and maybe they&#x27;ll be patterns the team can implement each time those same issues come up. As a result, those errors won&#x27;t come up as frequently.<p>If the monitors are too noisy with non-errors, then a couple things could be going on. Let&#x27;s say that the code 500s when someone passes an invalid argument, or a record isn&#x27;t found. Those probably shouldn&#x27;t be 500s, so the code needs to be updated for them to not be. On the other hand, if there&#x27;s a monitor checking for more than 5 401&#x27;s in a minute, maybe that&#x27;s a bit strict and should be changed to &quot;more than 10 401s a minute, every minute for 10 minutes; OR more than 200 401s a minute&quot; - that way you catch the big ugly case of &quot;our auth service is down&quot; and aren&#x27;t caught by people failing to enter their password a bunch (but giving up).<p>If the code is an absolute and unfixable mess and you don&#x27;t want to help fix it, if management is not interested in improving common pitfalls, then maybe it&#x27;s time for you to look for another job.<p>Here&#x27;s some additional reading: <a href="https:&#x2F;&#x2F;sre.google&#x2F;sre-book&#x2F;being-on-call&#x2F;" rel="nofollow">https:&#x2F;&#x2F;sre.google&#x2F;sre-book&#x2F;being-on-call&#x2F;</a>')