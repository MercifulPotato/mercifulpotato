Item(by='raptortech', descendants=None, kids=None, score=None, time=1602640373, title=None, item_type='comment', url=None, parent=24771856, text='This seems like a HUGE insight! As I understand it, they show that RL can effectively be recast as two sub-problems:<p>1. learning a policy that imitates your own behavior on prior experience, which is a trivial supervised learning problem<p>2. learning how to weight the importance of prior experiences (learning a data distribution), for which the authors have derived a lower bound<p>Given a pool of experience, this seems like a fantastic off-policy method to optimize arbitrary reward functions. The main shortcomings I see with this method is that it still does not lead to any significant insights into how to collect new data online, which is a major open problem in RL.<p>I&#x27;m also wondering why the authors didn&#x27;t publish any experiments to show that it works...')