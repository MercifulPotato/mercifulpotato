Item(by='temuze', descendants=None, kids=[25279998], score=None, time=1606932632, title=None, item_type='comment', url=None, parent=25277511, text='I&#x27;m glad more people are tackling this problem. There still isn&#x27;t a good solution to real-time aggregation data at large scale.<p>At a previous company, we dealt with huge data streams (~1TB data &#x2F; minute) and our customers expected real-time aggregations.<p>Making an in-house solution for this was incredibly difficult because each customer&#x27;s data differed wildly. For example:<p>- Customer A&#x27;s shards might have so much cardinality where memory becomes an issue.<p>- Customer B&#x27;s shards might have so much throughput where CPU becomes a constraint. Sometimes a single aggregation may have so much throughput where you need to artificially increase the cardinality and aggregate the aggregations!<p>This makes the optimal sharding strategy very complex. Ideally, you want to bin-pack memory-constrained aggregations with CPU-constrained aggregations. In my opinion, the ideal approach involves detecting the cardinality of each shard and bin-packing them.')