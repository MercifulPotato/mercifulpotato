Item(by='defen', descendants=None, kids=[25577665, 25576158], score=None, time=1609282329, title=None, item_type='comment', url=None, parent=25575682, text='&gt; Many other sciences have IRB and human factors approvals for experiments that involve or may involve humans.<p>As far as I know, those are for experiments that directly interact with humans as part of the experiment itself. They exist because of a long history of unethical researchers testing interventions (or lack thereof) on people without their knowledge or informed consent - e.g. Tuskegee Syphilis Experiment, MK-ULTRA, Stanford prison experiment (there are a lot). The closest equivalent in CS of directly-interacting experiments would be HCI research as you said. I could also see a strong argument being made for research that uses the creative or copyrighted output of a person without their consent - for example, their face as part of training facial recognition software, their written words as part of training GPT-3, their voice as part of training voice recognition software, etc.<p>However the incident in question is really about a different kind of thing - it&#x27;s asking researchers to speculate on the future ramifications of their research as it pertains to progressive ideals - in practice this means, &quot;How could this negatively affect minorities or the environment?&quot; These aren&#x27;t inherently bad things to think about, but as you get further and further away from concrete applications of ML, it begins to look more and more like a religious ritual than something that is actually trying to address the stated problems.')