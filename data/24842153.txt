Item(by='mumblemumble', descendants=None, kids=[24846500], score=None, time=1603226517, title=None, item_type='comment', url=None, parent=24841964, text='I&#x27;m not trying to say that neural networks are inherently constrained. I&#x27;m saying that, in typical usage, they tend to be used a certain way that I believe introduces some useful constraints. You <i>can</i> use a single hidden layer and an arbitrary activation functions, but, in practice, it&#x27;s a heck of a lot more common to use multiple hidden layers and tanh.<p>It&#x27;s worth noting that neural networks didn&#x27;t take off with Hornik et al. style simple-topology-complex-activation-function universal approximators. They took off a decade or so later, with LeCun-style complex-topology-simple-activation-function networks.<p>That arguably suggests that the paper is of more theoretical than practical interest. It&#x27;s also worth noting that one of the practical challenges with a single hidden layer and a complex activation function is that it&#x27;s susceptible to variance. Just like polynomial regression.')