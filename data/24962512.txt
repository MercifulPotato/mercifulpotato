Item(by='johnchristopher', descendants=None, kids=[24964543], score=None, time=1604264891, title=None, item_type='comment', url=None, parent=24960337, text='&gt; I&#x27;ve been trying to get this working lately and nothing works &quot;well&quot;. Motion is popular, but is limited to mjpeg rather than h.264. uv4l is closed source and not up to date with current OS versions. None of the various cvlc incantations to do better actually seem to work on my pi zero w, etc.<p>Since you use motion, I suppose you want to have some kind of detection going on.<p>I was toying with 2 pi0+cam and one pi4 some weeks ago.<p>Here&#x27;s my conclusion (I should write a blog post about it):<p>- use a pi0+camera dedicated to live streaming ; that&#x27;s the one you want to connect to to see what&#x27;s going on (fixed IP, RTSP stream)<p>- use another pi0+camera or add an IR sensor to the first pi0, in the same spot, to detect motion events (either through motion installed on the pi or the IR sensor)<p>- use a third connected pi4 for continuous recording of the live stream (in chunks of 5 minutes) to HDD&#x2F;SSD and&#x2F;or recording of the live feed once a motion is triggered (you can use motion hooks to trigger API on this pi from the pi0)<p>- you could also simply do motion detection on the pi4 but you are at the mercy of artifacts in the stream and they WILL trigger motion detection ; that&#x27;s why you want to do motion detection closest to the source<p>Motion introduces too much latency to use as a two-ine-one &quot;detect and live stream&quot; (MJPEG conversion takes a lot of CPU clock and adds artifacts).<p>Most motionEyeOS tutorials I read are PoC that makes you install motionEyeOS on every pi and use the motion MJPEG stream instead of an h264 feed from the camera. It also introduces a lot of CPU bottlenecks and unreliable network connectivity.<p>I found that motion web UI is now enough for live streaming of what motion &quot;sees&quot; but motionEyeOS helps understanding many of motion options. It&#x27;s especially useful to draw masks. And then you move on to building your own infrastructure with those bricks (http API, live streaming, hl264 streaming, etc.).')