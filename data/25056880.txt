Item(by='quietbritishjim', descendants=None, kids=[25057880], score=None, time=1605086820, title=None, item_type='comment', url=None, parent=25056789, text='I&#x27;m not saying write your whole program in C. I&#x27;m saying, <i>at worst</i>, write a tiny corner of it in C so that you don&#x27;t have to restructure the rest of your application - overall it would be simpler, even for most mostly-Python devs. Or, much more likely, use a library like numpy which you were probably going to do anyway. I regularly use numpy and don&#x27;t find myself thinking &quot;I wish didn&#x27;t have to write all this C code...&quot;. (I must admit, an unfortunate consequence of this is that I&#x27;ve interviewed a few junior candidates that were so used to writing vectorised code that they seemed to be terrified of using a for loop!)<p>I admit I didn&#x27;t watch the whole of the talk. (Personally I much prefer learning from text than video, and I was put off by the GIL bit anyway.) From the parts I saw, it seemed like the GIL issue was critical for motivating everything he did afterwards, but perhaps there were other reasons that became clear later. In that case, he could have avoided mentioning the GIL altogether (given that he ended up being so misleading about it).<p>&gt; The real world usecase, as with basically all python, is io.<p>That is just straight up not true. Yes, IO is a valid use case of Python but it definitely isn&#x27;t &quot;basically all Python&quot;. I&#x27;m sure the vast majority of data scientists use Python, and for most of them their workloads are almost entirely computational (using deep learning libraries with C&#x2F;C++&#x2F;CUDA backends).')