Item(by='mjb', descendants=None, kids=None, score=None, time=1603719278, title=None, item_type='comment', url=None, parent=24892590, text='&gt; I did my undergrad CS thesis on this topic and implemented a &quot;power of two choices&quot; load balancing module in NGINX.<p>Cool!<p>&gt; When simulating load distribution across servers, two-choices does an outstanding job at keeping things evenly distributed. However, I was unable to observe a meaningful difference between load balancing algorithms when measuring request latency from servers under load.<p>That&#x27;s very interesting. Were you serving static content? Or running code, too?<p>Intuitively, I&#x27;d expect that NGINX serving static content would tend to scale fairly linearly up to the saturation point, and then get rapidly bad. That&#x27;s a pretty pessmisic case for load balancer performance measurement, because you won&#x27;t see effects until you&#x27;re running the whole system very hot. That &#x27;very hot&#x27; modality is important to real-world applications, but certainly makes benchmarking challenging. Does that sound right to you?<p>In my experience, that tends to change when running complex business logic code, or generating dynamic content. There, the load-latency graph has a higher linear slope. The effects of good load balancing are more immediately visible. It&#x27;s also more visible in things which explicitly have longer latency with higher load, like some kinds of queue-based and stream-based systems.')