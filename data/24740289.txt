Item(by='Voloskaya', descendants=None, kids=None, score=None, time=1602349567, title=None, item_type='comment', url=None, parent=24739642, text='&gt; is anyone doing so to your knowledge? current barrier as far as i understand is that it costs an estimated 4.6MM USD in compute to recreate.<p>That I know of, no one is looking into doing this in the open source community no. I think it&#x27;s simply too hard and costly.<p>As I said in another comment, even if an open source org. managed to raise 4.6M$, they still wouldn&#x27;t be able to do it as the infrastructure needed to train a model of that size is not available publicly on any cloud.<p>The cluster Microsoft built for OpenAI is about 285k CPUs and 10k V100 GPUs, this is absolutly huge. From GPT-3&#x27;s paper we know the training required 3640 PFLOPs&#x2F;days of compute. \nA single V100 GPU is about 14 TFLOPs in half precision, so if you had 75 of those GPUs (which is already massive by open source standard, that&#x27;s about 5 DGX-2 nodes), it would take about 10 years of training (by that same calculation, it still took about a month on their hypercluster if they used all the GPUs). And that is probably an optimistic estimate since I am using Nvidia&#x27;s marketing slide to evaluate a V100 FLOPs.<p>Even if somehow an open source organisation was able to get their hand on a cluster half the size of the one OpenAI used, by the time they figure out all the data scraping&#x2F;cleaning part, found the correct hyper parameters by experimenting on smaller models, and developed the many other things and tricks you need to do to train those models (optimizers, correct checkpointing&#x2F;resuming strategy etc.) I believe one of OpenAI&#x2F;Microsoft&#x2F;Google&#x2F;Nvidia would already have (or be very close to) announcing the next bigger model, and then it&#x27;s back to square 1.<p>I think it&#x27;s just too big a project (with too little usefuleness) to justify undertaking if you are not a massive organization.<p>The closest open source org that could pull this off that I can think of would be HuggingFace. They have the skill and descend funding, and they have the incentive (I believe they also want to enter that NLP API space), but they don&#x27;t have access to that kind of compute infrastructure (that I know of). They also have been open source so far, but nothing guarantees that would share the checkpoints of such a model should they train it.')