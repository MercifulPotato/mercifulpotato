Item(by='jorangreef', descendants=None, kids=[25683212], score=None, time=1610095647, title=None, item_type='comment', url=None, parent=25681198, text='&gt; For most code, you don&#x27;t know how fast it &quot;ought to&quot; be<p>This is exactly it. I was working on a &quot;fastest substring search&quot; challenge back in 2016 for some prize money and I was trying to think about the methodology I should use to win the competition.<p>I realized that the biggest problem was not actually the problem of optimizing the substring search defined by the competition, but rather knowing when my entry would almost certainly be faster than the other entries.<p>After that, the next biggest problems were still not the substring problem, but the types of machine the competition organizers would benchmark on, and the various sizes of the input data they would use.<p>I decided to start with the practical limit and then work backwards from there. In other words, measure something almost like an identity function followed by a simple linear for loop buffer scan, and then use this &quot;null-cipher&quot; limit as my competitor, because there&#x27;s nothing faster than doing almost no work.<p>All this lead away from theoretically optimal algorithms such as Ahoâ€“Corasick towards something based on Rabin-Karp that could also switch dynamically to a second fallback brute force algorithm optimized for small data, depending on the length of the input data. There was also a curve of precomputed branch points in the algorithm to know when to switch, calculated according to the type of machine the organizers would use, probably server grade rather desktop.<p>Learning this lesson was probably the most important so far for me. These days when I start with a design problem, I figure out what the &quot;null-cipher&quot; performance would be (this kind of number is usually fairly cheap to obtain: like the speed of light in a vacuum if you were optimizing fiber links, or the max hello world http requests per second for a typical server), and then use back-of-the-envelope numbers to work backwards from there to know where the implementation should reasonably be expected to land up if there are no performance bugs. This way you know you&#x27;ve got a winner.<p>This is the exact opposite to starting with something and then trying to make it faster, and I find it saves wasted debugging time by avoiding implementations that can never be made fast enough for the requirements.')