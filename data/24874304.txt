Item(by='cbsmith', descendants=None, kids=None, score=None, time=1603487957, title=None, item_type='comment', url=None, parent=24873842, text='Thanks to all that complexity in HTTP (particularly HTTP2), there is no need to pay that headers size cost in most cases, particularly ones where bandwidth tends to be a concern. Yes, naive and &quot;simple&quot; implementations totally waste a TON of space in headers, and even if you exploit all that complexity HTTP is still inefficient in terms of network bandwidth consumption, much as even compressed JSON is still inefficient with its use of bytes.<p>All I&#x27;m saying is that while JSON is horribly inefficient for some payloads, the reality is that most serialization formats tend to be efficient for specific kinds of payloads, but still have plenty of common cases they don&#x27;t attempt to be efficient with. For example, machine readable formats like protobuf, thrift, avro, etc. employ some variation on varint encoding to keep small integers compactly represented, they encode strings relatively inefficiently; They encode field type &amp; number, followed by a varint for length, followed by uncompressed UTF-8 encoded payload... and I&#x27;ve found that if anything it seems once you go through the compression ringer it often isn&#x27;t much different in terms of space consumption vs. JSON... and sometimes it can be <i>less</i> efficient. Given how often fields end up being encoded as strings (even though they should be enums, ints, etc.), this has resulted in a lot of surprising cases for teams who have tried switching away from JSON to reduce payload sizes. If you cared about efficiency, you&#x27;d probably have a special encoding mechanism for short strings, and then a reasonably compact (but fast to parse) representation for longer strings, probably using something like SCSU encoding instead of UTF-8.')