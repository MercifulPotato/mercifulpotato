Item(by='csnover', descendants=None, kids=None, score=None, time=1603929913, title=None, item_type='comment', url=None, parent=24924301, text='First, I want to stress that I agree wholeheartedly that the social networking sites’ focus on engagement is clearly harmful to society, and the dark patterns they use to drive engagement are morally repugnant. I want that to stop as much as anybody. I’m not seeing an answer in your reply to my questions of what change to Section 230 you think would do something about that, though.<p>My understanding of S230 is that it says (1) information services are considered <i>distributors</i> rather than <i>publishers</i> when it comes to liability, and (2) if they exercise limited editorial control in good faith by moderating objectionable content they are <i>still</i> considered distributors rather than publishers for the sake of liability. (Also, there are already various exceptions to the liability shield, so it is not a blanket immunity.)<p>While you may be tired of the argument that sites will shut down, it is one legitimate potential outcome of a change to S230, and one we’ve already seen in action: SESTA-FOSTA caused Craigslist to close their personals sections even though they had no <i>intent</i> to facilitate sex trafficking and it would have been <i>reasonable</i> for them to claim that the overwhelming majority of posts were not illegal. They simply couldn’t accept the liability risk.<p>Furthermore, as someone who has hosted an online forum about retrocomputing since 2002, losing the liability protections of S230 in the way Ted Cruz seems to want it (by eliminating the liability shield for sites which moderate content) would absolutely push me to shutter the whole site. So, please don’t act like this isn’t a real threat; there’s both precedent <i>and</i> I’m telling you what it would cause <i>me</i> to do as a hobbyist site operator. It doesn’t matter if I could eventually win a court case—I am unable and unwilling to accept liability for what other users decide to say.<p>Since—again, as far as I know—it is not illegal for social networking sites to use algorithms or dark patterns to drive engagement, and since S230 is a shield only against liability for the content of posts made by users even in the presence of limited good faith moderation of the content, I ask once again the same questions: what is it that you think Section 230 does that “allows platforms to be judge, jury, and executioner”? (It doesn’t allow the platforms to modify UGC in a way which changes its meaning, for example.) What harm, specifically, do you think the Section 230 liability shield is causing? (It doesn’t shield providers against crimes that don’t exist, or crimes that they commit themselves.) What change, specifically, do you think should be made to cure that harm? How would your proposed change not cause sites with UGC to either shut down entirely (thereby eliminating the ability to speak out at all), require pre-approval for all UGC (thereby increasing the amount of censorship), or be overrun by spammers and trolls (thereby making the site useless)?<p>Looking forward to any answers to my questions above.<p>(EDIT: Clarity and tone.)')