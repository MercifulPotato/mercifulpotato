Item(by='mslot', descendants=None, kids=[25931447], score=None, time=1611741069, title=None, item_type='comment', url=None, parent=25911075, text='Sharding a matching engine is indeed pretty hard, and requires redundancy and very deliberate data modelling choices.<p>That does seem like a fun exercise :).<p>The general rules of the game are: You can only scale up throughput of queries&#x2F;transactions that only access 1 shard (some percentage going to 2 shards can be ok). You can only scale down response time of large operations that span across shard since they are parallelized. You should only join distributed tables on their distribution column. You can join reference tables on any column.<p>The thing that comes to mind is to use a reference table for any user data that is used to find&#x2F;score matches. Reference tables are replicated to every node and can be joined with distributed tables and each other using arbitrary join clauses, so joining by score or distance is not a problem, but you need to store the data multiple times.<p>One of the immediate benefits of reference tables is that reads can be load-balanced across the nodes, either by using a setting (citus.task_assignment_policy = &#x27;round-robin&#x27;) or using a distributed table as a routing&#x2F;parallelization scheme.<p><pre><code>    CREATE TABLE profiles (\n        user_id bigint primary key,\n        location geometry,\n        profile jsonb\n    );\n    SELECT create_reference_table(&#x27;profiles&#x27;);\n    \n    CREATE TABLE users (\n        user_id bigint primary key references profiles (user_id),\n        name text,\n        email text\n    );\n    SELECT create_distributed_table(&#x27;users&#x27;, &#x27;user_id&#x27;);\n    \n    -- replicate match_score function to all the nodes\n    SELECT create_distributed_function(&#x27;match_score(jsonb,jsonb)&#x27;);\n    \n    -- look up profile of user 350, goes to 1 shard\n    SELECT * FROM users u, profiles p WHERE u.user_id = p.user_id AND u.user_id = 350;\n    \n    -- find matches for user #240 within 5km, goes to 1 shard\n    SELECT b.user_id, match_score(a.profile, b.profile) AS score\n    FROM users u, profiles a, profiles b\n    WHERE u.user_id = 240 AND u.user_id = a.user_id \n    AND match_score(a.profile,b.profile) &gt; 0.9 AND st_distance(a.location,b.location) &lt; 5000 \n    ORDER BY score DESC LIMIT 10;\n</code></pre>\nThe advantage of having the distributed users table in the join is mainly that you divide the work in a way that keeps each worker node&#x27;s cache relatively hot for a specific subset of users, though you&#x27;ll still be scanning most of the data to find matches.<p>Where it gets a bit more interesting is if your dating site is opinionated &#x2F; does not let you search, since you can then generate matches upfront in batches in parallel.<p><pre><code>    CREATE TABLE match_candidates (\n        user_id_a bigint references profiles (user_id),\n        user_id_b bigint references profiles (user_id),\n        score float,\n        primary key (user_id_a, user_id_b)\n    );\n    SELECT create_distributed_table(&#x27;match_candidates&#x27;, &#x27;user_id_a&#x27;, colocate_with :=&#x27;users&#x27;);\n    \n    -- generate match candidates for all users in a distributed, parallel fashion\n    -- will generate a match candidate in both directions, assuming score is commutative\n    INSERT INTO match_candidates\n    SELECT a.user_id, b.user_id, match_score(a.profile,b.profile) AS score\n    FROM users u, profiles a, profiles b\n    WHERE u.user_id = a.user_id \n    AND match_score(a.profile,b.profile) &gt; 0.9 AND st_distance(a.location,b.location) &lt; 5000 \n    ORDER BY score DESC LIMIT 10;\n</code></pre>\nFor interests&#x2F;matches, it might make sense to have some redundancy in order to achieve reads that go to 1 shard as much possible.<p><pre><code>    CREATE TABLE interests (\n        user_id_a bigint references profiles (user_id),\n        user_id_b bigint references profiles (user_id),\n        initiated_by_a bool,\n        mutual bool,\n        primary key (user_id_a, user_id_b)\n    );\n    SELECT create_distributed_table(&#x27;interests&#x27;, &#x27;user_id_a&#x27;, colocate_with :=&#x27;users&#x27;);\n    \n    -- 240 is interested in 350, insert into 2 shards (uses 2PC)\n    BEGIN;\n    INSERT INTO interests VALUES (240, 350, true, false);\n    INSERT INTO interests VALUES (350, 240, false, false);\n    END;\n    \n    -- people interested in #350, goes to 1 shard\n    SELECT * FROM interests JOIN profiles ON (user_id_b = user_id) WHERE user_id_a = 350 AND NOT initiated_by_a;\n    \n    -- it&#x27;s a match! update 2 shards (uses 2PC)\n    BEGIN;\n    UPDATE interests SET mutual = true WHERE user_id_a = 240 AND user_id_b = 350;\n    UPDATE interests SET mutual = true WHERE user_id_a = 350 AND user_id_b = 240;\n    END;\n    \n    -- people #240 is matched with, goes to 1 shard\n    SELECT * FROM interests JOIN profiles ON (user_id_b = user_id) WHERE user_id_a = 240 AND mutual;\n</code></pre>\nFor data related to a specific match, you can perhaps use the smallest user ID as the distribution column to avoid the redundancy.<p><pre><code>    CREATE TABLE messages (\n        user_id_a bigint,\n        user_id_b bigint,\n        from_a bool,\n        message_text text,\n        message_time timestamptz default now(),\n        message_id bigserial,\n        primary key (user_id_a, user_id_b, message_id),\n        foreign key (user_id_a, user_id_b) references interests (user_id_a, user_id_b) on delete cascade\n    );\n    SELECT create_distributed_table(&#x27;messages&#x27;, &#x27;user_id_a&#x27;, colocate_with :=&#x27;interests&#x27;);\n\n    -- user 350 sends a message to 240, goes to 1 shard\n    INSERT INTO messages VALUES (240, 350, false, &#x27;hi #240!&#x27;);\n    \n    -- user 240 sends a message to 350, goes to 1 shard\n    INSERT INTO messages VALUES (240, 350, true, &#x27;hi!&#x27;);\n    \n    -- user 240 looks at chat with user 350, goes to 1 shard\n    SELECT from_a, message_text, message_time\n    FROM messages \n    WHERE user_id_a = 240 AND user_id_b = 350\n    ORDER BY message_time DESC LIMIT 100;  \n</code></pre>\nThis exercise goes on for a while. You still get the benefits of PostgreSQL and ability to scale up throughput of common operations or scale down response time of batch operations, but it does require careful data model choices.<p>(Citus engineer who enjoys distributed systems puzzles)')