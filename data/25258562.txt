Item(by='tomsmeding', descendants=None, kids=None, score=None, time=1606768240, title=None, item_type='comment', url=None, parent=25253427, text='How fast does it have to be? I was curious and implemented a trivial loop in C++ that draws &quot;scatter plot&quot; on your benchmark (scatter plot with lots of [0,1]^2 points). Its concept of &quot;scatter plot&quot; is: 1920x1080 image, draw a non-antialiased (i.e. ugly) filled blue circle of radius 4 at each point, where the unit square is stretched over the entire image.<p>Doing this (C++, CPU) takes ~0.21 seconds for 2 million points, and ~2.06 seconds for 20 million points. The result is a solid blue rectangle, but that&#x27;s beside the point.<p>Doing the same on a trivial GPU implementation using CUDA, where I cheated by not even using atomic operations (so this only works because the dots are solid), takes ~0.23 seconds for 2 million points and ~2.3 seconds for 20 million points. Apparently the (my? (GTX 1050 mobile)) GPU doesn&#x27;t help here.<p>For comparison, with matplotlib (`plt.scatter(x, y); plt.show()`): for 2 million points, plt.scatter() takes ~10 seconds, and plt.show() takes about 3 seconds fill first image. Zooms&#x2F;resizes etc. seem to be the same time, or if less points show, proportional to the number of visible points.<p>So unsurprisingly my cheap C++ code is faster than matplotlib (it doesn&#x27;t even anti-alias), but is it faster than Matlab? I have neither Octave nor Matlab, perhaps you can compare with matplotlib?')