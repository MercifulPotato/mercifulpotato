Item(by='jacques_chester', descendants=None, kids=[25280700], score=None, time=1606931267, title=None, item_type='comment', url=None, parent=25278679, text='Suppose you have normalized your data schema, up to at least 3NF, perhaps even further up to 4NF, 5NF or (as Codd intended) BCNF.<p>Great! You are now largely liberated from introducing many kinds of anomaly at insertion time. And you&#x27;ll often only need to write once for each datum (modulo implementation details like write amplification), because a normalised schema has &quot;a place for everything and everything in its place&quot;.<p>Now comes time to query the data. You write some joins, and all is well. But a few things start to happen. One is that writing joins over and over becomes laborious. What you&#x27;d really like is some denormalised intermediary views, which transform the fully-normalised base schema into something that&#x27;s more convenient to query. You can also use this to create an isolation layer between the base schema and any consumers, which will make future schema changes easier and possibly improve security.<p>The logical endpoint of doing so is the Data Warehouse (particularly in the Kimball&#x2F;star schema&#x2F;dimensional modelling style). You project your normalised data, which you have high confidence in, into a completely different shape that is optimised for fast summarisation and exploration. You use this as a read-only database, because it massively duplicates a lot of information that could otherwise have been derived via query (for example, instead of a single &quot;date&quot; field, you have fields for day of week, day of month, day of year, week of year, whether it&#x27;s a holiday ... I&#x27;ve built tables which include columns like &quot;days until major conference X&quot; and &quot;days since last quarterly release&quot;).<p>Now we reach the first problem. It&#x27;s too slow! Projecting that data from the normalised schema requires a lot of storage and compute. You realise after some scratching that your goal all along was to pay that cost upfront so that you can reap the benefits at query time. What you want is a <i>view</i> that has the physical characteristics of a <i>table</i>. Meaning you want to write out the results of the query, but still treat it like a view. You&#x27;ve &quot;materialized&quot; the view.<p>Now the second problem. Who, or what, does that projection? Right now that role is filled by ETL, &quot;Extract, Transform and Load&quot;. Extract from the normalised system, transform it into the denormalised version, then load that into a data warehouse. Most places do this on a regular cadence, such as nightly, because it just takes buckets and buckets of work to regenerate the output every time.<p>Now enters Materialize, who have a secret weapon: timely dataflow. The basic outcome is that instead of re-running an <i>entire view query</i> to regenerate the materialized view, they can, from a given datum, determine exactly what will change in the materialized view and <i>only</i> update that. That makes such views potentially thousands of times cheaper. You could even run the normalised schema and the denormalised projections on the same physical set of data -- no need for the overhead and complexity of ETL, no need to run two database systems, no need to <i>wait</i> (without the added complexity of a full streaming platform).')