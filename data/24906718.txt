Item(by='Jugurtha', descendants=None, kids=None, score=None, time=1603801500, title=None, item_type='comment', url=None, parent=24890008, text='From my perspective in our effort to build our machine learning platform[0], I have to look at things in terms of impact on our capacity to execute projects faster in a more repeatable way, and help our data scientist colleagues.<p>I keep an eye on new things, but also notice and ask them about what they use and how, then work these frameworks into our platform without compromising <i>flexibility</i>.<p>This of course comes with some frustrations when you do actual projects that involve more than one person, and where documentation that starts with &quot;first, download your dataset to disk&quot; becomes off-putting.<p>For example, we look at how to use PyTorch with S3 object storage, and you stumble on threads where <i>technical support staff</i> tells the asker to find examples on the internet.[1]<p>This brings us to try and find ways to make it work on larger datasets in the context of our platform. This is not particular to PyTorch, though the support thread is hilarious. Looking for ways to use object storage with Tensorflow wasn&#x27;t obvious. The docs show how you could use files giving a path string, but you have to dig a bit deeper in the source code to become aware you could give it a file-like object, and then you have to get the bytes from somewhere, wrap it, and give it to the function that consumes data. Sure, there&#x27;s the `file_io`, but again, it is not super obvious and you have to inherit that to simplify usage and reduce the &quot;activation energy&quot; for data scientists.<p>This is generally true for other parts of the pipeline, and is a reason why we don&#x27;t buy into the hype of &quot;end-to-end&quot; machine learning or &quot;complete lifecycle management&quot; announcement at conferences.<p>For example, we do automatic model detection from code and log the models and parameters with MLflow for now so that data scientists don&#x27;t have to <i>remember</i> or <i>know how to</i>. It&#x27;s all done for them. MLflow has documentation on the ability to &quot;deploy&quot; these models. However, it breaks when these models expect higher-dimensional input (tensors) and expects a DataFrame, so we&#x27;re looking into pandas&#x27; MultiIndex and things like that[2]. But this shows how something that is <i>obvious and common</i> in the real world lacks support, or worse, the issue is closed by an intern who doesn&#x27;t see how it is a problem, which has happened, or a bot automatically closing the issue.<p>We reach out to people to see how they are doing things, and they reply that they write custom code to handle these cases that are <i>not</i> edge cases. And for us, who are working precisely to reduce &quot;custom code&quot; so people can train, track, deploy, monitor, and manage models consistently, reliably, and systematically, this is not good enough and drives us to solve these problems without relying on our proposed changes to be merged into the main tree or forking the repo and having to maintain that fork and conflicts.<p>- [0]: <a href="https:&#x2F;&#x2F;iko.ai" rel="nofollow">https:&#x2F;&#x2F;iko.ai</a><p>- [1]: <a href="https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;will-pytorch-support-cloud-storage&#x2F;24869" rel="nofollow">https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;will-pytorch-support-cloud-sto...</a><p>- [2]: <a href="https:&#x2F;&#x2F;github.com&#x2F;mlflow&#x2F;mlflow&#x2F;issues&#x2F;3570" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;mlflow&#x2F;mlflow&#x2F;issues&#x2F;3570</a>')