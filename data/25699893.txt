Item(by='parsimo2010', descendants=None, kids=[25700261], score=None, time=1610196136, title=None, item_type='comment', url=None, parent=25697823, text='This is an informative article, but it is missing a couple key things.  Maybe the article was strictly aiming to be factual, but it felt like it was taking the position that 230 is good and should stay in place.<p>With the assumption that it means to argue for keeping 230, they failed to convince me that it was a necessary piece of regulation.  Yes, they corrected many misconceptions, but at the end of the day there is still a key argument against 230, and it goes like this:<p>There are a lot of things posted by 3rd parties on websites that lead to harmful outcomes.  Companies that leave that content up are able to make profit from the views&#x2F;clicks generated.  They have already demonstrated the ability to moderate content, they chose not to do so when it’s not favorable to their profit.  Why should I protect platforms or give them the option to not moderate harmful content?  If you’re making a profit from it, that’s part of your product and you should hold some liability.  This wouldn’t absolve 3rd parties that post harmful content from also having liability because the courts regularly rule on splitting liability and deciding which people hold which portion of blame.  Repealing 230 allows us to hold companies accountable for leaving harmful content up when they could have taken it down, it would not require the courts to fine them every time something bad happens- it would just give the courts the option to do so when it’s the right call.  As of now, 230 is too strong of protection.<p>I know they can do it.  We have real time profanity filtering in video game chat, social media automatically detects faces for tagging, I know with a few automated solutions and a team of mods (paid and&#x2F;or volunteer), and online platform can do a reasonable job.  HN does a great job with surprisingly few mods.  Reddit does it and has a pretty robust process for addressing harmful subreddits.  YouTube polices non-advertiser-friendly content, I know they could moderate more if required.  Sure, some things slip through the cracks, but maybe a competent defense and a reasonable court can decide that a company isn’t liable for one-off failures, provided it has good processes in place to catch most of the issues.  Like Ford isn’t held liable every time someone crashes a car and dies, they are only held liable when they are aware of their cars being dangerous and not doing anything about it- but after issuing a recall, the liability is back on the car owner for not getting it fixed.  Anyway, the courts and lawyers are well equipped to decide questions of liability without Section 230.<p>I guess the other thing I want to come back around to that the article didn’t really address, is that right now the protection seems unbalanced&#x2F;one-sided because it protects platforms from choosing not to moderate, but it doesn’t force them not to moderate.  IMHO, if the Facebooks and Twitters are allowed to say “we don’t have the ability to police our content” then they shouldn’t be deplatforming controversial people like Alex Jones.  It’s hypocritical to say that tech companies need protections and then be in favor of kicking people off.  I’m not actually sad when someone like Jones or Trump gets the ban hammer.  But I have a hard time justifying allowing companies to have moderation power and then allowing them to choose only to use it when a situation gets so bad that it affects their profit.  Like, you can’t be watching the a house burn while holding a fire hose and not expect me to be mad when you decide to water your garden instead.  If you’re actually unable to moderate, you had better not be making headlines by moderation.  Because it seems to me like you’re begging the question- oh, so you clearly can moderate <i>some</i> things, let’s figure out how much money you made while ignoring the harm you were causing up until that point.<p>What I’m actually in favor of by the way, is repealing the wholesale protection of Section 230 and replacing it with something that requires a <i>reasonable</i> level of moderation, or provides a little bit of protection for companies that have made a good attempt at moderation.  That would be a much better incentive for companies than the incentive 230 currently provides.')