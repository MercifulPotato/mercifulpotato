Item(by='salawat', descendants=None, kids=[24844269], score=None, time=1603217569, title=None, item_type='comment', url=None, parent=24840267, text='The concept of optima is heavily dependent upon constraints on dimensionality.<p>If you&#x27;re surrounded by walls at a 3D coordinate (walls in this sense is usually something akin to an a priori constraint on step size, which itself is the upper and lower bounds of the imposed delta introduced to the current step to try to find a new direction to go in numerical gradient descent), but you can arbitrarily &quot;jump&quot; 4 dimensionally, and there exists a point in the fourth dimensional space where your 3 dimensional space is no longer constrained from moving in the lower 3 dimensions again, you&#x27;ve essentially &quot;avoided&quot; a local optima, because your optimization function can continue to shift to find something better.<p>At least this holds if we&#x27;re talking gradient descent, where the definition of an optima for a function is a point wherein any numerical deviation within your error tolerance always ends up converging to the same point.<p>If you take that same technique and apply it to higher dimensional spaces, the more dimensions you have, the less likely (in theory) your model is from getting stuck.<p>I know for a fact this doesn&#x27;t always hold though, as almost every GPT2&#x2F;3 model I come across I can still manage to get it snarled in predictive loops, where it does nothing but suggest the same thing over and over and over and over and over and over again, indicating a locally maximal optima for the predictor.<p>One of my favorite way to trip them up is generally some variant of &quot;I once heard a story from a man who heard it from a man, who heard it from a man,... usually it sets it up for the loop. Sometimes you need to massage it a bit, but it&#x27;s generally pretty easy to lead the predictor into a loop.<p>If you really want to blow the theory there are no higher dimensionality optima though, just look at other people. If there were not higher dimensional optima, why do bad habits exist, and get converged on so readily that we actively have to discourage, label, or avoid them?<p>The fact is that for a general function simulator, the trick isn&#x27;t not falling victim to higher dimensional optima, but learning to recognize what and when you can safely tolerate some, and when you can&#x27;t, because you really can&#x27;t avoid the damn things in a resource or physically constrained problem space.')