Item(by='schoen', descendants=None, kids=[25024111], score=None, time=1604822153, title=None, item_type='comment', url=None, parent=25023647, text='But entropy is relative to an attacker (it measures the attacker&#x27;s uncertainty about your password). The 1.2 bits&#x2F;character measurement is about grammatical English text that occurs in a communicative setting. That means that before you get there, you probably already know what word goes at the end of this sentence.<p>You can measure this with human native speakers, or with a compression algorithm, or with a naive letter n-gram or word n-gram model, and get different statistics -- each of which is correct in the sense of <i>each model&#x27;s</i> uncertainty about how the text continues.<p>In the password setting, words can be used in all sorts of ways, some of which are very unpredictable to an attacker, so it&#x27;s plausible that each word adds at least logâ‚‚(len(dictionary)) to the password entropy when it&#x27;s chosen at random from a dictionary, even when the attacker expects that the password selection method could have used that dictionary.')