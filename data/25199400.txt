Item(by='copy-pastries', descendants=None, kids=None, score=None, time=1606232396, title=None, item_type='comment', url=None, parent=25196660, text='These are great questions :-) If I remember correctly from the paper, the authors used quite small grids, but they were actually 3D, like your question. They gave each  &quot;slice&quot; \n(one full dimension of the tensorx1x1) to a different core. Cores had to get border data from 8 surrounding memories, which would be a crazy ratio on most architectures, but works because memory accesses are 1 instruction cycle on the CS-1, or 1 cycle per hop if the local memory isn&#x27;t right by that core. I think that the interconnects on the CS-1 were also toruses (wrapped around at the edges of the grid), which gives you some flexibility.<p>In general, the decomposition of tiles (or blocks in 3d) to cores can follow lots of different strategies depending on what is best for the architecture and whether distributed cores have an all-to-all interconnect, or some sort of torus, or whether the block size can be made to suit some aspect of the processor (e.g. vector length), and the they is to balance the load equally over different cores. Depending on the problem, it might require more advanced graph partitioning algorithms than simple spatial decomposition.')