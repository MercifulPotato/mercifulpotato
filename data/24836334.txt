Item(by='Arathorn', descendants=None, kids=[24838694, 24837865], score=None, time=1603195346, title=None, item_type='comment', url=None, parent=24833184, text='&gt; 1. Would the system discussed have anyway to diminish &quot;cancel culture&quot; like attacks?<p>I would differentiate between &quot;cancel culture&quot;, where folks gang up and react disproportionately to a misdeed - versus a smear campaign where people try to weaponise the moderation system by spreading malicious reputation (e.g. your ESB example).<p>The idea is that both could be solved by having an open ecosystem of relative reputation lists.  If a trusted authority (e.g. iwf.org.uk) were to publish a vetted list of hashes of known bad content that you definitely don&#x27;t want on your server, then you&#x27;d obviously expect them to review and verify entries to that list.  If they got sloppy and let randoms try to smear each other by submitting ESB-haters to the list, then there would be a scandal and people would stop trusting that reputation list.  Alternatively, if ESB-fans started framing ESB-haters with child abuse imagery, then obviously that&#x27;s a problem for the cops rather than the moderation list verifiers.<p>Alternatively, if some random vigilante decided to start publishing up a reputation feed of people they happen to think are are child abusers, and then started to stuff it with other randoms they happen not to like (whether that&#x27;s ESB-haters), then the hope is that armed with appropriate visibility on the filters which they are applying, people would spot that this is a disreputable source of reputation asap and run a mile.  For instance, you might have accidentally subscribed to the #child-abuse-vigilante reputation feed, and your Matrix client might say &quot;btw, 98% of the rooms in this community have been filtered out by the #child-abuse-vigilante feed data&quot;.  You might click on the link to check the names of the rooms which have disappeared, and if they turn out actually to have names like &quot;ESB was overrated&quot; then you might choose to dig further and then ring the alarm that #child-abuse-vigilante is not to be trusted, and unsub.  You could also publish a reputation feed for reputation feeds (no, really).<p>&gt; 2. Is there a way we could test this outside of Matrix?<p>Absolutely.  There is nothing Matrix specific here, or even decentralisation-specific.  We&#x27;re just trying to prove it in Matrix because we have a relatively large community, containing a fairly representative mix of different actors, but we&#x27;re not so big as to make it hard to experiment - and because this is frankly an existential threat to the long-term success of the project :)<p>The room for discussing this is #matrix-reputation:matrix.org (although public convo is a bit sporadic; this should change as we spin up more experiments in the coming weeks&#x2F;months however).')