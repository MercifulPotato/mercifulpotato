Item(by='lilactown', descendants=None, kids=[25503401, 25504113], score=None, time=1608608272, title=None, item_type='comment', url=None, parent=25499196, text='I probably didn&#x27;t make it any more clear in my reply. Transducers don&#x27;t win based on allocations IME but because it removes the number of iterations required.<p>Take the case: (-&gt;&gt; (for ,,,) (map ,,,) (filter ,,,))<p>`for`, `map` and `filter` will all create lazy sequences, holding a reference to the previous one. The problem here is when the `filter` seq gets realized, it will first realize the `map` seq, which will realize the `for` seq. Each sequence will wait for the next one to realize before iterating over it. So in this case it will iterate twice; once for the `map`, and then again for the `filter`.<p>As you know, transducers combine these steps together so that you only iterate over the initial collection once.<p>My other comment was making the point that the author has conflated &quot;laziness&quot; with &quot;immutable data&quot; AFAICT. The lazy seqs in the first example they give will be slower w&#x2F;o transducers, but the other problem is that the overhead from all the allocations required for creating a bunch of immutable hash maps that are then destroyed immediately after is also non-negligible, and seems to be a source of the authors performance problems.')