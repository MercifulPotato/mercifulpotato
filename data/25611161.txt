Item(by='scottlamb', descendants=None, kids=None, score=None, time=1609573938, title=None, item_type='comment', url=None, parent=25608231, text='&gt; As in it&#x27;s impossible, or that&#x27;s too much load? That would be 10 000 groups, each with 100 senders and some number of hundreds of receivers, but with membership changing relatively slowly. There would be lots of packets to move, but not a lot of IGMP to snoop, i think.<p>Millions of memberships in total. That seems far enough out of the norm that I&#x27;m skeptical it&#x27;d be well-supported on standard switches. I could be wrong though.<p>&gt; Service mesh style? I think the main effect is to bundle the messages into bigger packets, using a single group, since you still need to deliver the same set of messages to every machine. You actually end up delivering more messages, since there isn&#x27;t any selectivity. I honestly don&#x27;t know how this pans out!<p>Yeah, that&#x27;s the idea. I don&#x27;t know either, but I&#x27;d expect bundling would make them significant cheaper due to fewer wake-ups, better cache effectiveness, etc.<p>&gt; The total number of load messages sent is fewer in this model than in the model in the article, i think, because each backend sends a single multicast message, instead of N unicast TCP messages. The total number of messages delivered will be much higher - maybe about a hundred times higher?<p>In the model in the article, the load reports can ride along with an RPC reply (when active) or health check reply (when not), so I don&#x27;t think they cost much to send or receive.<p>&gt; I was a bit confused by this, because i don&#x27;t think it&#x27;s a lot of state (one number per backend), so it doesn&#x27;t seem like it would be hard to maintain and access.<p>It might be more about lock contention than total CPU usage.<p>&gt; As i said, you keep a set of connections open, distribute requests between them using P2C or a weighted random choice, but also periodically update the set according to load statistics. This means you&#x27;re not stuck with a static set of backends.<p>Sorry, somehow I missed the second paragraph of your original message. But it seems like you wouldn&#x27;t want those periodic updates to be too slow (not correcting problems quickly enough) or too fast (too much churn, maybe also &quot;thundering herd&quot; causing oscillations in a backend task&#x27;s channel count). I&#x27;m not sure if a sweet spot exists or not without some prototyping. Maybe it is workable. But Google and Twitter both have described approaches to having a good-enough subset choice right from the beginning, so why mess around with the periodic updates and the extra moving parts and state (IGMP memberships and&#x2F;or the aggregator thing)?')