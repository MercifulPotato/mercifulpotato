Item(by='Kim_Bruning', descendants=None, kids=[25521504], score=None, time=1608737079, title=None, item_type='comment', url=None, parent=25516573, text='The Paperclip Maximizer is a thought experiment that shows that if you apply a powerful AGI to something completely innocuous, things can go hilariously wrong over time.<p><a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;tag&#x2F;paperclip-maximizer" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;tag&#x2F;paperclip-maximizer</a><p>People are amused to read the story, and then go on their way.<p>But what happens if you apply a rather less powerful AI to something else even just a little bit less innocuous? (like optimizing to grab people&#x27;s attention, as is happening here).<p>I think you can get spectacular results.<p>And of course even if the AI is low powered, it can still work as an effective amplifier for human activity, both good and bad:  (See: Myanmar, Cambridge Analytica, etc)')