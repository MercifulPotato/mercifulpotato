Item(by='jlokier', descendants=None, kids=None, score=None, time=1611882143, title=None, item_type='comment', url=None, parent=25947244, text='&gt; the issue of the key leaking is still a problem with your proposed protocol<p>Fair enough.  I intended that it&#x27;s not a long term key, but something more appropriate like a session key.<p>Or better, more like a &quot;query key&quot; that limits what can be extracted from a session to whatever has been approved to be extracted.  (See &quot;zero-knowledge database&quot;.[1])<p>&gt; What you&#x27;re looking for is a protocol where N parties have to agree in order to decrypt any given message, and agreement to decrypt a particular message doesn&#x27;t allow them to decrypt any other messages<p>That&#x27;s what I meant, yes.  Sorry for not making that clear.<p>&gt; Same problem with giving the key only to an AI: The AI needs the key, and it will be difficult to ensure that the AI system isn&#x27;t hacked<p>Ah... The other thing I didn&#x27;t make clear is that the AI runs inside homomorphic encryption[2], or other protective bubble against access (one can imagine a quantum state with this property).  This is why I said we don&#x27;t have the technology to do it yet.  Not because of the AI, but because we don&#x27;t have sufficiently powerful methods to run an AI (or any large program) inside a bubble that prevents them from being inspected.  But we know it&#x27;s possible in principle.<p>[1] <a href="https:&#x2F;&#x2F;www.cs.jhu.edu&#x2F;~susan&#x2F;600.641&#x2F;scribes&#x2F;lecture15.pdf" rel="nofollow">https:&#x2F;&#x2F;www.cs.jhu.edu&#x2F;~susan&#x2F;600.641&#x2F;scribes&#x2F;lecture15.pdf</a><p>[2] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Homomorphic_encryption" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Homomorphic_encryption</a>')