Item(by='joshuamorton', descendants=None, kids=None, score=None, time=1604880928, title=None, item_type='comment', url=None, parent=25029998, text='For feature analysis (should we enable this new feature) I think Bayesian approaches are far better.<p>I work in a slightly different space, which is qualification of builds (this assumes you have feature flags and code, and the code shouldn&#x27;t enable features, but it may include refactoring, as well as the code that will later be enabled by a flag etc.)<p>For this, I originally wanted to push my org to do things in a more Bayesian, but it didn&#x27;t work. What did work was forcing org leaders to sit down and actually decide how costly true positives were, and therefore how much developer time they were willing to sink into chasing ghosts.<p>If you can say &quot;we expect to spend 4 hours of developer time investigating an alert here&quot;, and 1&#x2F;10 alerts will be real at our current sensitivity, then you can decide if these things are alright.<p>Ultimately we can&#x27;t control all the variables (our sensitivity requirements are informed by things like SLOs that we don&#x27;t directly control), but it does help make informed decisions about prioritization.<p>The type m&#x2F;s stuff is really neat, although in my space we don&#x27;t care too much about those, although that may be because we have conventionally gargantuan sample sizes.')