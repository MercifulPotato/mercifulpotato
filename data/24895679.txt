Item(by='syllogism', descendants=None, kids=[24896431], score=None, time=1603717317, title=None, item_type='comment', url=None, parent=24895404, text='Sorry you lost time on this!<p>We took a long time to get Thinc documented and stable, because there was a long period where I wasn&#x27;t sure where I wanted the library to go. The deep learning ecosystem in 2018 was pretty hard to predict, and we didn&#x27;t want to encourage spaCy users to adopt Thinc as their machine learning code if we weren&#x27;t sure what its status would be. So we actually never really got Thinc v7 stablised and documented.<p>This actually became a real issue in the previous version of spacy-transformers. It meant we were pushed into a design for spacy-transformers that really didn&#x27;t work well. The library wasn&#x27;t flexible enough, because there was no good way to interact with the transformers at the modelling level.<p>Pretrained transformers are interesting from an API perspective because you really don&#x27;t want to put the neural network in a box behind a higher-level API. You can use the intermediate representations in many different ways, so long as you can backprop to them. So you want to expose the neural networking.<p>Thinc v8 was redesigned and finally documented earlier this year: <a href="https:&#x2F;&#x2F;thinc.ai" rel="nofollow">https:&#x2F;&#x2F;thinc.ai</a> . We now have a clear vision for the library: you can write your models in the library of your choice and easily wrap them in Thinc, so spaCy isn&#x27;t limited to one particular library. For spaCy&#x27;s own models, we try to implement them in &quot;pure Thinc&quot; rather than a library like PyTorch or Tensorflow, to keep spaCy itself lightweight (and to stop you from having to juggle competing libraries at the same time).<p>So, it&#x27;s not quite true that we removed the docs for Thinc v7. We actually didn&#x27;t have a good solution to do the things you needed to do in the previous spacy-transformers, which  prompted a big redesign.')