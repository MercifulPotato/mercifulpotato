Item(by='jamesblonde', descendants=None, kids=None, score=None, time=1606030302, title=None, item_type='comment', url=None, parent=25175153, text='&gt;It&#x27;s somewhat unclear to me, but I think the combination of these statements means &quot;S3 is always treated as a block store, but sometimes the File == Variably-Sized-Block == Object. Is that right?<p>If the file is &quot;small&quot; (under a configure size, typically 128KB), it is stored in the metadata-layer, not on S3. Otherwise, if you just write the file once in one session (and it is under the 5TB object size limit in S3), there will be one object in S3 (variable size - blocks in HDFS are by default fixed size). However, if you append to the file, then we add a new object (as a block) for the append.<p>We have a new version under development (working prototype) where we can (in the background) rewrite all the blocks in a single file as a single object, and make the object readable by a S3 API. It will be released some time next year. The idea is that you can mark directories as &quot;S3 compatible&quot; and only pay for rebalancing those ones as needed. But you then have the choice of doing the rebalancing on-demand or as a background task, and prioritizing, and so on. You know the tradeoffs. \nYes, it would be easier to do this with GCS. But we did AWS and Azure first, as we feel GCS is more hostile to third-party vendors. The talks we have given at google (to the colossus team a couple of years ago and to Google Cloud&#x2F;AI - <a href="https:&#x2F;&#x2F;www.meetup.com&#x2F;SF-Big-Analytics&#x2F;discussions&#x2F;5766650486456320&#x2F;chat&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.meetup.com&#x2F;SF-Big-Analytics&#x2F;discussions&#x2F;57666504...</a> ) are like black holes of information transfer.')