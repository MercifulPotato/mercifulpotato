Item(by='LukeShu', descendants=None, kids=None, score=None, time=1607060533, title=None, item_type='comment', url=None, parent=25298521, text='(You&#x27;re making me remember things I haven&#x27;t thought about in a long time :) my apologies if some details are off)<p>The simple definition of a shader is &quot;a shader is a program that runs on the GPU instead of the CPU&quot; (or at least is intended to run on the GPU instead of the CPU; if you write a shader that makes use of a feature your GPU doesn&#x27;t support, Mesa will actually use LLVM to compile it to run on the CPU instead.  Performance will be terrible, but at least the program will run).  The compilation is only involved when shaders are involved, though on many graphics cards the driver will implement various apparently-non-shader things as shaders under the hood.<p>In the early days of OpenGL, it had what we now call a &quot;fixed pipeline&quot; where the GPU isn&#x27;t very configurable, you feed it geometry and textures, configure a few knobs, and then let it run.  You could feed it geometry (either polynomial curves or polygon vertices), then the first stage of the pipeline would approximate all the curves as polygon <i>vertices</i>.  Then the second stage would take that, and you&#x27;d feed it some lighting information and feed it some textures, and would render each polygon to an individual <i>fragment</i>.  Then the third stage would composite those fragments together in to the <i>framebuffer</i>.  Which it would then presumably display on your screen.  You didn&#x27;t quite have to do things exactly that way, but it was a set of large-ish &quot;blocks&quot; that weren&#x27;t terribly configurable.<p>And then very creative people would say &quot;I wish I could change this little aspect of how it processes vertices&quot; or &quot;... of how it processes fragments&quot;.  And so some hardware vendor would implement another knob on their GPU to configure a specific aspect of one of the stages in the pipeline, make up their own OpenGL extension to configure that knob, and if the OpenGL Architecture Review Board liked that extension, it might become a core part of the next minor release of OpenGL.  So it progressed toward a &quot;configurable pipeline&quot;.<p>Then they ended up with enough knobs that they said &quot;Let&#x27;s just let them write code that will run on the GPU to process the vertices and fragments, instead of adding more and more knobs&quot;.  So in OpenGL 2.0, we got vertex shaders and fragment shaders.  The OpenGL spec specified a C-like language, GLSL, which the graphics card driver would compile in to code that would run directly on the GPU.  And so you could run your own &quot;vertex shader&quot; instead of using the vertex-processing behavior that was baked-in to the graphics card in OpenGL 1.  You could write the original baked-in behavior as a shader; and so that&#x27;s what is happening under-the-hood in the drivers for most modern graphics cards.  So now we have a &quot;programmable pipeline&quot;.  At this point they were called shaders because largely what they allowed you to accomplish was, well, fancy shading.<p>And then very creative people wanted to be able to specify the behavior at more parts of the pipeline, and so OpenGL grew geometry shaders and pixel shaders and whatnot.  And so that&#x27;s the general direction that OpenGL is going, carving out more and more pieces of what used to be &quot;part of&quot; OpenGL and letting you replace it with your own shader code.  And then we even got compute shaders that don&#x27;t even have anything to do with graphics, but let you run general-purpose computation on the GPU instead of the CPU.  So the word &quot;shader&quot; a bit of a misnomer these days.<p>I got a lot out of CS 334 with Voicu Popescu at Purdue, but I&#x27;m not sure if much course material from that is online.  Also, I find the OpenGL specs to be surprisingly approachable, but I spend a lot of time reading specs so maybe that&#x27;s just me.')