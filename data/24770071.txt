Item(by='qsort', descendants=None, kids=[24771025, 24772058], score=None, time=1602621636, title=None, item_type='comment', url=None, parent=24769088, text='&gt; it would not be somehow wrong to extend the concept of intelligence to a certain class of machines, if it turns out to be useful and informative to do so.<p>No objections, but it&#x27;s a pretty big &quot;if&quot;. You could restate my point as &quot;there is no evidence it is in fact useful&quot;.<p>&gt; I take it, then, that you don&#x27;t agree with the sort-of Platonist view that algorithms have an existence independently of any implementation? I&#x27;m on the fence, myself, but lean towards the Platonist side.<p>I don&#x27;t, my position is essentially formalist. While I believe most research mathematicians would side with me here, your position is absolutely valid; famously, Kurt Godel was a Platonist, as are many others. My only observation here is that even from a Platonist point of view you are not really rejecting formalism, at least in the sense that while you don&#x27;t agree it&#x27;s the only view, I find it impossible to argue that one <i>can&#x27;t</i> view mathematical objects as formal constructs.<p>&gt; &quot;What&#x27;s happening in my brain is something we don&#x27;t have full scientific knowledge of, but we know it&#x27;s not computable.&quot; - but while the former is true, the status of the latter is not yet decided, so they are not identical propositions.<p>No, I don&#x27;t really hold that view. I probably worded that badly. My position is that the latter is unknown, and I would be content to accept that my brain is not fundamentally different than an algorithm if you showed me an algorithm that can effectively emulate my brain within an acceptable margin of error. Ironically, if it were possible to do that, it would be proof there is no &quot;intelligence&quot;, only &quot;computability&quot;, making the first entirely redundant.<p>&gt; One could say the same about specific ontologies -<p>Again, this is really thin ice. I don&#x27;t really know what to think about this because it&#x27;s getting too abstract for my monkey brain, but it&#x27;s certainly not obvious that a better model implies an ontological line between itself and reality. To put it bluntly, is a better model really &quot;more true&quot;, i.e. qualitatively different from a worse one?<p>I&#x27;ll take the seventh.')