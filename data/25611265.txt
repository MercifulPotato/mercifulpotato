Item(by='polm23', descendants=None, kids=None, score=None, time=1609575491, title=None, item_type='comment', url=None, parent=25611252, text='This is an interesting surprise about good old word vectors. From the README:<p>&gt; Although continuous bag of word (CBOW) embeddings can be trained more quickly than skipgram (SG) embeddings, it is a common belief that SG embeddings tend to perform better in practice. This was observed by the original authors of Word2Vec [1] and also in subsequent work [2]. However, we found that popular implementations of word2vec with negative sampling such as word2vec and gensim do not implement the CBOW update correctly, thus potentially leading to misconceptions about the performance of CBOW embeddings when trained correctly.<p>The upshot is that they get similar results with CBOW while training three times faster than skipgram.<p>Given the popularity of Transformers, and that Fasttext exists, I&#x27;m curious as to what inspired them to even try this, but it&#x27;s certainly an interesting result. There&#x27;s so much word vector research that relies on quirks of the word2vec implementation.')