Item(by='einpoklum', descendants=None, kids=[25863529], score=None, time=1611258990, title=None, item_type='comment', url=None, parent=25857253, text='&gt;  That [the method of statistical analysis] could be a problem in our particular context didn’t dawn on me and my colleagues — nor on anyone else in the field — before [whoever]&#x27;s discovery.<p>This is troubling to me. It does not seem like the bias was due to something arcane, some finer point in advanced statistics, something hidden from view etc. The poster says:<p>&gt; It involved regression towards the mean — when noisy data are measured repeatedly, values that at first look extreme become less so.<p>This may not be 100% straightforward when it&#x27;s buried in the middle of a paper, but if you actually consider the methodolgy you are likely to notice this happening.<p>So here&#x27;s what _I_ learn from this case:<p>* It is possible that the reviewers at Nature don&#x27;t properly scrutinize the methodological soundness of some submissions (I say &quot;possible&quot; since this is a single example, not a pattern)<p>* PhD avisors, like the author&#x27;s, may not be exercising due dilligence on statistical research done with their PhD candidates. The author&#x27;s advisor had this to say:<p>&gt; &quot;It&#x27;s great that we’ve persisted in attempting to understand our methodology and findings!”<p>so he says it&#x27;s &quot;great&quot; that they did not fully understand their methodology before submitting a paper using it. Maybe that&#x27;s not exactly what he meant, but still, pretty worrying.')