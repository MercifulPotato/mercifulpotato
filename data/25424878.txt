Item(by='drdeca', descendants=None, kids=[25435401], score=None, time=1607988776, title=None, item_type='comment', url=None, parent=25422077, text='I was curious how much the entropy is reduced.<p>As a simplifying assumption, assume everyone agrees about which of any 2 strings are more memorable.<p>If someone takes m random samples, and of those, takes the one they find most memorable, how much does this reduce the entropy? If there are N possible strings, and so with a uniform distribution there would be, uh, -log_2(1&#x2F;N) bits of entropy, I think(?) (because, summing over the N terms of -(1&#x2F;N) * log_2(1&#x2F;N) , gives a total of log_2(N) )  \nIf one takes the maximum of m samples, what does that look like? The cdf of the uniform distribution over the terms (identified with their order in the list ordered by memorability) would be P[x \\le a] = a&#x2F;N , and with m independent samples , P[max(x_1,x_2,...,x_m) \\le a] = (P[x \\le a])^m = (a&#x2F;N)^m = (1&#x2F;N)^m a^m,\nand so the pdf would be, around (1&#x2F;N)^m * m * a^(m-1) (approximating it as continuous because N is large. I am not sure that this is a reasonable approximation.)  \nThen, the sum becomes, uh, again approximating as continuous, integrating from a from 0 to N, (1&#x2F;N)^m * m * a^(m-1) * (-1) * log_2((1&#x2F;N)^m * m * a^(m-1)) da ,\nwhich is integral of (1&#x2F;N)^m * m * a^(m-1) * (-1) * ( m<i>log_2(1&#x2F;N) + log_2(m) + (m-1)</i>log_2(a)) da\nwhich is, (m<i>log_1(1&#x2F;N) + log_2(m)) + integral of (1&#x2F;N)^m </i> m<i>(m-1) </i> a^(m-1)*log_2(a) da ...<p>uh..... ok I just threw wolframalpha at it, and I got, -log_2(m&#x2F;N) + ((m-1)&#x2F;(m ln(2)))  \nwhich, subtracting that from the initial -log_2(1&#x2F;N) , gives log_2(m) - ((m-1)&#x2F;(m ln(2))),<p>and that &quot;((m-1)&#x2F;(m ln(2)))&quot; is about like, 1 or 2 or therabouts (it is 0 if m=1 of course).<p>so, if all the perhaps questionable approximations I made didn&#x27;t mess this all up (and I didn&#x27;t mess this up in some other way), I think that says that, if you pick the most memorable out of m random strings, by doing so you reduce the entropy by about log_2(m) + 1 bits.<p>That doesn&#x27;t sound too bad to me, really. Well, I suppose it depends how many bits you have to spare, and how big of an m you pick.')