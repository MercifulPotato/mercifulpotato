Item(by='zaroth', descendants=None, kids=[25458393], score=None, time=1608223257, title=None, item_type='comment', url=None, parent=25452567, text='I wonder what is the primary intended role of Google&#x27;s &quot;Ethical AI&quot; efforts in the first place.<p>You could hire a bunch of leading AI researchers and task them to do R&amp;D to <i>identify</i> the pitfalls of latest-gen AI with a focus on prospective dangers, biases, unintended side-effects. You could ask them to create new methods to <i>quantify</i> the degree &#x2F; severity of these effects. You could ask them to <i>invent</i> new methodologies to specifically protect against these effects.<p>Separate from all of this, they may or may not be tasked with a <i>governance</i> function to review or score internal models according to their metrics and produce impact reports. Perhaps they work side-by-side with AI developers to ensure their newly developed techniques are applied throughout the processing pipeline, etc.<p>The (non-expert) impression I get from ethical AI research is that it hasn&#x27;t moved much past the obviousness phase. The revelation that the source material for your training set determines the behavior of your model. That models are not neutral or granted some higher perspective just because they&#x27;re some sort of <i>magic algorithm</i> because ultimately they are statistical engines driven by their datasets. That models can be misused and misapplied if the end-user treats them so. And that models cost a lot of resources to train, which <i>like anything</i> can be translated into tons of CO2 emissions.<p>A more nuanced analysis would look at the cost&#x2F;benefit of what the model achieves versus the energy cost or potential bias that would be present in any alternative method. For example, it&#x27;s a strict win&#x2F;win situation if GPT-3 takes 10 million tons of CO2 to train, but displaces work that otherwise would have taken 11 million tons of CO2. All models will have bias -- &quot;bias engine&quot; is actually a good way to describe the fundamental nature and purpose of the algorithm! But at least models can have strictly <i>measureable</i> and totally consistent levels of bias that perhaps can even be controlled for as desired, unlike manual human labor which will demonstrate wildly variable and much harder to control levels of bias. See, for example, the controversial use of AI models to set bail. [1]<p>As I side note, I hate the opening paragraph of TFA;<p>&gt; <i>Tension between Google&#x27;s Ethical AI group and executives at the company rose on Wednesday, as employees sent a list of demands regarding the recent departure of one of the team&#x27;s leaders â€” a prominent Black woman in a field that&#x27;s largely White and male.</i><p>That level of race baiting in an Op Ed would deem it unpublishable. The effect (if not the intent) of that opening is to essentially shut off the reader&#x27;s critical thinking skills before they even get going.<p>[1] - <a href="https:&#x2F;&#x2F;www.nytimes.com&#x2F;2017&#x2F;12&#x2F;20&#x2F;upshot&#x2F;algorithms-bail-criminal-justice-system.html" rel="nofollow">https:&#x2F;&#x2F;www.nytimes.com&#x2F;2017&#x2F;12&#x2F;20&#x2F;upshot&#x2F;algorithms-bail-cr...</a><p>[1] - <a href="https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;algorithms-supposed-fix-bail-system-they-havent&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;algorithms-supposed-fix-bail-sys...</a>')