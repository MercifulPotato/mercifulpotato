Item(by='StefanKarpinski', descendants=None, kids=[24749390, 24748533, 24748433, 24749387, 24748360], score=None, time=1602444328, title=None, item_type='comment', url=None, parent=24747092, text='You may be missing the point of the comparison, which is this: to compare the performance of the de facto standard CSV parsers of each system when used the way that a typical user would use them, i.e. to parse a CSV file into a data frame.<p>Regarding your specific points:<p>1. Yes, it&#x27;s possible to rig up some way of parsing in parallel in Python, like splitting the CSV file into multiple files and then parsing them in separate processes and then joining that. But no matter how you do it, you have to jump through extra hoops to do it. And that is not something most users do.<p>2. I don&#x27;t entirely follow this point. Perhaps using PyArrow&#x27;s parser would be faster than what is timed here, but is that what the typical Python data science user would do? Most Python users use Pandas and the CSV parsing that it provides. If that changes, then it would be good to do a new benchmark.<p>3. Yes, floating-point decoding is hard, but Julia&#x27;s is fully accurate, so if it&#x27;s also faster than Python&#x27;s how is it an unfair comparison? If Julia was cheating by doing imprecise float parsing, sure, that would be a valid complaint, but it&#x27;s not.<p>These &quot;conflations&quot; sound suspiciously like excuses. If float parsing is slow, make it faster. If concrete object decoding is slow, stop doing that. If threading would be faster, bite the bullet and use threading. If the language doesn&#x27;t provide good support for these things, then maybe the language is at fault.')