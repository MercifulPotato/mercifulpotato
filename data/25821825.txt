Item(by='szemet', descendants=None, kids=[25821991], score=None, time=1610978387, title=None, item_type='comment', url=None, parent=25821559, text='&gt; Using extension modules is just a time-tested, highly organized, modular, robust design pattern.<p>I really don&#x27;t get this. I&#x27;am fully on the side that limitations may increase design quality. E.g I accept the argument that Haskell immutability often leads to good design, I also believe the same true for Rust ownership rules (it often forces a design where components have a well defined responsibility: this component only manages resource X starting from { until }.)<p>But having a performance boundary between components, why would that help?<p>E.g. This algorithm will be fast with floats but will be slow with complex numbers. Or: You can provide X,Y as callback function to our component, it will be blessed and fast, but providing your custom function Z it will be slow.<p>So you should implement support for callback Z in a different layer but not for callback X,Y, and you should rewrite your algorithm in a lower level layer just to support complex numbers. \nWill this really lead to a better design?')