Item(by='GregarianChild', descendants=None, kids=[25979747], score=None, time=1612106815, title=None, item_type='comment', url=None, parent=25974157, text='An interesting question is: why are we not seeing                 \nfast and scalable GPU implementations of GAs? (I&#x27;m not trying to trick     \nyou, genuinely curious).<p>I think one problem, making GAs as they are understood today (maybe there is a better way),  is the freewheeling, unconstrained nature of GA        \nfitness functions. If the ambient fitness function has a lot of            \ndata-dependent branching, SIMD will be slow. Moreover GAs have two         \nphases: measuring fitness and mutation&#x2F;crossing over to produce the next            \ngeneration. Is the mutation&#x2F;crossover phase SIMD friendly? Typically, it            \ninvolves randomness ...<p>In contrast, neural net training is essentially GeMM (General Matrix       \nMultiply) which is an incredibly predictable workload -- almost no         \ndata dependent branching. So none of the things that make processors       \nslow and complicated (caches, prefetching, branch prediction,              \nreservation station, scheduler ...)  is needed: instead you can just       \nstream the data from main memory directly into the matrix multiplier       \n(tensor core). This means you can use the available transistors (and       \nenergy) much more efficiently.<p>As far as I am aware, GAs have not, so far, been reduced to as simple       \nuniform and predictable a workload as GeMM, which I believe is why GAs     \ndon&#x27;t usually get run on a GPU (not to mention more specialised            \nhardware like TPUs).<p><pre><code>   run each machine almost \n   entirely independently.\n</code></pre>\nI am not sure how this works with mutation&#x2F;crossover. Learning happens by &#x27;throwing away&#x27; unpromising search avenues (genes). So if different machines do not synchronise their populations, they don&#x27;t learn from each other. Synchronisation OTOH is very slow.')