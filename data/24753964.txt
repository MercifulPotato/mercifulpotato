Item(by='StefanKarpinski', descendants=None, kids=[24754529, 24758588], score=None, time=1602507888, title=None, item_type='comment', url=None, parent=24748582, text='There are two potential issues that this might disregard:<p>1. cold file cache<p>2. JIT compile time<p>The cold&#x2F;hot file cache issue affects all languages equally, so it doesn’t invalidate the comparison. It is possible that CSV file is not in cache and a system&#x27;s disk I&#x2F;O is slow enough that it becomes a bottleneck, which would make all parsers equally slow. However, this is not very realistic because CSV files—especially large enough ones where you care about speed—are almost always compressed—So you&#x27;re not going to be bottlenecked on disk read. Assuming that compressed disk read + decompression is fast enough to keep the data flow high, you&#x27;re back to CSV parsing being the bottleneck.<p>The JIT compile cache only affects Julia. The reason is it not included in the benchmark results is because it is a small, fixed overhead: it is only paid on the first CSV file read (no matter the size), and if you read a larger data set, the compile time does not increase. Since the point of benchmarks is typically to project from a smaller case how long it would take to do even larger tasks, you don&#x27;t want to include small, fixed overheads.<p>For some concrete numbers, I just timed reading a tiny CSV file on my MacBook Pro 2018 and the first read, including compile time took 4 seconds. The second read took 0.000347 seconds. So that&#x27;s the fixed overhead we&#x27;re talking about here for reading a CSV file: about 4 seconds on the very first CSV file you read. People can be the judge of whether that&#x27;s a showstopper for them or not.')