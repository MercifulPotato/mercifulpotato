Item(by='theptip', descendants=None, kids=None, score=None, time=1612225990, title=None, item_type='comment', url=None, parent=25993524, text='Thanks for getting in touch, happy to share.<p>1. We ended up using GCP&#x27;s hosted Composer to get started more quickly, which doesn&#x27;t seem to have been updated to Airflow 2.0 yet. I&#x27;ll put that on the list for evaluation.<p>2. A few usecases that I immediately hit complexity walls on:<p>A) Having a &quot;staging&quot; version of our pipelines so that we don&#x27;t break the prod ETL; it was really difficult to find a canonical method for having common DAG code that&#x27;s parameterizable per env. The fact that all of the DAGs live side-by-side in the same directory means I have to run the same job for a &quot;prod push&quot; as a &quot;staging push&quot; (i.e. if I get the staging deploy wrong I could break prod). Given that we deploy version vN+1 to staging, check it&#x27;s working, and only then deploy vN+1 to prod, we ended up with some weird config injection code to let us have two folders containing copies of the same DAG scripts with different config. This just felt janky.<p>B) Managing Python dependencies between different apps was also painful; for example we wanted to add Meltano, and so that app brings in a bunch of deps, which broke our main dags when I naively updated the main python pip env to install the new meltano requirement. Using the K8s operator lets us effectively have a venv per dag but the pattern of using one python env across the whole Airflow install bit me very early on and seemed pretty unscalable.<p>3. I haven&#x27;t looked at KEDA, I&#x27;ll take a look.<p>4. We&#x27;re using GCP Composer for now, though I looked at the Helm chart too.')