Item(by='redsymbol', descendants=None, kids=None, score=None, time=1610583922, title=None, item_type='comment', url=None, parent=25757398, text='A good solution might be an &quot;exponential back-off&quot; temporary ban.<p>In this, an open-source project adopts a policy that the mental health and wellbeing of its developers is a priority, and in particular abusive language from end users is not accepted. Specifically, if a user communicates with such negativity (e.g. commenting this way on a github issue), two things happen:<p>1) That comment is deleted<p>2) That user is prohibited from posting on this project&#x27;s issues for 30 days.<p>That&#x27;s for the first offense. If, after 30 days, the user does something like this again, they are temporarily banned for 60 days. And then 120.<p>You get the idea.<p>If you don&#x27;t like 30 days as a base, then it can be 24 hours, or 90 days, or whatever the project decides is appropriate. Regardless of these or other parameters, this strikes a balance between accepting needed feedback from the community, and &quot;canceling&quot; someone from contributing useful feedback in the future.<p>In fact, I bet you this will have a role in conditioning certain people to communicate in more nurturing and kind ways. A permaban would likely NOT do that, but a short temporary ban that increases on repeat offenses probably will.<p>I am not sure if Github supports this already, or provides some mechanism that could be used by the project maintainers to manually implement it...<p>But if you are developing on any platform where end users sometimes communicate toxically with volunteer developers just trying to make the world a better place, maybe this idea is worth considering.')