Item(by='ninjin', descendants=None, kids=[25690703, 25690169], score=None, time=1610117911, title=None, item_type='comment', url=None, parent=25684694, text='Yikes, you are right… I just linked a private repo. &#x27;^^ I have poked the rest of the group and it seems that at least a tweet was made [1] – but not much else remains. Describing it from memory, we ran ELMo and BERT on Wikipedia and then allowed similarity search between a query and showed heat maps to a matched context. Nothing particularly deep compared to yours that go into the transformer “machinery”, but I think it captures very well how most Question Answering models still operate: Embed query and contexts in a high-dimensional space, compare, find semantically plausible span, and done!<p>[1]: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;Johannes_Welbl&#x2F;status&#x2F;1065309654740361217" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;Johannes_Welbl&#x2F;status&#x2F;106530965474036121...</a><p>Work and articles like yours has truly had an impact on me, even though they are largely qualitative. We always say “Turing complete” this and “Turing complete” that, but theoretical statements such as this have little practical utility to me as we all know that what can be learnt and what is learnt are two very different things. For example, “Visualizing and Understanding Recurrent Networks” by Karpathy et al. (2015) [2] that you list as inspiration blew my mind in terms of for example neurons that monotonically decrease from the sentence start. I remember Karpathy giving a talk on it in London and what struck me was how he simply had gone to manually inspect the neurons manually (heresy!) as there were only a few thousand of them any way. That playfulness, truly admirable.<p>[2]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1506.02078" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1506.02078</a><p>Another anecdote, now from “Attention Is All You Need” by Vaswani et al. (2017) [3] where I was far from sold on Transformers as a model until Uszkoreit gave a talk at an invitation-only summit where he showed those cherry-picked attention heads that “flipped” based on whether an object was animate or not. I approached him after the talk and asked why it was not in the paper as it was awesome! Maybe I am biased because I give a large role to intuition in science, but analysis such as this is far more valuable to me as a researcher than yet another point of BLEU or a 10th dataset. Again, my bias, but I feel that there is a need for new ways of thinking in terms of both “hard” empiricism and “soft” analysis in machine learning as we seemingly are now having to mature given the attention we are receiving.<p>[3]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1706.03762" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1706.03762</a><p>Apologies if I am rambling, it is midnight now and I barely slept last night.')