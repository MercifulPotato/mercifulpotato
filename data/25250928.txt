Item(by='baryphonic', descendants=None, kids=None, score=None, time=1606713206, title=None, item_type='comment', url=None, parent=25250638, text='&gt; This is a good exposition of some formal definitions for &#x27;interpretability&#x27; in the context of machine learning, but I am still not really clear on why such a property is necessary or even desirable in the context of high dimensional statistical learning algorithms.<p>Because the models can fail, and we want to know how to prevent them from failing further. In a pure black box model, we know about the test and validation runs, and not much else. When Google thus deploys a model that classifies accounts as toxic or not and then cancels the toxic ones, regardless of how many domains you manage or YouTube followers you have or even whether you have a YouTube TV subscription, you&#x27;d prefer knowing why the model chose to give you the axe. You might even prefer a &quot;human in the loop&quot; when the system makes a call but doesn&#x27;t really have confidence.<p>For certain areas like NLP, sure, it&#x27;d be tough. But for CV tasks or many other ML tasks, some form of explanation would be invaluable and much more (human) user-friendly.')