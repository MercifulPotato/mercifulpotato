Item(by='YeGoblynQueenne', descendants=None, kids=None, score=None, time=1603230063, title=None, item_type='comment', url=None, parent=24827804, text='I like to quote Yann LeCun on how neural nets work like our minds:<p><i>IEEE Spectrum: We read about Deep Learning in the news a lot these days. What’s your least favorite definition of the term that you see in these stories?</i><p><i>Yann LeCun: My least favorite description is, “It works just like the brain.” I don’t like people saying this because, while Deep Learning gets an inspiration from biology, it’s very, very far from what the brain actually does. And describing it like the brain gives a bit of the aura of magic to it, which is dangerous. It leads to hype; people claim things that are not true. AI has gone through a number of AI winters because people claimed things they couldn’t deliver.</i><p><a href="https:&#x2F;&#x2F;spectrum.ieee.org&#x2F;automaton&#x2F;artificial-intelligence&#x2F;machine-learning&#x2F;facebook-ai-director-yann-lecun-on-deep-learning" rel="nofollow">https:&#x2F;&#x2F;spectrum.ieee.org&#x2F;automaton&#x2F;artificial-intelligence&#x2F;...</a><p>Also, like I say above, it doesn&#x27;t matter how many images a human sees- what matters is how many she needs to see before learnign to recognise a thing. The example of an unchanging, two-dimensional caricature of a horse is evidence enough that, even if we do see billions of &quot;images&quot; as you say (I&#x27;m not sure it makes sense to speak of &quot;images&quot; int the sense you use it) we don&#x27;t need to see all those billions of them before we learn what things look like.')