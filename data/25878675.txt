Item(by='fossuser', descendants=None, kids=[25878944], score=None, time=1611365935, title=None, item_type='comment', url=None, parent=25878442, text='I&#x27;m confused - I thought the reason that it&#x27;s hard for Intel to add more decoders is because x86 ISA doesn&#x27;t have fixed length instructions. As a result you can&#x27;t trivially scale things up.<p>From that linked article:<p>--<p>Why can’t Intel and AMD add more instruction decoders?<p>This is where we finally see the revenge of RISC, and where the fact that the M1 Firestorm core has an ARM RISC architecture begins to matter.<p>You see, an x86 instruction can be anywhere from 1–15 bytes long. RISC instructions have fixed length. Every ARM instruction is 4 bytes long. Why is that relevant in this case?<p>Because splitting up a stream of bytes into instructions to feed into eight different decoders in parallel becomes trivial if every instruction has the same length.<p>However, on an x86 CPU, the decoders have no clue where the next instruction starts. It has to actually analyze each instruction in order to see how long it is.<p>The brute force way Intel and AMD deal with this is by simply attempting to decode instructions at every possible starting point. That means x86 chips have to deal with lots of wrong guesses and mistakes which has to be discarded. This creates such a convoluted and complicated decoder stage that it is really hard to add more decoders. But for Apple, it is trivial in comparison to keep adding more.<p>--<p>Maybe you and astrange don&#x27;t consider fixed length instruction guarantees to be necessarily tied to &#x27;RISC&#x27; vs. &#x27;CISC&#x27;, but that&#x27;s just disputing definitions. It seems to be an important difference that they can&#x27;t easily address.')