Item(by='jedbrown', descendants=None, kids=[25064052], score=None, time=1605132755, title=None, item_type='comment', url=None, parent=25062650, text='PETSc developer here. You&#x27;re correct that we don&#x27;t have a sparse QR. I&#x27;m curious about the shapes in your problem and how you use the rank-revealed factors.<p>If you&#x27;re a heavy user of SuiteSparse and upset about the license, you might want to check out Catamari (<a href="https:&#x2F;&#x2F;gitlab.com&#x2F;hodge_star&#x2F;catamari" rel="nofollow">https:&#x2F;&#x2F;gitlab.com&#x2F;hodge_star&#x2F;catamari</a>), which is MPLv2 and on-par to faster than CHOLMOD (especially in multithreaded performance).<p>As for PETSc&#x27;s preference for processes over threads, we&#x27;ve found it to be every bit as fast as threads while offering more reliable placement&#x2F;affinity and less opportunity for confusing user errors. OpenMP fork-join&#x2F;barriers incur a similar latency cost to messaging, but accidental sharing is a concern and OpenMP applications are rarely written to minimize synchronization overhead as effectively as is common with MPI. PETSc can share memory between processes internally (e.g, MPI_Win_allocate_shared) to bypass the MPI stack within a node.')