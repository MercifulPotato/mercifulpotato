Item(by='wahern', descendants=None, kids=None, score=None, time=1604732143, title=None, item_type='comment', url=None, parent=25011444, text='IME, binary files are really only faster for fixed-sized, sequential layouts. Once you have to start parsing the data, the distinctions between binary and string formats, both in their semantics and performance, begin to quickly disappear.<p>JSON is so slow because of how people use JSON: they rely heavily on objects&#x2F;dictionaries instead of arrays. And they do that because it&#x27;s idiomatic for people to explode JSON into, basically, ASTs and then copy the values to native data structures, unless they simply use it directly, as is common in scripting languages. That parsing into a giant AST, operating on the AST, and then recomposing it back into a giant byte string before writing it out is what is ridiculously inefficient. You spend many more CPU instructions and much more memory bandwidth juggling pointers. And the library and implementation language abstractions mean you can only optimize for the characteristics of the abstraction, not of the actual problem at hand, which is where 80% of the optimization potential lies. No amount of zero-copy I&#x2F;O, compiler code elision, or other fancy techniques could ever compensate for that gap, not at scale.<p>It&#x27;s easy to maximize IOPS with fixed-field binary formats because it never even occurs to people to parse the messages into ASTs or to manipulate them as such. Whether or not a language supports type punning a byte buffer into a native data structure, as in C, you&#x27;ll naturally use an approach that relies on directly extracting and operating on scalars. Code flow will match the semantics of the actual data and the functional problem. But when a message format, whether text or binary, has dynamically sized fields, logically nested fields, and especially when it uses dictionaries&#x2F;associative arrays, most programmers&#x27; instinct is to use a generic parser that returns an abstract instance implemented using generic data structures, exposing generic interfaces for generic operations... because there&#x27;s a library for that, and programmers are taught to use libraries whenever possible.[1] It&#x27;s easy to find pre-existing solutions when you redefine the problem; in this case, redefining the problem to parsing JSON rather than parsing and manipulating the data at hand.<p>[1] And even if there isn&#x27;t a library for it, people end up creating one, because another rule is the importance of code and data encapsulation. And invariably what people end up encapsulating is ancillary technical characteristics, not the meat of the functional problem.')