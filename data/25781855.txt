Item(by='proverbialbunny', descendants=None, kids=[25789468], score=None, time=1610655296, title=None, item_type='comment', url=None, parent=25780621, text='Most libraries load entire notebooks from top to bottom when executing, and I believe papermill does too.  (Please correct me if I&#x27;m wrong, as I&#x27;ve not used papermill.)<p>This is great for making a dashboard, a report, or some other kind of analytics, but when it comes to a service the customer uses, you typically never want to load the whole notebook.  This is where the industry standard way of loading the whole notebook tends to fall on its face.<p>What we do is the cells that will end up in prod are written as functions inside of the notebook.  This helps reduce globals when writing the notebook, so it is good form when prototyping, but also it allows just those functions to be called from the notebook, instead of running the entire notebook.<p>You will probably want to write your own library to do this, but in the mean time there is one that works for this purpose <a href="https:&#x2F;&#x2F;github.com&#x2F;grst&#x2F;nbimporter" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;grst&#x2F;nbimporter</a>  (Ironically the author doesn&#x27;t recognize this use case.)<p>Using nbimporter you can import a notebook without loading it.  You can then call functions within that notebook and only those functions get loaded and called.<p>In my notebooks I have a process function which is like main(), but for for feature engineering.  On the prod side the process function is called from the notebook.  Process calls all of the necessary cells&#x2F;functions for me in the correct order.  This way the py wrapper only has to call one function, then the ML predict function gets called, so it&#x27;s pretty small on the .py wrapper side.  There are tests written on the .py side, IO functions and what not too.<p>Data engineers love their classes, so it&#x27;s easy to write a class that calls the notebook, and best of all calling a single function this way does not load globals, so the data engineers are happy.  It&#x27;s a nice library, because otherwisw you&#x27;d have to write your own (which you may end up wanting to do).<p>This way if the model doesn&#x27;t work as intended in production it&#x27;s my fault.  We log everything, so I can run the instance prod caught on my local machine, figure out what is going on, update the model, and then it can be deployed instantly.<p>Version numbers on the engineering side I can&#x27;t comment on as they have their own method, but on my end the second the model writes to a database then I strongly push for having a version number column or a version number metadata table in the database, so it&#x27;s easy for me to access for future analysis.')