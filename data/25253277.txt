Item(by='kavalg', descendants=None, kids=[25255663], score=None, time=1606741243, title=None, item_type='comment', url=None, parent=25250768, text='While I accept your counterargument (esp. regarding credit scores, autonomous vehicles and the like) I think interpretability in deep models is only meaningful if you have accurate, objective labels. But how often is this the case? For example, when people refer to concepts, they often rely on some established culture, a common understanding, rather than things you can measure. Let&#x27;s take the simple example of classifying spoiled fruit at the grocery store. You can train a ConvNet and it will probably learn to recognize some visual traits of &quot;spoiledness&quot;, but how objective can it really be, given that humans don&#x27;t always agree what &quot;spoiled&quot; really means? In other words, fruit is spoiled only when a large enough social group says it is spoiled. So if that is your label, then the model can only reflect the shared understanding of &quot;spoiledness&quot; in the given social group. Then interpretability can help you check if the model is looking at plausible things (e.g. the fruit, not the background). However, this won&#x27;t really tell you &quot;why&quot; the model thinks this fruit is spoiled. You can draw an interesting analogy between this and ensemble models, where the &quot;social group&quot; of the models in the ensemble forms the shared culture.<p>Also and interesting interpretability approach that the article did not mention is SHAP[0]<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;slundberg&#x2F;shap" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;slundberg&#x2F;shap</a>')