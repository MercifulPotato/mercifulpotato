Item(by='IfOnlyYouKnew', descendants=None, kids=[25293507, 25294712, 25294803, 25297801, 25293573, 25293434, 25298741], score=None, time=1607022355, title=None, item_type='comment', url=None, parent=25292845, text='I find it strange to take issue with the &quot;I&#x27;m so tired&quot; line. It would seem to be entirely appropriate for circumstances where you see an argument that you have replied to many times but that completely ignores your reply. It also just doesn&#x27;t strike me as particularly hostile or profane. Quitting twitter and making a show of it strikes me as a far more emotional reaction, but nobody here seems to take any issue with that?<p>In this case, the issue is that &quot;it&#x27;s the dataset&quot; is attacking a straw man: nobody is accusing ML practitioners to be explicit racists actively conspiring for their models to be racially biased. Everyone understands that it is (often) a result of the underlying dataset.<p>But, the argument goes, understanding the genesis of such bias isn&#x27;t enough to excuse it. At least not when those models are put to actual use in, say, law-enforcement contexts. Or credit agencies, or hiring decisions, or really anything.<p>If your dog happened to bite any red-haired kid it came across, it doesn&#x27;t matter that you think red-haired kids are just as good as others, or that the dog was once mistreated by a red-haired kid. You&#x27;re not going to allow the dog to get near any red-haired kids. And if you do, you&#x27;ll face charges of at least negligence when some red-haired kid gets mauled.<p>To tie this back to AI, it would mean that as long as your models produce racially biased results, they simply aren&#x27;t ready for deployment or publication. Go find better data until your work is no longer liable to inflict harm on anybody.<p>That&#x27;s the standard for all other industries: if your car is shown to reliably kill pedestrians below a certain height in crash tests, you go back to the drawing board. Adding a sticker &quot;don&#x27;t operate car in the vicinity of children&quot; isn&#x27;t enough.<p>You can disagree with that reasoning (although you&#x27;d be wrong). What you can&#x27;t do, as a leader in AI, is to pretend to have never heard of it, or that it is so obviously wrong that it warrants no reply whatsoever.<p>&gt; have to call out the &quot;I&#x27;m so tired&quot; bit every time I see it.<p>so... I guess you&#x27;re tired of it?')