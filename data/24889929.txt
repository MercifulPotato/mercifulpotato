Item(by='Voloskaya', descendants=None, kids=None, score=None, time=1603660471, title=None, item_type='comment', url=None, parent=24887952, text='You can indeed train state of the art text models with this data provided you come up with some better architecture that what we currently have. What I am saying is that you cannot train state of the ART &quot;GPT&quot; models like the link is saying.<p>Your link is for GPT-2, GPT-3 used much, much more data.<p>GPT-3 was trained in part on &quot;books2&quot; corpus which is not public but seems to basically be the same thing as this: 200k books * 100k words per book on average * ~3 token per words = 60B tokens, books2 is 55B tokens so it checks out.<p>The total amount of tokens that GPT-3 was trained on from all sources is a combined 500B tokens, this is merely 10% of what they have.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2005.14165.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2005.14165.pdf</a>')