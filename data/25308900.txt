Item(by='rprenger', descendants=None, kids=None, score=None, time=1607119940, title=None, item_type='comment', url=None, parent=25308165, text='In figure 2, those curves are all from one epoch each right?  Which is to say the model has never seen the same data twice?<p>It&#x27;s true you don&#x27;t need regularization if you&#x27;ve never seen the same data twice, but that&#x27;s a similar regularization to early stopping.  You&#x27;d expect the larger number of parameters would make the training error drop faster with fewer tokens as well due to improved optimizability.  But rather than &quot;larger models need less data&quot;, I&#x27;d say the take away is more &quot;larger models need fewer steps to optimize training error&quot;.  None of the models get &quot;good&quot; until they&#x27;ve seen a number of tokens similar to the number of parameters.<p>Unless you&#x27;ve run many epochs on small data sets and seen the same results, in which case that&#x27;s pretty weird&#x2F;cool.')