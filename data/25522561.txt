Item(by='cgreerrun', descendants=None, kids=[25523278], score=None, time=1608759769, title=None, item_type='comment', url=None, parent=25522106, text='&gt; Thanks for sharing!<p>You&#x27;re welcome.<p>&gt; Why did you use GBDTs instead of NNs?<p>I mostly wanted to build an implementation to see how it worked; I was more familiar with GBDTs than NNs, so I figured I&#x27;d start with that. At its heart, AlphaZero is the marriage of two great ideas: using a Monte Carlo Tree Search (MCTS) to efficiently look ahead and find good moves and using a powerful ML model (like a ResNet) as a bot&#x27;s intuition about which positions are good to be in (value network) and which moves are good when you&#x27;re in which positions (policy network). So if a GBDT is powerful enough for your use case, the &quot;ML Model&quot; component in the MCTS+ML Model AlphaZero setup should be able to be swapped out with it if you want.<p>But I was also curious if GBDTs would do almost as well as a NN, because GBDTs can be much more efficient w.r.t. cost&#x2F;energy. At the time when AlphaZero came out, I think it cost &gt;$10M to train a superhuman Go algo. Nowadays KatoGo [1] can do it for &lt;$50K. The most expensive part of training is the self play. You basically have bots play millions of games against each other and learn from the results of those games. Getting value&#x2F;policy predictions each move from the ML models is a majority of the computation during self play, so if you make that more efficient, you should be able to train a bot faster&#x2F;cheaper.<p>Check out this HN thread if you&#x27;re interested in more AlphaX shenanigans: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=23599278" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=23599278</a><p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;lightvector&#x2F;KataGo" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;lightvector&#x2F;KataGo</a>')