Item(by='breatheoften', descendants=None, kids=[25554371], score=None, time=1609100500, title=None, item_type='comment', url=None, parent=25542011, text='The article makes the claim that models which show similar train and test losses demonstrate minimal overfitting -- and are therefore less likely generally to exhibit a lot of text memorization.<p>I wonder the degree to which this inference is true in practice with respect to information like phone numbers...  How exactly are the train and test sets formed in a de-correlated-with-respect-to-memorization-of-phone-numbers manner for models of GPT class that are trained on corpus&#x27;s the size of the internet?<p>If a particular person&#x27;s phone number occurs 1000 times in the corpus prior to being split into train&#x2F;test sets, what are the chances that the number only appears in either the train or test set but not both?')