Item(by='glinscott', descendants=None, kids=[25766568, 25773242], score=None, time=1610551289, title=None, item_type='comment', url=None, parent=25759430, text='If anyone wants to experiment with training these nets, it&#x27;s a great way to get exposed to a nice mix of chess and machine learning.<p>There are two trainers currently, the original one, which runs on CPU: <a href="https:&#x2F;&#x2F;github.com&#x2F;nodchip&#x2F;Stockfish" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;nodchip&#x2F;Stockfish</a>, and a pytorch one which runs on GPU: <a href="https:&#x2F;&#x2F;github.com&#x2F;glinscott&#x2F;nnue-pytorch" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;glinscott&#x2F;nnue-pytorch</a>.<p>The SF Discord is where all of the discussion&#x2F;development is happening: <a href="https:&#x2F;&#x2F;discord.gg&#x2F;KGfhSJd" rel="nofollow">https:&#x2F;&#x2F;discord.gg&#x2F;KGfhSJd</a>.<p>Right now there is a lot of experimentation to try adjusting the network architecture.  The current leading approach is a much larger net which takes in attack information per square (eg. is this piece attacked by more pieces than it&#x27;s defended by?).  That network is a little slower, but the additional information seems to be enough to be stronger than the current architecture.<p>Btw, the original Shogi developers really did something amazing.  The nodchip trainer is all custom code, and trains extremely strong nets.  There are all sorts of subtle tricks embedded in there as well that led to stronger nets.  Not to mention, getting the quantization (float32 -&gt; int16&#x2F;int8) working gracefully is a huge challenge.')