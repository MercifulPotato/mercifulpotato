Item(by='sriku', descendants=None, kids=[25355557, 25355456], score=None, time=1607487230, title=None, item_type='comment', url=None, parent=25346456, text='Not a paper, but a fantastic talk by Samy Bengio &quot;Towards better understanding generalisation in deep learning&quot; at ASRU2019.<p>Some pretty mind blowing insights - ex: if you replace one layer&#x27;s weights in a trained classification network with the initialisation weights for the layer (or some intermediate checkpoint as well), many networks show relatively unaffected performance for certain layers ... which is seen as a generalisation since it amounts to parameter reduction. However, if you replace with fresh random weights (although initialisation state is itself another set of random weights), the loss is high! Some layers are more sensitive to this than others in different network architectures.<p>I recently summarised this to a friend who asked &quot;what&#x27;s the most important insight in deep learning?&quot; - to which I said - &quot;in a sufficiently high dimensional parameter space, there is always a direction in which you can move to reduce loss&quot;. I&#x27;m eager to hear other answers to that question here.')