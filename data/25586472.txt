Item(by='meowface', descendants=None, kids=[25607716], score=None, time=1609364357, title=None, item_type='comment', url=None, parent=25585199, text='There are many people like Nick Bostrom and Eliezer Yudkowsky who are much more on the &quot;finding possible solutions&quot; side than the fear side.<p>I&#x27;d consider myself a major futurist and techno-optimist who eagerly anticipates near and far AI advances, but I think some dose of fear is very healthy here, too, though. I want the top AI and AI risk researchers to constantly consider and fear worst-case scenarios, so that they&#x27;re hopefully less likely to occur.<p>Kind of like nuclear research, even if only for energy purposes: very exciting, but you should still fear accidents and their consequences. You just need to be rational about the fear and let it guide you towards developing fail-safes rather than  paralyzing in despair.<p>Pascal&#x27;s Wager-style, the possibility of infinite negative utility should instill visceral terror and drive behavior almost no matter how low the probability is; except this one isn&#x27;t a mugging because creating a god turns out to be a lot more plausible than a vengeful one already watching.')