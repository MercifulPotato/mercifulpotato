Item(by='gevorg_s', descendants=None, kids=None, score=None, time=1610609670, title=None, item_type='comment', url=None, parent=25772777, text='no worries at all, love the Questions!<p>re comparison: we have always wanted to use a free open-source self-hosted tool that would let us group metrics&#x2F;runs by hyperparams, experiment context(train, val, test ...) and any other adjacent info about the training runs. Be able to aggregate groups of metrics, be able to give them different styles, divide them into subplots, search through the runs easily (without regexps on super-long names) etc. As far as I checked last times no such features aren&#x27;t built for those tools. This is huge motivation behind Aim.<p>Probably the closest to this is W&amp;B but it&#x27;s not open-source and doesn&#x27;t allow to see full context of the runs while comparing them (separate module). \nHaven&#x27;t used Losswise tbh.<p>We are trying to build a way that would allow to compare 1000s of ML training runs at the same time while still making the full info (context) of the runs available. \nThis is what I meant by &quot;new paradigm&quot;.\n(It turns out this is a fun problem :) ).<p>We have been working on Aim just a few months only (3 of us) and it&#x27;s in very early stages. Most of the ideas we have aren&#x27;t really shipped yet.<p>But it&#x27;s already very useful for many RL researchers who run lots of experiments and those experiments are sensitive to hyperparameters. Aim seems to be able to handle them.<p>Have you checked out the live demo from the README?<p>Check out my blogpost on TowardsDataScience for more info on Aim (<a href="https:&#x2F;&#x2F;towardsdatascience.com&#x2F;3-ways-aim-can-accelerate-your-ai-research-c03643ae6558" rel="nofollow">https:&#x2F;&#x2F;towardsdatascience.com&#x2F;3-ways-aim-can-accelerate-you...</a>).<p>Hope this info is useful and makes sense. Would be awesome to connect. \nI would love to learn more about your use-cases and needs in these tools. My twitter is @gevorg_s.')