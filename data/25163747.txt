Item(by='TheOtherHobbes', descendants=None, kids=[25165056, 25163916, 25165638, 25163802], score=None, time=1605899309, title=None, item_type='comment', url=None, parent=25162754, text='It was a cultural problem. Symbolics were really trying to build their own LISP-specific DEC-10-a-like, using DEC-style mini and mainframe computer design traditions, but at slightly lower cost.<p>There were a number of projects like that around at the time, including the Three Rivers&#x2F;ICL PERQ which was optimised for PASCAL, and arguably DEC&#x27;s PDP-11 range whose entire architecture was closely aligned with C and eventually C++, pointers and all.<p>These were all interesting machines without a future, because the early 80s were a crossover when it turned out that model was too bloated to be sustainable. DEC scraped through the bottleneck with Alpha, but couldn&#x27;t keep it together as business. Meanwhile the 16-bit and early 32-bit architectures were eating everyone&#x27;s lunch. SGI and Sun flared up in this space and died when their price&#x2F;performance ratio couldn&#x27;t compete with commoditised PCs and Macs - which happened sooner than hardly anyone expected.<p>This is obvious now, but it wasn&#x27;t at all obvious then. The workstation market and the language-optimised market both looked like they had a real future, when in fact they were industrial throw-backs to the postwar corporate model.<p>So it wasn&#x27;t just the AI winter that killed Symbolics - it was the fact that both hardware and software were essentially nostalgic knock-offs of product models from 5-10 years earlier that were already outdated.<p>Meanwhile the real revolution was happening elsewhere, starting with 8-bit micros - which were toys, but very popular toys - and eventually leading to ARM&#x27;s lead today, via Wintel, with Motorola, the Mac, and the NeXT&#x2F;MacOS as a kind of tangent.<p>The same cycle is playing out now with massively accelerated GPU hardware for AI applications, which will eventually be commoditised - probably in an integrated way. IMO Apple are the only company to be thinking about this integration in hardware, and no one at all seems to be considering what it means for AI-enhanced commoditised non-specialised software yet.<p>Apple are giving it some thought, Google are thinking about it technologically, plenty of people are attempting Data Engineering - but still, the current bar for application ideas seems very quite limited compared to the possibilities a personal commoditised integrated architecture could offer, because again current platforms have become centralised and industrialised.<p>There&#x27;s a strong centrifugal and individualistic tendency in personal computing which I suspect will subvert that - and we&#x27;ll see signs of it long before the end of the decade.')