Item(by='avianes', descendants=None, kids=None, score=None, time=1605997239, title=None, item_type='comment', url=None, parent=25164841, text='&gt; Nothing like it can be done for an x86 cpu. The way modern x86 handles this is typically with a uop cache. But this costs a lot of power, area and only provides full decode width for a relatively small pool of insns -- 4k on modern Zen, for example.<p>The ÂµOP cache is actually not an explanation of how x86 machines perform the decode, it is rather a way to bypass the decode which is expensive and difficult for them.<p>And the most difficult part is not really the decoding in macro op, the difficulty is to find the boundaries of the instructions to be decoded in parallel.<p>Because the alignment of each instruction to be decoded depends on the size of the previous instructions, this is a sequential process.<p>For example, one way to do this is to use instruction size prediction.\nIntel and AMD hold several patents related to instruction length prediction [1].<p>[1]: <a href="https:&#x2F;&#x2F;patents.google.com&#x2F;patent&#x2F;US20140281246A1&#x2F;en" rel="nofollow">https:&#x2F;&#x2F;patents.google.com&#x2F;patent&#x2F;US20140281246A1&#x2F;en</a>')