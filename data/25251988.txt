Item(by='mhx77', descendants=None, kids=None, score=None, time=1606726942, title=None, item_type='comment', url=None, parent=25251585, text='This is really cool, I&#x27;ll give squashfs-tools-ng a try!<p>&gt; Does the Raspbian image comparison compare XZ compression against SquashFS with Zstd?<p>That&#x27;s correct. It&#x27;s not an exhaustive matrix of comparisons.<p>&gt; Also, is there any documentation on how the on-disk format for DwarFS and it&#x27;s packing works which might explain the incredible size difference?<p>The format as of 0.2.0 is actually quite simple. It&#x27;s a list of compressed data blocks, followed by a metadata block (and a schema describing the metadata block). The metadata format is implemented by and documented in in [1].<p>There are probably 3 things that contribute to compression level:<p>1) Block size. DwarFS can use arbitrary block sizes (artificially limited to powers of two), and uses a much larger block size (16M) by default. SquasFS doesn&#x27;t seem to be able to go higher than 1M.<p>2) Ordering files by similarity.<p>3) Segment deduplication. If segments of files overlap with previously seen data, these segments are referenced instead of written again. The minimum size of these segments can be configured and defaults to 2k. For my primary use case, of the 47.6 GB of input data, 28.2 GB are saved by file-level deduplication, and another 12.4 GB by this segment-level deduplication. So before the &quot;real&quot; compression algorithms actually kick in, there are only 7 GB of data left. As these are ordered by similarity, and stored in rather big blocks, some of the 16M blocks can actually be compressed down to less then 100k.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;mhx&#x2F;dwarfs&#x2F;blob&#x2F;main&#x2F;thrift&#x2F;metadata.thrift" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;mhx&#x2F;dwarfs&#x2F;blob&#x2F;main&#x2F;thrift&#x2F;metadata.thri...</a>')