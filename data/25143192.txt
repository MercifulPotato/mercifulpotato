Item(by='dragontamer', descendants=None, kids=None, score=None, time=1605737700, title=None, item_type='comment', url=None, parent=25142852, text='Because bandwidth-optimized computers do more than just matrix-multiply all day long.<p>Tensor Processing Units are too specialized: they can&#x27;t traverse a linked list, they can&#x27;t traverse trees. They&#x27;re good at one thing and one thing only: matrix multiplication.<p>GPUs are still bandwidth-optimized and are good at matrix multiplication (but not as good as tensor units). But GPUs can traverse trees and new data-structures. Ex: BVH trees for raytracing, or linked lists... or whatever else you need. Its a general computer, a weird... terrible latency computer with HUGE bandwidth... but that&#x27;s still useful in many compute applications.<p>--------------<p>Matrix multiplication is the cornerstone of many scientific problems. But you still need software to manipulate the data into the correct &quot;form&quot;, so that the matrix multiplication units can then process the data.<p>Its in this &quot;preprocessing&quot; or &quot;postprocessing&quot; phase where GPUs do best. You can implement bitonic sort for highly-parallel sorting &#x2F; searching. You can perform GPU-accelerated join networks for SQL. Etc. etc.<p>And even then, NVidia&#x27;s A100 have incredibly good matrix multiplication units. So you&#x27;re really not losing much anyway.')