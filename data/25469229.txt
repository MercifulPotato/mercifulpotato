Item(by='alexhutcheson', descendants=None, kids=None, score=None, time=1608311432, title=None, item_type='comment', url=None, parent=25467221, text='There are two important sets of ML workloads: training and inference. This Intel blog post does a reasonably good job summarizing the differences: <a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;artificial-intelligence&#x2F;posts&#x2F;deep-learning-training-and-inference.html" rel="nofollow">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;artificial-intellige...</a>.<p>The hardware requirements for training are much higher than for inference. Roughly:<p>- Training: Requires massive throughput and compute, so is done on high-end GPUs or TPUs in datacenters (or on researchers&#x27; beefy workstations)<p>- Inference: Compute requirement depends on the size and complexity of the model, but many models can be run on low-end smartphone hardware.<p>Generally speaking, any time you see &quot;ML hardware&quot; in a consumer device, that hardware is for inference, not training. Apple&#x27;s Neural Engine is an example, of this. Other current examples include the ARM Ethos-N series and Qualcomm AI Engine, among others. Often this hardware is designed to balance inference performance with power consumption, so that inference can be done efficiently on battery-powered devices.<p>Training workloads on the M1 use the GPU, not the Neural Engine: <a href="https:&#x2F;&#x2F;blog.tensorflow.org&#x2F;2020&#x2F;11&#x2F;accelerating-tensorflow-performance-on-mac.html" rel="nofollow">https:&#x2F;&#x2F;blog.tensorflow.org&#x2F;2020&#x2F;11&#x2F;accelerating-tensorflow-...</a>')