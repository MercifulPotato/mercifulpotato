Item(by='m00x', descendants=None, kids=[25298232, 25297717], score=None, time=1607042254, title=None, item_type='comment', url=None, parent=25297318, text='&gt; Ultimately the engineers and other humans behind the scenes are the ultimate arbiters of success, loss functions are chosen by humans, and swapping training data on the same model won&#x27;t change those early decisions<p>How would changing loss functions alter this? This makes no sense.<p>Hyperparameter tuning is done iteratively, and used get the best score on the test dataset. Do you really believe the engineers hand picked examples and purposefully trained it so that looked more &quot;white&quot; and disregarded the test scores?<p>Training&#x2F;test data is 100% the cause for this bias.')