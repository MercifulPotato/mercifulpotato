Item(by='lovecg', descendants=None, kids=None, score=None, time=1608067111, title=None, item_type='comment', url=None, parent=25424878, text='Love this! Thanks, it was a casual idea of mine that I didn’t really think through before.<p>Here’s a slightly different approach to this. Let’s instead assume that the set of “memorable” strings is constant (say of size N&#x2F;M where N is the number of all strings) and the user hits as many retries as needed to get a string from the memorable set. If the number of retries is a random variable X, then if we know the distribution of X we know M. Since the number of bits lost is something like \\log_2(M), we just want to find out how X relates to M.<p>EX = \\sum_{i\\geq 0}i<i>(1-1&#x2F;M)^i</i>(1&#x2F;M)\n= WolframAlpha :) = M - 1<p>So it matches: if your average number of tries is M - 1, you lose something like \\log_2(M) bits of entropy.<p>Makes me feel better about all those times when I hit retry a dozen times.')