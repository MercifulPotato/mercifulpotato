Item(by='zerocrates', descendants=None, kids=None, score=None, time=1610177834, title=None, item_type='comment', url=None, parent=25698053, text='Basically the issue prior to 230 was that the major precedents worked out like this: if you didn&#x27;t moderate <i>at all</i>, you were not treated as a publisher and were not liable for the speech of your users. But if you did do some moderation&#x2F;blocking, you <i>could</i> be seen as a publisher of the things you didn&#x27;t block. So you have a kind of &quot;all or nothing&quot; type of situation.<p>The problem is: in terms of legal risk, &quot;just allow everything&quot; is safer, since it&#x27;s quite expensive or even infeasible to completely moderate the platform. But Congress wanted to encourage internet companies to do moderation for things like pornography and other &quot;indecent&quot; content (this is part of the Communications Decency Act, after all), which is why this &quot;Good Samaritan&quot; section exists: to remove the specter of liability that could arise from even light-touch moderation. (But note that despite that title, the language is broad and not conditional on the presence or absence of any moderation scheme.)<p>Repeal has a few possibilities, one of which is as you said, just eliminate user-controlled content completely from some spaces, or moderate it extremely heavily (like requiring manual review before something is posted). But, the other extreme is also possible, of totally removing moderation to escape liability the other way around.')