Item(by='j-pb', descendants=None, kids=[25915541, 25917376, 25921355, 25915435], score=None, time=1611664747, title=None, item_type='comment', url=None, parent=25914442, text='Familiarity isn&#x27;t nearly enough if you want to implement something.<p>Talking about RDF is absolutely meaningless without talking about Serialisation (and that includes ...URGH.. XML serialisation), XML Schema data-types, localisations, skolemisation, and the ongoing blank-node war.<p>The semantic web ecosystem is the prime example of &quot;the devils in the detail&quot;. Of course you can explain to somebody who knows what a graph is, the general idea of RDF: &quot;It&#x27;s like a graph, but the edges are also reified as nodes.&quot;\nBut that omits basically everything.<p>It doesn&#x27;t matter if SparQL is learnable or not, it matters if its implementable, let alone in a performant way. And thats really really questionable.<p>Jena is okay-ish, but it&#x27;s neither pleasant to use, nor bug free, although java has the best RDF libs generally (I think thats got something to do with academic selection bias). RDF4J has 300 open issues, but they also contain a lot of refactoring noise, which isn&#x27;t a bad thing.<p>C&#x27;mon, rdflib is a joke. It has a ridiculous 200 issues &#x2F; 1 commit a month ratio, buggy as hell, and is for all intents and purposes abandonware.<p>rdflib.js is in memory only, so nothing you could use in production for anything beyond simple stuff.\nAlso there&#x27;s essentially ZERO documentation.<p>And none of those except for Jena even step into the realm of OWL.<p>&gt; What are the alternatives?<p>Good question.<p>SIMPLICITY!<p>We have an RDF replacement running in production that&#x27;s twice as fast, and 100 times simpler.\nOur implementation clocks in at 2.5kloc, and that includes everything from storage to queries,\nwith zero dependencies.<p>By having something that&#x27;s so simple to implement, it&#x27;s super easy to port it to various programming languages,\nexperiment with implementations, and exterminate bugs.<p>We don&#x27;t have triples, we have tribles (binary triples, get it, nudge nudge, wink wink).\n64 Byte in total, fits into exactly one cache line on the majority of Architectures.<p>16byte subject&#x2F;entity | 16 byte predicate&#x2F;attribute | 32 byte object&#x2F;value<p>These tribles are stored in knowledge bases with grow-set semantics,\nso you can only ever append (on a meta level knowledge bases do support non-monotonic set operations),\nwhich is the only way you can get consistency with open world-semantics, which is something that the OWL people\napparently forgot to tell pretty much everybody who wrote RDF stores, as they all have some form of non-mononic\ndelete operation. Even SparQL is non-monotonic with it&#x27;s optional operator...<p>Having a fixed size binary representation makes this compatible with most existing databases,\nand almost trivial to implement covering indices and multiway joins for.<p>By choosing UUIDs (or ULIDs, or TimeFlakes, or whatever, the 16byte don&#x27;t care) for subject and predicate we completely\ncircumnavigate the issues of naming, and schema evolution.\nI&#x27;ve seen so many hours wasted by ontologists arguing about what something should be called.\nIn our case, it doesn&#x27;t matter, both consumers of the schema can choose their own name in their code.\nAnd if you want to upgrade your schema, simply create a new attribute id, and change the name in your code to point to it instead.<p>If a value is larger than 32 byte, we store a 256bit hash in the trible, and store the data itself in a a separate blob store (in our production case S3, but for tests it&#x27;s the file stystem, we&#x27;re eyeing a IPFS adapter but that&#x27;s only useful if we open-sourced it).\nWhich means that it&#x27;s also working nicely with binary data, which RDF never managed to do well. (We use it to mix machine learning models with symbolic knowledge).<p>We stole the context approach from jsonLD, so that you can define your own serialisers and deserialisers depending on the context they are used in.\nSo you might have a &quot;legacyTimestamp&quot; attribute which returns a util.datetime, and a &quot;timestamp&quot; which returns a JodaTime Object.\nHowever unlinke jsonLD these are not static transformations on the graph, but done just in time through the interface that exposes the graph.<p>We have two interfaces. One based on conjunctive queries which looks like this (JS as an example):<p>```<p><pre><code>  &#x2F;&#x2F; define a schema\n  const knightsCtx = ctx({\n    ns: {\n      [id]: { ...types.uuid },\n      name: { id: nameId, ...types.shortstring },\n      loves: { id: lovesId },\n      lovedBy: { id: lovesId, isInverse: true },\n      titles: { id: titlesId, ...types.shortstring },\n    },\n    ids: {\n      [nameId]: { isUnique: true },\n      [lovesId]: { isLink: true, isUnique: true },\n      [titlesId]: {},\n    },\n  });\n\n  &#x2F;&#x2F; add some data\n  const knightskb = memkb.with(\n    knightsCtx,\n    (\n      [romeo, juliet],\n    ) =&gt; [\n      {\n        [id]: romeo,\n        name: &quot;Romeo&quot;,\n        titles: [&quot;fool&quot;, &quot;prince&quot;],\n        loves: juliet,\n      },\n      {\n        [id]: juliet,\n        name: &quot;Juliet&quot;,\n        titles: [&quot;the lady&quot;, &quot;princess&quot;],\n        loves: romeo,\n      },\n    ],\n  );\n\n  &#x2F;&#x2F; Query some data.\n  const results = [\n    ...knightskb.find(knightsCtx, (\n      { name, title },\n    ) =&gt; [{ name: name.at(0).ascend().walk(), titles: [title] }]),\n  ];\n</code></pre>\n```<p>and the other based on tree walking, where you get a proxy object that you can treat as any other object graph in your programming language,\nand you can just navigate it by traversing it&#x27;s properties, lazily creating a tree unfolding.<p>Our schema description is also heavily simplified. We only have property restrictions and no classes.\nFor classes there&#x27;s ALWAYS a counter example of something that intuitively is in that class, but which is excluded by the class definition.\nAt the same time, classes are the source of pretty much all computational complexity. (Can&#x27;t count if you don&#x27;t have fingers.)<p>We do have cardinality restrictions, but restrict the range of attributes to be limited to one type. That way you can statically type check queries and walks in statically typed languages. And remember, attributes are UUIDs and thus essentially free, simply create one attribute per type.<p>In the above example you&#x27;ll notice that queries are tree queries with variables. They&#x27;re what&#x27;s most common, and also what&#x27;s compatible with the data-structures and tools available in most programming languages (except for maybe prolog). However we do support full conjunctive queries over triples, and it&#x27;s what these queries get compiled to. We just don&#x27;t want to step into the same impedance mismatch trap datalog steps into.<p>Our query &quot;engine&quot; (much simpler, no optimiser for example), performs a lazy depth first walk over the variables and performs a multiway set intersection for each, which generalises the join of conjunctive queries, to arbitrary constraints (like, I want only attributes that also occur in this list). Because it&#x27;s lazy you get limit queries for free. And because no intermediary query results are materialised, you can implement aggregates with a simple reduction of the result sequence.<p>The &quot;generic constraint resolution&quot; approach to joins also gives us queries that can span multiple knowledge bases (without federation, but we&#x27;re working on something like that based on differential dataflow).<p>Multi-kb queries are especially useful since our default in-memory knowledge base is actually an immutable persistent data-structure, so it&#x27;s trivial and cheap to work with many different variants at the same time. They efficiently support all set operations, so you can do functional logic programming a la &quot;out of the tar pit&quot;, in pretty much any programming language.<p>Another cool thing is that our on-disk storage format is really resilient through it&#x27;s simplicity.\nBecause the semantics are append only, we can store everything in a log file. Each transaction is prefixed with a\nhash of the transaction and followed by the tribles of the transaction, and because of their constant size, framing is trivial.<p>We can loose arbitrary chunks of our database and still retain the data that was unaffected. Try that with your RDMBS,\nyou will loose everything. It also makes merging multiple databases super easy (remember UUIDs to prevent naming collisions, monotonic open world semantics keep consistency, fixed size tribles make framing trivial), you simply `cat db1 db2 &gt; outdb` them.<p>Again, all of this in 2.5kloc with zero dependencies (we do have one on S3 in the S3 blob store adapter).<p>Is this the way to go? I don&#x27;t know, it serves us well. But the great thing about it is that there could be dozens of equally simple systems and standards, and we could actually see which approaches are best, from usage.\nThe semantic web community is currently sitting on a pile of ivory, contemplating on how to best steer the titanics that are protege, and OWLAPI through the waters of computational complexity. Without anybody every stopping to ask if that&#x27;s REALLY been the big problem all along.<p>&quot;I&#x27;d really love to use OWL and RDF, if only the algorithms were in a different complexity class!&quot;')