Item(by='banachtarski', descendants=None, kids=[25963280], score=None, time=1611954946, title=None, item_type='comment', url=None, parent=25962056, text='I&#x27;m not a hardware engineer, but I am a GPU-focused graphics engineer.<p>&gt; C-style fine-level control over a GPU device as though it were a CPU.<p>Personally, I think this is a fool&#x27;s errand, and this has nothing to do with my desire for job security or anything. When I look at how code in the ML world is written for a GPU for example, it&#x27;s really easy to see why it&#x27;s so slow. The CPU and GPU architectures are fundamentally different. Different pipelining architecture, scalar instead of vector, 32&#x2F;64-wide instruction dispatches, etc. HLSL&#x2F;GLSL and other such shader languages are perfectly &quot;high level&quot; with other needed intrinsics needed to perform relevant warp level barriers, wave broadcasts&#x2F;ballots&#x2F;queries, use LDS storage, execute device level barriers, etc. This isn&#x27;t to say that high level shader language improvements aren&#x27;t welcome, but that trying to emulate a CPU is an unfortunate goal.')