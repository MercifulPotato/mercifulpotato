Item(by='dragontamer', descendants=None, kids=[24841627], score=None, time=1603221513, title=None, item_type='comment', url=None, parent=24841106, text='&gt; I guess we arenâ€™t solving the same problems: memory allocation is trivial is my domain, mapping complex nested control flow in SIMD is the hard part.<p>Those statements are just confusing to me. In most GPU-code, you have crazy amounts of parallelism and therefore don&#x27;t really care what order those statements execute in.<p>As such, if you can allocate memory, and map those if&#x2F;else statements into a collection of queues and&#x2F;or stacks (depending on if you want a breadth-first search, or depth-first search pattern).<p>In effect: you use memory allocation to solve the complex control flow issue. Maybe its more obvious with code:<p>Instead of doing:<p><pre><code>    if(baz()){\n      foo();\n    } else {\n      bar(); &#x2F;&#x2F; Thread divergence!!\n    }\n</code></pre>\nDo:<p><pre><code>    if(baz()){\n      pushIntoFooQueue();\n    } else {\n      pushIntoBarQueue(); \n      &#x2F;&#x2F; Thread divergence, but not much of a penalty\n    }\n\n    while(fooIsNotEmpty()){ &#x2F;&#x2F; No thread divergence at all\n        task = SIMDPopFoo();\n        task.execute();\n    }\n\n    while(barIsNotEmpty()){\n        task = SIMDPopBar();\n        task.execute();\n    }\n</code></pre>\nThis is heavier on the memory units, because you now have to manage the data-structures. But this style practically negates the thread-divergence penalty completely. If you&#x27;re lucky, your fooQueue and barQueue fit in __shared__ memory.<p>Bonus points: Not only is thread-divergence negated, but you also achieve effective load-balancing across your workgroup. If Thread#0 spawns 20 items for FooQueue, after pushing&#x2F;popping from the queue, those 20 Foo-tasks will be assigned to 20 different threads.<p>-----<p>From there on out, you&#x27;re just pushing &#x2F; popping different parts of your code to various queues and&#x2F;or stacks. But this is only really a valid solution if you have a decent memory allocator that knows where and how to clean up these queues &#x2F; stacks (especially if you have nodes starting to point to each other for dependency management)<p>I haven&#x27;t really solved this problem &quot;in general&quot;, but is clear that the queues should be sorted into topological order, and that any tasks that depend onto each other need to be run in different iterations. It really depends on how much you&#x27;re willing to spend on organizing this execution information.<p>In any case: the memory allocation issue is one-and-the-same with the thread-divergence &#x2F; complex instruction flow issue to me. You need to create a data-structure that organizes the instruction flow, and any complex data-structure will need memory management as soon as you start linking things together. The above uses a queue or stack, but things can get more complicated.<p>--------<p>Not that Julia, Python, ISPC or anything really... solves this problem. But memory management is very useful for this &quot;style&quot; (I&#x27;m mostly doing ref-counted C++ with a custom allocator myself. But such trees or graphs of links can grow into the CPU-side and end up using the default memory allocator)')