Item(by='Reelin', descendants=None, kids=None, score=None, time=1604302052, title=None, item_type='comment', url=None, parent=24964419, text='&gt; &gt; the studies didnâ€™t achieve statistical significance, a controversial but commonly used threshold for publication<p>&gt; why would you publish a non statistically significant result<p>There&#x27;s actually two separate things going on here.<p>The first is that researches (at least in the biomedical sciences) often decide not to waste their time writing up results that are either weak, inconclusive, or negative. This is only mildly controversial - everyone involved recognizes the inherent time availability constraint, but knowing what <i>didn&#x27;t</i> work can help to inform the field at large in it&#x27;s own way.<p>The second is that the term &quot;statistically significant&quot; itself has become _highly_ controversial in recent years. The issue is that when you perform statistical tests, they generally (oversimplifying) spit out a number telling you what the odds of your results being correct are.<p>In concrete terms, say you have some graphs that show a slight improvement in some condition when a drug is used. Is the drug actually effective, or is the &quot;improvement&quot; actually just random measurement errors? So you run a statistical test on your data and it gives you a score of 80%. It&#x27;s telling you that there&#x27;s an 80% chance that your results were due to an actual improvement and a 20% chance that they were due to random chance.<p>If you had unlimited time and money you could just keep collecting results endlessly. Eventually, that score would either go towards 100% (it definitely works) or 0% (it definitely doesn&#x27;t work). But this is the real world, where we don&#x27;t have unlimited time and money (particularly academic researchers).<p>So when is your result worth publishing? How do you decide when to throw in the towel? And if you&#x27;re reviewing papers for a journal, how low a score is acceptable before you vote to reject the paper on the grounds that the conclusions are unreliable and not worth looking at?<p>Enter the term &quot;statistical significance&quot;. At some point, people started classifying scores that were above some arbitrary threshold as &quot;significant&quot; and those below it as &quot;not significant&quot;. But there&#x27;s an obvious problem here - some measure of probability flipping from (say) 89.999% to 90.000% doesn&#x27;t do anything magical! Worse, what a future reader intends to do with the results will determine how important any given score is in that particular case. Clearly, results and their associated score need to be interpreted in context instead of blindly. Using a term such as &quot;statistical significance&quot; flies in the face of that by actively encouraging lazy thinking.<p>So the controversy being referred to in that specific sentence you quoted isn&#x27;t the decision not to publish but rather the usage of the term itself. (Which is confusing, because the article at large is addressing the controversy surrounding not publishing.)')