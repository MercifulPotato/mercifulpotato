Item(by='dragontamer', descendants=None, kids=None, score=None, time=1610387983, title=None, item_type='comment', url=None, parent=25732591, text='It should be noted that SIMD-on-CPU is not to be the fastest SIMD in the world... but instead the closest SIMD to your data.<p>SIMD today largely exists in the realm of GPUs: the NVidia 3090 is a dedicated SIMD-architecture for example. But as an &quot;external&quot; device on a PCIe 4.0 port, your CPU can only stream 30GBps worth of data to it (the max-bandwidth of PCIe 4.0 x16)<p>The AVX512-units inside of an Intel chip is the same GPU-SIMD architecture, but smaller. More importantly, it shares L1 cache, and therefore has TBps worth of data-sharing.<p>As such, AVX512 becomes useful if you need a fast CPU to calculate some things, and then switch off to SIMD-paradigm for some reason.<p>---------<p>What&#x27;s a good example of this &quot;switchoff&quot; ?? Well, Stockfish-nnue (now default in Stockfish 12) is a great example.<p>You might know that SIMD-compute, in particular GPUs, are the one of the best architectures for calculating neural nets.  But GPUs are terrible with branchy-code (which happens often in chess: &quot;if(bishop) then (calculate bishop moves)&quot; is rather difficult to do efficiently on a GPU! Its hard to explain why, but its called &quot;branch divergence&quot;. Look it up if you feel like it).<p>Anyway, a CPU is best at the &quot;if(bishop)&quot; kind of computation. But GPUs are best at neural nets.<p>Stockfish-nnue beats Leela-Zero, by using AVX512 on a small-and-lightweight neuralnet, but still uses the CPU for the fast if() statements.<p>This is only made possible because AVX512 and the rest of the CPU shares the same L1 &#x2F; L2 cache, meaning you can get TBps worth of data-sharing, instead of GBs if you went &quot;off-chip&quot; to another computer (like calling a GPU-to calculate a neural net).<p>Furthermore, the NNUE neural net in Stockfish is composed of if-statements: only PART of the neural net updates, which means it can never be efficiently implemented on a GPU. Only something with fast if() statements can implement NNUE. (EDIT: Well... maybe not &quot;never&quot;, but it&#x27;d be non-obvious to code up a solution. I think its possible, now that I think of it... but its not done in typical GPU code these days)<p>GPUs &#x2F; classical SIMD compute want all of its data to update uniformly, without if() statements mucking up the logic. CPUs have giant branch-prediction and hyperthreading and other such features to speed up if-statements.<p>-----<p>A small SIMD for small SIMD-calculations, that combines with fast CPU-style branch-heavy code. A nightmare for GPUs to implement. Its something only possible with CPU + SIMD instruction assist.<p>------<p>GPUs are likely still best for large-scale SIMD. Like giant matrix multiplications (or many matrix-multiplications, like in video games). Where if-statements are less common.')