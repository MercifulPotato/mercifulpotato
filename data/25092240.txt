Item(by='the8472', descendants=None, kids=None, score=None, time=1605361480, title=None, item_type='comment', url=None, parent=25092055, text='&gt; If the recommendation is that everything that matters should be in a sqlite database because posix is so hard to use, well, that sounds like we agree more than we disagree.<p>I don&#x27;t mean everyone should use sqlite. We just need more safe abstractions over posix APIs that cover different use-cases. If you need to concurrently query and modify many small records, then sure, go with sqlite.<p>&gt; Are there? I can&#x27;t think of many. Maybe copying big files?<p>It&#x27;s not just the big files. Millions of small files would cause some overhead if you actually tried commiting them instead of relying on best-effort behavior (see git case below). Temp and cache directories. Archival. Compiler artifacts. Often you don&#x27;t even want them to actually hit the disk at all if you have enough RAM. A file can act as nothing more than named swap space.<p>&gt; When I do a git commit, its plenty fast enough. But I definitely don&#x27;t want my repository to get corrupted if my computer crashes.<p>Sure, that&#x27;s for the canonical git storage, which rarely touches many files anyway. But if you checkout a worktree (e.g. by switching branches) then putting all the thousands of files from the .git object storage into the work tree does not need durability. You want to be able to work on those files as soon as possible, even if they&#x27;re just in the page cache and not written to disk yet. The command shouldn&#x27;t block on fsync.')