Item(by='mcguire', descendants=None, kids=None, score=None, time=1608746630, title=None, item_type='comment', url=None, parent=25519949, text='Not really, no.<p>My first experience with that behavior was with AIX in the early &#x27;90s: allocation always succeeds (unless you&#x27;ve exhausted your address space) because it is backed by virtual memory. If you <i>do</i> run low on physical memory, every process gets a SIGDANGER and is expected to free up memory. If that fails, the kernel starts sending SIGKILLs.<p>Unfortunately, exactly none of the normal utilities understood the protocol and upon running low, the kernel just started shooting processes in the head until the situation improved. For some reason, one of the first processes chosen was usually inetd, meaning you could no longer remotely log in to see what was going on.<p>When I complained, I was told that the machines were frequently used for scientific computing, where they would allocate giant arrays which were then used sparsely, so it was the intended behavior. And that the X server at the time made use of the behavior.<p>So, ... yech.')