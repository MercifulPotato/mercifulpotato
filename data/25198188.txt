Item(by='qayxc', descendants=None, kids=None, score=None, time=1606224175, title=None, item_type='comment', url=None, parent=25197663, text='&gt; Building an unconstrained AGI is an existential risk<p>But why? So far no-one has been able to explain this to me.<p>An AGI in at of itself is nothing but a brain-in-a-vat. The exponential increase in scientific knowledge was based on the scientific method, which replaced the ancient Greece discussion-based epistemology with a cycle of observation-hypothesis-&gt;test-&gt;observation-&gt;...<p>An isolated AGI cannot test its predictions about the world. Since the unconstrained space of possible explanations is bigger than the space of explanations constrained by observational evidence, there is no way an AGI can gain useful knowledge without access to external observations.<p>This means we are in full control of how &quot;intelligent&quot; (by whatever metric) an AGI can even get by restricting its access to information. But even unlimited access to (passive) information only gets you so far, as some models require data that cannot be obtained passively (i.e. they require deliberate controlled experiments).<p>The final nail in the coffin of dangerous AGI is interaction with the physical world. Yes, even a toddler is a terrifying menace to millions of people if I place a button right next it that sets off a thermonuclear bomb in a city centre.<p>But how about we just don&#x27;t do that? An AGI with limited or no physical interaction with the world (directly via robot body or indirectly through remote access) can&#x27;t be any more harmful and menacing than the late Stephen Hawking.<p>There&#x27;s no need to put AI on a leash, since there&#x27;s a final naturally limiting factor: energy. Switch off the cooling system and your AGI has to throttle down lest it faces fiery death.')