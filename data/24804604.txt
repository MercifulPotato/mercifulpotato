Item(by='throwawaygh', descendants=None, kids=[24805097], score=None, time=1602879275, title=None, item_type='comment', url=None, parent=24804503, text='<i>&gt; No primary moderation action should be made based on human input</i><p>1. That means no HN.<p>2. I normally don&#x27;t have to remind people of this at places like HN, but... algorithms are written by... humans! Supervised algos use data labeled by... humans!<p><i>&gt; Automated moderation should look for identifiable harms (i.e. illicit content, directed threats, terrorism)</i><p>Why do you list terrorism separately from directed threats?<p>What is the line&#x2F;difference between &quot;terrorism&quot; and an &quot;undirected threat&quot;?<p>Are militia groups that don&#x27;t make directed threats terrorists? Are radical religious groups that don&#x27;t make directed threats terrorists? What if they are run by actual terrorists but none of the speech amounts to a directed threat?<p>Speaking of which, what is a terrorist organization? Is the KKK? What about small white nationalist or black power militia groups? What about QAnon? What about antifa? What about BLM? What about Westboro Baptist? What about the Black Panthers?<p>There are people -- elected officials -- who think each of those is a terror organization.<p>So, defining terrorist organization is absolutely a political fight. Maybe we avoid that and just talk about directed threats&#x2F; Ok. Does that mean that Al Qaeda allowed to operate on FB as long as they don&#x27;t make directed threats? In fact, that FB is prohibited from not allowing Al Qaeda on as long as they don&#x27;t make directed threats? That seems like not a solution anyone is going to get behind.<p>We haven&#x27;t even gotten past the &quot;obviously terrorism=bad&quot; and we already have to declare whether BLM, QAnon, Westboro, or militia groups are &quot;terrorists&quot;. Which some senators believe is the case and is a 100% political question.<p><i>&gt; illicit content</i><p>Is Ginsberg&#x27;s <i>Howl</i> illicit? Is a picture of two women kissing illicit? What about non-sexualized nude breasts? What about nude male bodies? What about an erect penis but in a non-erotic context? Will the dominant answers to these questions be the same in 50 years?<p>Lots of people would say a site that allows pictures of heterosexual kissing but not not pictures of homosexual kissing is obviously taking a political position, but that was outside the realm of &quot;political opinion&quot; when I entered adulthood! Any public homosexual display of affection was <i>obviously</i> illicit.<p><i>&gt; absolutely nothing should be removed or blocked based on vague and nebulously defined concerns over &quot;misinformation&quot;.</i><p>What does vague mean? What does nebulously defined mean? What is the difference between misinformation and libel? What is the difference between misinformation and dangerous information? Is it impressible to remove a video that&#x27;s targeted at kids and encourages huffing glue as a fun and harm-free activity?<p>Anyone who has moderated a forum knows that such an algorithm is going to have all sorts of holes and perceived biases. I&#x27;ve <i>never</i> written an automod that some user doesn&#x27;t get pissed off about.<p>More generally: that&#x27;s just straight-up moderation, it has nothing to do with tweaks to recommendation algos.<p>What if Twitter realizes that people leave the site if they see stuff about abortion but stay if they see stuff about LGBT rights? Again, viewpoint-neutral, Americans just one day start yawning about abortion and really polarize on LGBT stuff. Can they prioritize posts about LGBT rights over posts about abortion as long as the content served up on the preferred topic is viewpoint-neutral and the only algorithmic goal is more lingering eyeballs?<p>If no to that, how about sports news vs. SCOTUS decision news?<p>If yes to that, what about COVID case counts vs. Jobs Report numbers?<p>Even more generally: anyone who&#x27;s stayed up to date on robust machine learning knows that defining good notions of robustness -- and political neutrality is a type of robustness -- is very much an open problem. So even if we had a precise definition of political neutrality, which I don&#x27;t think we do, &quot;simply create an algorithm that has that property&quot; is very much an open algorithmic problem.<p>In fact, there are even some impossibility theorems in this space. So even if we can define neutrality in a perfectly neutral way -- which we can&#x27;t -- this might be like passing a constitutional amendment that demands a voting system has all of: Non-dictatorship, unrestricted domain, monotonicity, IIA, and non-imposition. You can legislatively demand &quot;the perfect voting system&quot;, but the universe is not obliged to ensure the existence of such a thing. Same for some types of robust ML, and no one knows which side of an impossibility theorem some precise-enough-to-code notion of political neutrality might fall on.<p>Which also brings up the REAL question: are tweaks to recommendation algorithms allowed? Obviously we can&#x27;t ask FB&#x2F;Twitter to freeze their recommendation algos -- it&#x27;s their core product. So. If they notice an &quot;obvious bias&quot; and tweak the algorithm to correct for it, who decides whether that was a biased human intervention or a totally appropriate bug fix? Oh, right, a politically appointed FTC.<p>I think that &quot;politically neutral&quot; is impossible to formalize in code because it is a fundamental contradiction in terms. But even if it does, I suspect that any reasonable lists of formal specifications might be either mathematically impossible to train a classifier to respect or else at least AGI-complete to actually implement. But if you disagree, I&#x27;m happy to clone the Github repo and mess around with your proposal.')