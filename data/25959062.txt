Item(by='bachmeier', descendants=None, kids=[25960810], score=None, time=1611937779, title=None, item_type='comment', url=None, parent=25957929, text='&gt; Sure, if you choose a bad starting point, your initial samples might not be representative of the overall distribution, but if a handful of non-representative points can massively impact your result, then I&#x27;m not sure how stable your result was to begin with (how do you know there isn&#x27;t some other set of low-probability high-impact points that your sampler just missed through luck?).<p>You&#x27;re right, and most comments I&#x27;ve seen over the years on the post conveniently miss that he addresses that:<p>&gt; This unbiasedness argument is rubbish. If you start at x and I start at x then your MCMC run is no better than mine. If you used burn-in and I didn&#x27;t, then you are entitled to woof about approximate unbiasedness and I am not. But that woof does not make your estimator any better.<p>My interpretation has always been this, and I think it&#x27;s correct: You need a good starting point. There&#x27;s no reason to think burn-in gives you a good starting point. Instead, use something that&#x27;s actually intended to give a good starting point, like the mode.')