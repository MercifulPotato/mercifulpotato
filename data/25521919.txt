Item(by='standevbob', descendants=None, kids=None, score=None, time=1608755882, title=None, item_type='comment', url=None, parent=25517274, text='Stan supports optimization (L-BFGS) to find (penalized) maximum likelihood or MAP estimates where they exist.  Bayesian estimates are typically posterior means, which involve MCMC rather than optimization, and the result is usually far away from the maximum likelihood estimate in high dimensions.   I wrote a case study with some simple examples here:  <a href="https:&#x2F;&#x2F;mc-stan.org&#x2F;users&#x2F;documentation&#x2F;case-studies&#x2F;curse-dims.html" rel="nofollow">https:&#x2F;&#x2F;mc-stan.org&#x2F;users&#x2F;documentation&#x2F;case-studies&#x2F;curse-d...</a><p>Adding new parameters scales as O(N^5&#x2F;4) in HMC, whereas it scales as O(N^2) in Metropolis or Gibbs.  It&#x27;s quadrature that scales exponentially in dimension.  There&#x27;s also a constant factor for posterior correlation, which can get nasty.  I regularly fit regressions for epidemiology or genomics or education with 10s or even 100s of thousands of parameters on my notebook with one core and no GPU.<p>MCMC or optimization can be sub-linear or super-linear in the data, depending on the statistical properties of the posterior.  Some non-parametric models like Gaussian processes can be cubic in the data size, whereas regressions are often sub-linear (doubling the data doesn&#x27;t double computation time) because posteriors are better behaved (more normal in the Gaussian sense) when there&#x27;s more data and hence easier to explore in fewer log density and gradient evaluations.')