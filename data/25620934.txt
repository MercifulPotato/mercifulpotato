Item(by='meowface', descendants=None, kids=None, score=None, time=1609669814, title=None, item_type='comment', url=None, parent=25607716, text='I&#x27;d recommend reading Bostrom&#x27;s and Yudkowsky&#x27;s writing on this (for example: &quot;Superintelligence: Paths, Dangers, Strategies&quot;). Note these are philosophers trying to build AI, so they&#x27;re definitely not luddites or anything. Same with Elon Musk; whatever one might think of him, he&#x27;s definitely not a luddite trying to convince people to stop developing technology, yet he&#x27;s also very concerned about superintelligent AI.<p>It has nothing to do with sci-fi. It&#x27;s a complex and difficult-to-predict philosophical problem.<p>It&#x27;s certainly possible some AIs may decide to just leave. Or maybe some will leave and some will stay and be ordered by a government to kill a few hundred thousand people. Or maybe some will leave and one will stay and malfunction and cause a neurotoxin to be released (at least until you throw its various personality cores into an incinerator).<p>If you assume there exists an entity which can continuously improve itself until it&#x27;s much smarter and more powerful than any human, then that alone is a risk, since you may not be able to predict or have any control over what it may wittingly or unwittingly do, or what its objectives may be, if any, or how it may perceive things, or how vulnerable it might be to tampering from humans or other AIs, etc.<p>Of course, these existential issues are likely decades or perhaps centuries away, but the discussion is about the theoretical possibilities irrespective of the timeline.')