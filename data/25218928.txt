Item(by='sharpneli', descendants=None, kids=[25219981, 25226356, 25220878], score=None, time=1606389414, title=None, item_type='comment', url=None, parent=25217353, text='The current CPU&#x27;s already are VLIW in a fashion. Its just that they have a hardware jitter for it. That&#x27;s what superscalar OoO CPU basically is, a piece of HW that generates VLIW instructions based on the normal instructions that come in. A superscalar execution port is basically just one subinstruction in VLIW. Explicit VLIW only helps you to save that piece of silicon from the chip. And it&#x27;s not that much in the grand scheme of things. It used to be, but not anymore.<p>Static compile time VLIW means one cannot really make it wider anymore, or narrower. Dynamically doing it on runtime means a cheaper and smaller core can just be narrower. Remove an instruction slot for one integer ALU? Fine, no issues. Everything still works, albeit slower by that much. Make a beefier chip? Perfect, it got faster by the amount of instruction level parallelism that was available.<p>In addition a compiler cannot really see across function boundaries (except if it&#x27;s statically determinable). A jitter can. Modern chips have reordering window of hundreds of instructions, M1 apparently goes up to over 600. That&#x27;s quite a lot of stuff there that it can dynamically reorder across. A compiler might not have noticed that due to some weird dynamic call that was not visible during compilation time there is now an FPU instruction that could be inserted here, an OoO processor can.<p>Due to that explicit VLIW is basically dead outside of some highly specific applications, like DPS and whatnot.<p>In a similar fashion ridculously wide vector units were obsoleted by the approach pioneered by GPU&#x27;s. Just add few things to allow masking based on branches and you get SIMT approach. Write as if it was scalar code and it&#x27;ll run on HW that has vector lengths going from none at all up to whatever, NVidia has 32 wide vector units as an example.')