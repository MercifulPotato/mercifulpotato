Item(by='probably_wrong', descendants=None, kids=[25728846], score=None, time=1610372480, title=None, item_type='comment', url=None, parent=25728356, text='I do not see how GPT-3 could solve the basic architectural problem that the parent comment quotes, namely, that <i>&quot;driven as it is by information that is ultimately about language use, rather than directly about the real world, it roams untethered to the truth&quot;</i>.<p>As an experiment I used a GPT-3-powered website [1] to see what GPT-3 has to say about bears, and the first answer was:<p>&gt; <i>&quot;Weird that every day, there are so many cute&#x2F;funny&#x2F;entertaining bears to enjoy online but hardly any on the ground.&quot;</i><p>When asked about <i>beards</i>, the first answer has no relation with beards at all:<p>&gt; <i>&quot;If a person doesnâ€™t constantly outwit, outplay, outlast, others, the strong eat the weak.&quot;</i><p>And then there&#x27;s that time when GPT-3 told someone to kill themselves [2].<p>While funny and (mostly) grammatically correct, these &quot;thoughts&quot; are nonsense and no amount of extra parameters is going to solve the disconnection between GPT-3 and reality. I imagine you could condition GPT-3 to generate text for a specific piece of data in such a way that guarantees the correctness of its output, but at that point you might as well throw GPT-3 away and write a rule-based system.<p>[1] <a href="https:&#x2F;&#x2F;thoughts.sushant-kumar.com&#x2F;bears" rel="nofollow">https:&#x2F;&#x2F;thoughts.sushant-kumar.com&#x2F;bears</a><p>[2] <a href="https:&#x2F;&#x2F;www.nabla.com&#x2F;blog&#x2F;gpt-3&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.nabla.com&#x2F;blog&#x2F;gpt-3&#x2F;</a>')