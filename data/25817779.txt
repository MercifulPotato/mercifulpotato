Item(by='Jugurtha', descendants=None, kids=None, score=None, time=1610940579, title=None, item_type='comment', url=None, parent=25816618, text='Congratulations for your new laptop : )<p>What we mean by automatic model detection is that you don&#x27;t have to add experiment tracking code to your notebook to &quot;log&quot; most of the usual stuff such as model, parameters, metrics; we do that for you so you don&#x27;t clutter your notebook. You <i>can</i> do it explicitly if you want to or if you&#x27;re trying to log something in particular, but you mostly don&#x27;t have to think about it, or generate experiment names, or worry about where you are logging your models.<p>- You use a notebook to write code to train and evaluate your model<p>- You schedule it (<a href="https:&#x2F;&#x2F;iko.ai&#x2F;docs&#x2F;resources&#x2F;img&#x2F;async.gif" rel="nofollow">https:&#x2F;&#x2F;iko.ai&#x2F;docs&#x2F;resources&#x2F;img&#x2F;async.gif</a>)<p>- Your notebook is executed and the experiment is tracked (notebook, model, parameters, metrics). This way you can compare your runs.<p>- You can then deploy your model to a &quot;REST API&quot; (<a href="https:&#x2F;&#x2F;iko.ai&#x2F;docs&#x2F;resources&#x2F;img&#x2F;deploy.gif" rel="nofollow">https:&#x2F;&#x2F;iko.ai&#x2F;docs&#x2F;resources&#x2F;img&#x2F;deploy.gif</a>), or build its Docker image and push it to use elsewhere (<a href="https:&#x2F;&#x2F;iko.ai&#x2F;docs&#x2F;resources&#x2F;img&#x2F;model_docker_image.gif" rel="nofollow">https:&#x2F;&#x2F;iko.ai&#x2F;docs&#x2F;resources&#x2F;img&#x2F;model_docker_image.gif</a>).<p>Then you can monitor your deployed model&#x27;s performance on a live dashboard.<p>&gt;<i>So at some point I am not sure that Jupyter is going to be right. But I like it so maybe I can stretch out it&#x27;s utility.</i><p>Generally speaking, you use the notebook to train models and you use these models to return predictions. In the workflow I described above, the models produced are deployed and interacted with through requests.')