Item(by='webmaven', descendants=None, kids=None, score=None, time=1604855481, title=None, item_type='comment', url=None, parent=25019819, text='<i>&gt; Is there any good reason why a fully-connected network needs more than one hidden layer?</i><p>A sufficiently (infinitely?) wide shallow fcnn can work in theory, but is basically impossible to train[0].<p>It might be feasible to transform or &#x27;compile&#x27; a trained deep network into a shallow &amp; wide one, but I&#x27;m not sure there would be any benefits, absent sufficiently wide parallel hardware[1]<p>[0] Then again, deep networks were impossible to train for quite a while too, due to the exploding gradient problem.<p>[1] Although, the Cerebras Wafer-Scale chip does exist. Hmm.')