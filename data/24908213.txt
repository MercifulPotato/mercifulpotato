Item(by='leftyted', descendants=None, kids=[24911292], score=None, time=1603811825, title=None, item_type='comment', url=None, parent=24908114, text='&gt; In our heads, language is a combination of words and concepts, and knowledge can be encoded by making connections between concepts, not simply words. If there is no concept or idea backing up the words, it can hardly be called knowledge.<p>Great point.<p>&gt; A language model such as GPT-3 operates only on words, not concepts. It can make connections between words on the basis of statistical correlations, but has no capacity for encoding concepts, and therefore cannot &quot;know&quot; anything.<p>Are you sure?  Aren&#x27;t &quot;concepts&quot; encoded in how language is used, at least to some degree?<p>LeCun does say that models that explicitly attempt represent knowledge perform better than GPT-3 in terms of answering questions.  I&#x27;m no expert but I believe him.')