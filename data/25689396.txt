Item(by='jalammar', descendants=None, kids=[25690017], score=None, time=1610135405, title=None, item_type='comment', url=None, parent=25688876, text='Wonderful! Thanks!<p>I am curious about those recent O(L) attention transformers (see slide 106 of <a href="http:&#x2F;&#x2F;gabrielilharco.com&#x2F;publications&#x2F;EMNLP_2020_Tutorial__High_Performance_NLP.pdf" rel="nofollow">http:&#x2F;&#x2F;gabrielilharco.com&#x2F;publications&#x2F;EMNLP_2020_Tutorial__...</a>). If these methods are converging towards a new self-attention mechanism, I&#x27;d love to try illustrating that.<p>What other attention modes are you referring to? Did something in particular catch your attention?')