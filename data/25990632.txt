Item(by='srean', descendants=None, kids=[25991411, 25992229], score=None, time=1612199721, title=None, item_type='comment', url=None, parent=25990234, text='&gt; Isn&#x27;t multi-armed bandit a simple reinforcement learning algo<p>It is indeed.<p>Its a restricted form of it. In RL one can allow a state change after an action which in turn can make the same &#x27;arms&#x27;|&#x27;actions&#x27; behave differently because their behavior is tied to the state. The state one lands in can also exercise control over which state you end up next. Its for this reason some extra book keeping is necessary for full-fledged RL. But you are absolutely right that bandits are considered a simplified version of RL. By controlling the size of the state space one can control how bandit like the solution will behave.<p>There is also something called a contextual bandit that sits between pure bandits and RL. CBs do not have state change, but they do have access to a side information that can affect the &#x27;arms&#x27;. In RL one needs to think not only about  the reward but also about the possibility of ending up in a &#x27;dead-end| hard-to-recover-from&#x27; state because the immediate reward was high. CBs do not have such &#x27;traps&#x27; but have more modeling power because the reward of an arm can depend on this side-information, usually called context.<p>The heat that you are getting from some comments is unwarranted.<p>EDIT: Holy mother of monkey milk you have a ton of super interesting interviews ! Glad I ran into your content. Better late than never.')