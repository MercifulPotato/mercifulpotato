Item(by='brazzy', descendants=None, kids=None, score=None, time=1606736088, title=None, item_type='comment', url=None, parent=25252362, text='&gt; So if learned models give us access to some truths without access to their (human intelligible) explanations, I think we need to just embrace that.<p>The question is: if the model is not interpretable or understandable, how do you know that what it gives you is, in fact, truth?<p>You basically need some kind of external validation of the results. In the case of Go, the rules of the game and competion basically provide that in a very authoritative way. I don&#x27;t think this is the case for all that many domains.<p>As I see it, behind the desire for interpretability there are two main concerns:<p>* could a model that gives really good answers in all the cases we have tested still give catastrophically wrong answers in some cases we have not foreseen?<p>* could the model be relying on some flaw or bias in the training data which we haven&#x27;t realized?')