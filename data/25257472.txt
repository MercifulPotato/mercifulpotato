Item(by='DylanDmitri', descendants=None, kids=[25257764], score=None, time=1606762759, title=None, item_type='comment', url=None, parent=25255543, text='I wrote an &quot;anything-regressor&quot; in SymPy, where you provide an equation (like y=mx+b) and a dataset of x&#x27;s and y&#x27;s, and it solves for constants to get a line of best fit. Super simple to implement and worked for any equation (quartic, logistic, etc) and any dimension of input variables.<p>Eventual goal was trying to implement something like XGBoost, but by applying successive regression equations to the residuals instead of successive decision trees. E.g. figure out that a sin wave best fits the initial data, and then a linear best explains the remaining difference.<p>Worked pretty well on toy data sets, but was much too slow to scale to anything real. Still think the idea has promise for producing human interpretable ML models.')