Item(by='nullc', descendants=None, kids=None, score=None, time=1608753538, title=None, item_type='comment', url=None, parent=25518489, text='I once contemplated a notion of optimization process risks where we should consider how narrow a target out of the space of all possible universes the process could pull ours into.<p>Unfortunately, I realized that even a simple phase-locked loop -- like what you use to condition an oscillator to be in sync with TAI using GPS -- can actually hit absurdly narrow targets... e.g. two oscillators a continent apart humming away within parts per billion of each other,  yet that kind of process isn&#x27;t particularly dangerous.  Even brining millions of oscillators into sync isn&#x27;t going to cause some great harm.<p>I&#x27;m not sure how to reason about it, but I think people are much too concerned about AGI risks relative to dumb optimization process risks.  It&#x27;s similar to fretting about movie-plot villains when most actual evil in the world comes from indifference and bad incentives or people trying to &quot;help&quot;.')