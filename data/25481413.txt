Item(by='mattkrause', descendants=None, kids=None, score=None, time=1608413542, title=None, item_type='comment', url=None, parent=25481183, text='You can monkey around with it here: <a href="http:&#x2F;&#x2F;bionlp-www.utu.fi&#x2F;wv_demo&#x2F;" rel="nofollow">http:&#x2F;&#x2F;bionlp-www.utu.fi&#x2F;wv_demo&#x2F;</a> (choose the English Google News model, but this may not be exactly the same set&#x2F;model as the original report).<p>Man is to Woman as Doctor is to ___ gives 1) gynecologist\n2) nurse 3) doctors 4) physician 5) pediatrician<p>Woman is to Man as Doctor is to ___ gives:\n1) physician 2) doctors 3) surgeon 4) dentist 5) cardiologist<p>These are just generally near &quot;Doctor&quot; though: the ten nearest terms are physician, doctors, gynecologist, surgeon, dentist, pediatrician, pharmacist, neurologist, cardiologist, and nurse.<p>Some gender differences may persist (nurse is #2 for `woman`, but #68 for `man`, but it&#x27;s also near `woman` generally and you could imagine it gets a bit of a boost from the verb (&quot;to feed a baby&quot;) being attached exclusively to women too.<p>Anyway, my point is not that there&#x27;s no bias (there certainly can be--seed GTP-3 with a prompt about Muslims) but that one should be wary of thinking they know what the model is doing.')