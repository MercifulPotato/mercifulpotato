Item(by='tzs', descendants=None, kids=None, score=None, time=1606505930, title=None, item_type='comment', url=None, parent=25228359, text='What&#x27;s the error rate nowadays?<p>In the early 2000s, what I could find published suggested somewhere in the 0.2-1 bit error per GB per day.<p>I had a consumer desktop (no ECC) running as a home server, with plenty of spare memory, so I tried to verify that. I had a program that simply allocated 128 MB, wrote a pattern to it, and then every 60 seconds checked to see if any of the bits had changed.<p>At the low end of the expected error range, 0.2 bit errors per GB per day, my 128 MB pattern should have had an error on average every 40 days. Or every 80 days if the bit flipping only went one way because the pattern had an equal number of 0 bits and 1 bits.<p>I ran this for a couple years and never spotted an error. I never did figure out of the published error rate was too high, at least for the kind of RAM I had, or if there was something about my local environment that was making it harder for radiation that could flip bits from making it to my computer.')