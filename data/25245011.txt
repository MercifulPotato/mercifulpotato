Item(by='jmillikin', descendants=None, kids=[25246771], score=None, time=1606656544, title=None, item_type='comment', url=None, parent=25243159, text='I spent six years on Borg SRE, and two years so far working with Kubernetes, and this post reads like a strange combination of utopianism and adoration of obsolete ideas.<p>Picking some parts to comment on at semi-random:<p><pre><code>  &gt; For that, let’s keep old versions of pod definitions\n  &gt; around, and make it trivial to “go back to version N”.\n  &gt;\n  &gt; [...]\n  &gt;\n  &gt; Bonus things you get from this: a diffable history of\n  &gt; what happened to your cluster, without needing GitOps\n  &gt; nonsense. By all means keep the GitOps nonsense if you\n  &gt; want, it has benefits, but you can answer a basic “what\n  &gt; changed?” question using only data in the cluster.\n</code></pre>\nThis assumes either tiny clusters that will never run more than a hundred machines at a time, or an audit horizon measured in weeks. The clusters I help run today are much smaller than what some companies operate, but already we&#x27;re hitting scaling issues with the sheer amount of data that sticks around during normal operation. If we had to store multiple copies of _pods_ for longer than a couple minutes then there wouldn&#x27;t be an EBS volume with enough iops to handle background rescheduling.<p>If you want an audit log, the obvious place is Git. I don&#x27;t know why the OP derisively calls this &quot;GitOps nonsense&quot;, because Google does the same thing and you should too. Figuring out what changed two months ago is much easier when each change has a commit message and a reviewer.<p><pre><code>  &gt; The latter is the bane of MetalLB’s existence, wherein it\n  &gt; gets into fights with other load-balancer implementations.\n  &gt; That should never happen. The orchestrator should have\n  &gt; rejected MetalLB’s addition to the cluster, because\n  &gt; LB-related fields would have two owners.\n</code></pre>\nTo me the problem seems less an issue of field ownership, and more a problem of the network tier mutating parts of the workload scheduling tier. Why is MetalLB (or any other load balancer) changing Kubernetes state at all? Something has gone wrong here. The load balancer should watch the Kubernetes API to discover which endpoints exist and what their IPs are, and if it tries to _change_ state then that change should be blocked by the configured authorization policy.<p><pre><code>  &gt; So, for starters, let’s rip out all k8s networking.\n  &gt; Overlay networks, gone. Services, gone. CNI, gone.\n  &gt; kube-proxy, gone. Network addons, gone.\n</code></pre>\nIf the author tries to start designing their new network stack they&#x27;ll quickly have to put overlay networks and CNI and so on back in, because it turns out the real world gets a say in how we run our infrastructure and users need to be able to customize the boundary between Kubernetes and everything else.<p>Kubernetes already suffers from insufficient customization in some areas, and networking is one of the few bright spots where it gives in and lets the operator do whatever we want to. IPv4? IPv6? Dual stack heterogenous routing? CNI lets you mix-n-match anything you can put into a binary as long as it can output JSON, and if it were even slightly more opinionated then it wouldn&#x27;t be fit for purpose.<p><pre><code>  &gt; Let’s give every pod an IPv6 address. Yes, only an IPv6\n  &gt; address for now. Where do they come from? Your LAN has a\n  &gt; &#x2F;64 already (if it doesn’t, get with the program, I’m\n  &gt; not designing for the past here), so pluck IPs from there.\n  &gt;\n  &gt; [... lots of description of a thing that could be a\n  &gt;  CNI plugin ...]\n  &gt;\n  &gt; That leaves bare metal clusters out in the cold, sort-of.\n  &gt; I argue this is a good thing, because there is no\n  &gt; one-size-fits-all load balancing.\n</code></pre>\nAnd this is why you absolutely don&#x27;t want your workload scheduler to have opinions about networking. All of the stuff in there -- the hard requirement on an IPv6-aware fabric, the mission-impossible idea of routing traffic to pod IPs allocated at random from the full local subnet, the NAT64 (!!) -- can be done with a relatively small driver binary in any language that can call netlink, which means it&#x27;s all possible to that in Kubernetes _today_ without foreclosing on the idea of running outside a carefully curated cloud environment.<p><pre><code>  &gt; We’re going to focus on doing one thing really well: if\n  &gt; you send me a packet for a pod, I’ll get the packet to\n  &gt; the pod. You can take that and build LBs based on LVS,\n  &gt; nginx, maglev-style things, cloud LBs, F5 boxes, the\n  &gt; world’s your oyster. And maybe I’ll even provide a couple\n  &gt; “default” implementations, as a treat. I do have lots of\n  &gt; opinions about load-balancers, so maybe I can make you a\n  &gt; good one. But the key is that the orchestration system\n  &gt; knows nothing about any of this, all it does is deliver\n  &gt; packets to pods.\n</code></pre>\nThe workload scheduler (Kubernetes is a workload scheduler) shouldn&#x27;t be in the business of delivering packets. That&#x27;s up to the kernel and the network fabric. If your services&#x27; packets have to transit a userspace proxy on the way to their destination then you&#x27;re already in trouble, and if that proxy is implemented by a second-order Wireguard overlay then all hope is lost.<p><pre><code>  &gt; I think this mostly translates to syncing more data down\n  &gt; to nodes in persistent storage, so that nodes have\n  &gt; everything they need to come back up into the programmed\n  &gt; state, even from a cold boot. Conceptually, I want the\n  &gt; cluster orchestrator to populate a set of systemd units\n  &gt; on each machine, and then switch to a very passive role\n  &gt; in the node’s life.\n  &gt;\n  &gt; [...]\n  &gt;\n  &gt; One way to view this is that in my “distributed” cluster\n  &gt; my pods are more likely to be unreplicated pets.\n</code></pre>\nIt sounds like the author doesn&#x27;t need Kubernetes at all. They want Puppet, or something Puppet-shaped, but with all the extra complexity that comes from having a distributed control plane.<p>I don&#x27;t know why they would want that, since if they&#x27;re using Kubernetes at all then presumably they&#x27;ve got at least a few hundred machines being managed by a couple different product teams.<p>There is a place for a central service that writes stuff to `&#x2F;etc&#x2F;` and assumes that individual machines are meaningful, but that service&#x27;s target audience is completely separate from that of Kubernetes. There&#x27;s no point in trying to design a replacement for Kubernetes to fit that market, any more than trying to design a 16-wheeler that competes with minivans.')