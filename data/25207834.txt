Item(by='mewse', descendants=None, kids=[25227178, 25209448, 25208045], score=None, time=1606301076, title=None, item_type='comment', url=None, parent=25207111, text='GPT-3 can produce text that&#x27;s probabilistically similar to text that it&#x27;s been trained on, and observed as part of sample outputs.  If there was no huge corpus of human language to train it on, GPT-3 couldn&#x27;t even begin to give the illusion of thinking, and certainly couldn&#x27;t tell you (for example) that 3 is greater than 2, or even know that 3 and 2 were concepts that it perhaps should have opinions about.<p>The really interesting question (at least to me) is to what extent that observation is also true for humans.<p>I mean, we humans presumably built up our big corpus of human language&#x2F;knowledge all on our own over the lifetime of the species, which GPT-3 currently cannot do, but.. to what extent does human thinking just consist of probabilistic re-mixing of words, phrases, and sentences that we&#x27;ve seen before, that came to us through our continual training on segments we&#x27;ve been exposed to from that big &#x27;dataset&#x27; of human knowledge, and the best of which then get contributed back into that dataset?  How much more than that is actually going on, for us?<p>If what GPT-3 is doing shouldn&#x27;t really count as &#x27;thinking&#x27; (as seems intuitive to me, personally, though others may certainly disagree), then to what extent can we say that humans do anything qualitatively different?')