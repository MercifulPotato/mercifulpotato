Item(by='josephg', descendants=None, kids=[25092240], score=None, time=1605358952, title=None, item_type='comment', url=None, parent=25091711, text='&gt; Many existing filesystems simply cannot support such an API because they&#x27;re not journaling or CoW.<p>Thats fine. Legacy filesystems can just use a best-effort approach like they do now with 95% of programs. I already wouldn&#x27;t put data I care about on ext2. Programs could detect transaction support with a syscall then decide whether to fallback to old behaviour, lose reliability or for a database maybe just refuse to run entirely.<p>&gt; This suddenly looks more and more like a database than a posix filesystem.<p>POSIX filesystem apis aren&#x27;t fit for use when you care about correctness. And as I understand it ZFS, btrfs and APFS already look more like databases internally than traditional posix filesystems. If that ship is sailing anyway we may as well get all the benefits we can.<p>&gt; There are many use-cases that don&#x27;t need such hard guarantees and want throughput, minimal overhead and latency instead.<p>Are there? I can&#x27;t think of many. Maybe copying big files? We&#x27;ll need to keep supporting the current unsafe-by-default posix API forever for current software anyway. I just don&#x27;t think it should be the default for 95% of applications. When I do a git commit, its plenty fast enough. But I definitely don&#x27;t want my repository to get corrupted if my computer crashes.<p>If the recommendation is that everything that matters should be in a sqlite database because posix is so hard to use, well, that sounds like we agree more than we disagree. It seems like a pity though, and I think the filesystem is worth salvaging.<p>And with the API I&#x27;m suggesting, I don&#x27;t think large operations <i>need</i> to get much slower. The transactional API could just optimistically write at full speed to unlinked sectors on disk, then atomically update pointers to reference the new sectors when the transaction is committed. That wouldn&#x27;t add much amortized overhead. I&#x27;m sure there&#x27;s more complexity than that in practice, but it sounds pretty tractable and fast in the common case of a single writer. (And in the slow case where multiple programs try to fight over the same file, slow would be an improvement over the current behaviour of &quot;probably everything breaks&quot;.)<p>&gt; I think the postgres&#x2F;fsync error saga raised awareness of that and there were some proposals how to provide a better error channel to userspace.<p>Its still almost impossible to write a database correctly on top of the POSIX APIs. Especially if you want it to be correct. That seems fixable. We should fix it.')