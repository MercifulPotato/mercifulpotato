Item(by='blackbear_', descendants=None, kids=[24841539, 24841868], score=None, time=1603221138, title=None, item_type='comment', url=None, parent=24835336, text='There is another reason why training deep neural networks is not as difficult as it sounds: the landscape of the loss function seems to be made of broad &quot;U&quot;-shaped valleys that gently descend towards a small loss region. At initialization, the network is likely close to such valley, and once it gets there the rest of training is just a leisurely stroll.<p>Formally, people have studied the spectrum of the Hessian and found that most of its eigenvectors are quite small with only a few, much larger ones. It all started with [1], with several recent extensions.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1611.07476.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1611.07476.pdf</a>')