Item(by='slotrans', descendants=None, kids=[24723059], score=None, time=1602179480, title=None, item_type='comment', url=None, parent=24719187, text='The simple way to understand the utility of a data lake is &quot;S3 is phenomenally cheap&quot;.<p>Some people treat this as an excuse to throw whatever they want into it, without any organization or standardization... and the consequences of that are quite predictable.<p>But it doesn&#x27;t have to be that way. You can accumulate diverse and large data sets, in cheap cloud storage, while knowing <i>what</i> everything is and <i>where</i> you can find it.<p>As a trivial example, let&#x27;s say you have a typical OLTP database (or perhaps many), with useful data that is, unfortunately, mutable. You can store entire copies of those tables in your data lake for pennies a day, giving you the ability to recall a transactionally-consistent view of that data from various past times. This is something we&#x27;ve always been able to do using traditional tools, the difference is that storing the data in a &quot;data lake&quot; (i.e. S3) is orders-of-magnitude <i>cheaper</i>.<p>Another major use case, perhaps the most significant one, is storing the raw ingested data -- e.g. from telemetry collection, 3rd party exports, etc -- <i>along with each stage of its transformation</i>. By preserving the original input, along with all intermediate outputs, no information is ever lost. If a buggy transformation is discovered it no longer means your output is irrevocably corrupted, fixed results can be re-computed from wherever in the transform pipeline the bug manifested. And again, this was always possible, a data lake just makes it cheap enough to actually do on a large scale.')