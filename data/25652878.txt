Item(by='mlucy', descendants=None, kids=None, score=None, time=1609890129, title=None, item_type='comment', url=None, parent=25649740, text='This is really cool!  I especially like that there&#x27;s a premade colab notebook that lets you play with it: <a href="https:&#x2F;&#x2F;colab.research.google.com&#x2F;github&#x2F;openai&#x2F;clip&#x2F;blob&#x2F;master&#x2F;Interacting_with_CLIP.ipynb" rel="nofollow">https:&#x2F;&#x2F;colab.research.google.com&#x2F;github&#x2F;openai&#x2F;clip&#x2F;blob&#x2F;ma...</a> .<p>I&#x27;m a little surprised that the paper doesn&#x27;t seem to mention the effect of fine-tuning pretrained image and text encoders taken from somewhere else instead of learning the encoding from scratch.  I would naively expect that to take way less compute to get good results, and possibly generalize better.<p>I guess the point is to test whether this technique is actually good for learning new representations from scratch?  Still, I&#x27;m sure they must have run the experiment at some point just to see, and it would&#x27;ve been really interesting to see the numbers.')