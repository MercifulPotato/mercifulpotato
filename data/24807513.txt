Item(by='calebkaiser', descendants=None, kids=None, score=None, time=1602909354, title=None, item_type='comment', url=None, parent=24786778, text='I was expecting this to be more about running inference in production, though the information in the article itself was interesting on its own.<p>There does seem to be a dearth of writing on the actual topic of deploying models as prediction APIs, however. I work on an open source ML deployment platform ( <a href="https:&#x2F;&#x2F;github.com&#x2F;cortexlabs&#x2F;cortex" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;cortexlabs&#x2F;cortex</a> ) and the problems we spend the most time on&#x2F;teams struggle with the most don&#x27;t seem to be written about very often, at least in depth (e.g. How do you optimize inference costs? When should you use batch vs realtime? How do you integrate retraining, validation, and deployment into a CI&#x2F;CD pipeline for your ML service?).<p>Not taking anyway from the article of course, it is well written and interesting imo.')