Item(by='spekcular', descendants=None, kids=None, score=None, time=1606711321, title=None, item_type='comment', url=None, parent=25250638, text='One reason to want to understand what some &quot;black box&quot; is doing is distribution&#x2F;dataset shift, especially in medical applications.<p>For example, suppose you&#x27;re building a neural net to detect early-stage lung cancer on medical imaging, and you test&#x2F;train it on patients in a small set of hospitals. Often the hospital name is given on the image, and this can be used as a covariate to improve accuracy (due to the hospitals serving different populations with different demographics). But a model that does this may suffer when put into production at other hospitals.<p>Some real-life examples with this flavor are given at the end of these slides: <a href="https:&#x2F;&#x2F;mlhcmit.github.io&#x2F;slides&#x2F;lecture10.pdf" rel="nofollow">https:&#x2F;&#x2F;mlhcmit.github.io&#x2F;slides&#x2F;lecture10.pdf</a>.<p>See also Section 6.3 of this paper for how interpretability can help choose models with superior generalization: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1602.04938" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1602.04938</a>.')