Item(by='ArnoVW', descendants=None, kids=None, score=None, time=1604216924, title=None, item_type='comment', url=None, parent=24956459, text='It used to be that we had to chose between models that were &#x27;powerful&#x27; (complex neural networks) and those that were &#x27;interpretable&#x27; (linear r√©gression, trees). In the last years a lot of research has come out.<p>We have since then developed ways of  analysing the inner workings of neural networks.<p>You can understand how each feature drives the predicted result by using SHAP <a href="https:&#x2F;&#x2F;github.com&#x2F;slundberg&#x2F;shap" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;slundberg&#x2F;shap</a><p>You can analyse the individual layers and gain an understanding of how each layer encodes the input<p><a href="https:&#x2F;&#x2F;ai.googleblog.com&#x2F;2017&#x2F;11&#x2F;interpreting-deep-neural-networks-with.html" rel="nofollow">https:&#x2F;&#x2F;ai.googleblog.com&#x2F;2017&#x2F;11&#x2F;interpreting-deep-neural-n...</a><p><a href="https:&#x2F;&#x2F;www.kdnuggets.com&#x2F;2019&#x2F;07&#x2F;google-technique-understand-neural-networks-thinking.html" rel="nofollow">https:&#x2F;&#x2F;www.kdnuggets.com&#x2F;2019&#x2F;07&#x2F;google-technique-understan...</a><p>Do we &#x27;know&#x27; in an absolute way why a network thinks something? Not yet. But neither do we know that for humans. We just have a huge amount of experience with their failure modes and we work around them (see aviation).')