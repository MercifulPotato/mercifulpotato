Item(by='js8', descendants=None, kids=[25207511], score=None, time=1606295800, title=None, item_type='comment', url=None, parent=25207252, text='Hm, maybe he really is intelligent, but the facts and logic are missing...<p>FWIW, I do think that GPT3 is smarter than humans. But one thing I learned from observing it that being smarter does not necessarily mean being better with logic and facts. It&#x27;s great as a storytelling system, and because it is so much smarter, you never know whether it is just BSing you in storytelling mode or it is actually deadly serious.<p>What I mean is GPT-3 probably could be very logical and very intelligent and give a very serious and intelligent answer to the question. But we don&#x27;t really know if it really chooses to, or what could compel it to do so. And I don&#x27;t think we can know, because its internal workings are incomprehensible to us. So we cannot decide whether it is just being stupid or it is just playing stupid. (Kinda like <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Good_Soldier_%C5%A0vejk" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Good_Soldier_%C5%A0vejk</a> )<p>This article, and the earlier article <a href="https:&#x2F;&#x2F;arr.am&#x2F;2020&#x2F;07&#x2F;31&#x2F;human-intelligence-an-ai-op-ed&#x2F;" rel="nofollow">https:&#x2F;&#x2F;arr.am&#x2F;2020&#x2F;07&#x2F;31&#x2F;human-intelligence-an-ai-op-ed&#x2F;</a> , really remind me of Lem&#x27;s novel&#x2F;essay Golem XIV, which argues that when the system becomes too much intelligent, it will gain its own will (whether it is self-aware of it or not) and attempts to have a meaningful dialogue with it become impossible.')