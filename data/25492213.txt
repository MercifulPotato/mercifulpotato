Item(by='NovaX', descendants=None, kids=None, score=None, time=1608526051, title=None, item_type='comment', url=None, parent=25491524, text='Perhaps the author is referring to a technique such as how a cache can be made concurrent [1]. In that case every read is a write, as LRU requires moving entries. Since an LRU is often implemented as a doubly-linked list, it either is a very complex lock-free algorithm or performed under a lock. The lock-free approach doesn&#x27;t scale because all the recent entries move to the MRU position (tail), so there is contention.<p>The approach used in that article is to publish the events into multi-producer&#x2F;single-consumer queues. When a queue is full, then the batch can be performed under a lock to replay and catch-up the LRU. This way the lock is used to ensure single-writer, but does not suffer lock contention. The ring buffer is lock-free, but the cache itself is not. Lock-free can suffer contention or be more expensive algorithmically than if under a lock, so whichever strategy chosen requires careful consideration of the system performance.<p>[1] <a href="http:&#x2F;&#x2F;highscalability.com&#x2F;blog&#x2F;2016&#x2F;1&#x2F;25&#x2F;design-of-a-modern-cache.html" rel="nofollow">http:&#x2F;&#x2F;highscalability.com&#x2F;blog&#x2F;2016&#x2F;1&#x2F;25&#x2F;design-of-a-modern...</a>')