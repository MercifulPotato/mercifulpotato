Item(by='sradman', descendants=None, kids=None, score=None, time=1611260300, title=None, item_type='comment', url=None, parent=25862757, text='I think Intel SIMD (AVX-512) is still best-of-class for vector math. Run Length Encoded algorithms in column stores like SAP HANA are currently the key use-case (revenue&#x2F;market wise). What I&#x27;ve learned since the release of Apple M1 is that ISA extensions for Matrix math, as incorporated in Apple&#x27;s AMX, complement the Vector-centric SIMD ISA and are probably a key battle front for ML&#x2F;DL. A good Vector&#x2F;Matrix SIMD-like ISA should cover many ML&#x2F;DL use cases currently addressed using a discreet accelerator like Apple&#x27;s Neural Engine and Nvidia GPUs.<p>Amazon&#x27;s Graviton ARM Neoverse CPUs only implement ARM NEON SIMD rather than the newer SVE2. I don&#x27;t know if ARM SVE2 has Matrix math instructions like AMX does. Intel AVX++ might be an important alternative to discreet ML&#x2F;DL accelerators. DL Training accelerators appear to be focused on RDMA Over Converged Ethernet [1] (RoCE) and I&#x27;m assuming that this technology is new enough for Intel to gain a foothold.<p>I don&#x27;t know if I understand this space well enough but I think there are enough truths in this explanations to keep me from ruling out Intel.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;RDMA_over_Converged_Ethernet" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;RDMA_over_Converged_Ethernet</a>')