Item(by='wahern', descendants=None, kids=None, score=None, time=1602921111, title=None, item_type='comment', url=None, parent=24807568, text='&gt; I wonder how much of an advantage this is. Memory availability is constantly in flux, and Linux at least, goes to great lengths to reclaim memory before invoking the OOM killer, which is a last resort.<p>Whether or not it goes to &quot;great lengths&quot; before invoking the OOM killer is irrelevant. 1) On heavily loaded systems the OOM killer can trigger regularly, &#x27;causing all manner of havoc because it can non-deterministically kill unrelated processes[1], and 2) a good kernel <i>should</i> go to great lengths to service a memory request, but that merely begs the question if great lengths should involve shooting down unrelated processes.<p>&gt; What if memory frees up immediately after the application gets the error return, but before the app touches it?<p>What if what? What if I try to create a TCP connection to a host which rejected the connection, or the request timed out, but the request had been delayed just a little longer it would have succeeded? What does that have to do with whether the kernel should start killing unrelated TCP streams?<p>&gt; I&#x27;d trust the kernel more to manage this than applications themselves, because the kernel can make much better decisions on how to juggle things (evicting caches and whatnot). Plus, we all know how well we all test our error handling paths :)<p>The kernel has very little information with which to determine which process to kill as an appropriate response to resource exhaustion. It&#x27;s a difficult problem, but OOM provides one solution that is acceptable to those who wouldn&#x27;t otherwise even care at the expense of incapacitating the ability of smart software which cares strongly to implement correct and deterministic solutions. Moreover, we&#x27;re talking about the <i>kernel</i> here. Generally speaking, kernels should provide mechanism, not policy. By conflating mechanism and policy the OOM killer is a fundamentally and fatally flawed approach. There is no way to &quot;fix&quot; an OOM killer approach without obscuring the line between application software and the kernel. That might be fine for embedded systems, but for a general purpose, multi-user system (which in the age of k8s is finally making a comeback), it&#x27;s just plain wrong.<p>[1] For most of 2019 a QoI (not correctness) regression in Linux&#x27; memory reclamation and page buffer code resulted in JVM processes on our k8s clusters doing heavy disk I&#x2F;O indirectly triggering OOM on a daily basis, causing unrelated, twice-removed processes to be terminated as various internal allocations timedout. Sometimes very important services would get killed, like systemd or docker. Facebook and Google understand this problem well, which is why on all their clusters they run their own processes which try to <i>predict</i> imminent invocation of the OOM killer and throttle or shooting down processes from userspace. The whole charade is obviously ridiculous. Almost everybody would be better off with a simple rule that the process requesting an allocation was killed. That&#x27;s not as good as actually failing it and giving it the option to continue or die (much software does in fact exit on malloc failure), but still preferable. Then a ton of overwrought and buggy code in the kernel could disappear overnight, and smart applications would have path forward for achieving correct, deterministic behavior.')