Item(by='cestith', descendants=None, kids=[24862637, 24861895], score=None, time=1603391280, title=None, item_type='comment', url=None, parent=24861070, text='We pull unhealthy VMs and containers from rotation in real time. Serving a bunch of 500 series errors to clients because your application server took a nosedive isn&#x27;t great even with a 300 second TTL.<p>We don&#x27;t use NAT. When people say RFC1918 addresses aren&#x27;t routable they mean you can&#x27;t advertise them on the public Internet. You can totally have 10&#x2F;8 switched and routed internally. Even if you don&#x27;t want to mess with that, it&#x27;s entirely possible to set a proxy to proxy from its public IP to another public IP, but why waste the IP space, especially in v4? I can server 80 applications each with half a dozen or more backend systems from about six proxy servers. The proxy isn&#x27;t running the application and connecting to the RDBMS so each one can handle way, way more traffic than even an efficient application.<p>Using a proxy that&#x27;s layer 7 aware also lets you do things like scale different parts of a URI tree individually. If you&#x27;re not proxying, every copy of app.example.com serves everything under <a href="https:&#x2F;&#x2F;app.example.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;app.example.com&#x2F;</a> even if it&#x27;s one of 23 (about the most A records that will fit in UDP DNS) servers doing it. With a proxy, I can decide that <a href="https:&#x2F;&#x2F;app.example.com&#x2F;freestuff&#x2F;" rel="nofollow">https:&#x2F;&#x2F;app.example.com&#x2F;freestuff&#x2F;</a> is too busy and spikes the backends, and that the load of that and the rest of the stuff under &#x2F; is too hard to scale for properly when taken together. So I just tell my proxies that everything for <a href="https:&#x2F;&#x2F;app.example.com&#x2F;freestuff" rel="nofollow">https:&#x2F;&#x2F;app.example.com&#x2F;freestuff</a> goes to server set A and the rest of stuff in &#x2F; goes to server set B. Then the different performance and different demand for the two can be studied, improved, monitored, and reported separately at the VM or container (or even bare metal backend) level.<p>I can also throw memcached or Redis into the mix and do site-wide limiting on an IP that&#x27;s scanning or attacking my entire laundry list of applications. Even without those I can rate-limit what each proxy will accept from a single visitor globally or per backend type.')