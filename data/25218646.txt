Item(by='Vestergarden', descendants=0, kids=None, score=1, time=1606386198, title='Ask HN: Looking for Automated Testing Dogma', item_type='story', url=None, parent=None, text='Hi,<p>Intro:\nMy team and I are trying to implement automated testing into our workflow.\nWe are a small team with some strong internal conflicts towards testing such as TDD (Disrupts one&#x27;s workflow) and Coverage Unit Tests (Pointless). \nThese opinions are carved in stone, so lets for fun assume that these points are valid.<p>The structure of our project is build such it is possible to do TDD, so it is assumed that the current structure is not going to be a problem for automated testing.<p>I&#x27;ve advocated a middle ground, which I&#x27;m pretty sure a subset of ideas is sniffed from a Hacker News post from the front page.<p>Dogma:\nThe dogma is (as I remember):\n1. Write tests that encapsulate the essentials of the feature.\n2. A test class for each subpart of the feature is created.\n3. Run every test successfully before committing.\n4. Reported bugs have a designated bug test, and is linked to the card in our SCRUM board.<p>Explanation of Dogma:\n1 and 2 is such testing is done and 2 that the structure of tests are clear. Number 2 should make it easy to write a test after examine a bug, as a class should be available. 4 would link the tests to a more in depth explanation of the commit and the test and try to avoid the same bug occurring twice.<p>The gains of our testing would be to have a minimum testing workload but document essential features and bugs through testing while maturing the team in the field of Automated Testing.<p>Question:\nAs mentioned above, I think, I&#x27;ve read something similar on this page a while ago and would like if any of you had some documentation to dogmas&#x2F;ideas similar to what has been sketched above?\nAnd pitfalls beyond the assumptions?<p>Thanks :)')