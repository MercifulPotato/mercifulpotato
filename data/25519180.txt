Item(by='standevbob', descendants=None, kids=None, score=None, time=1608740494, title=None, item_type='comment', url=None, parent=25516749, text='That&#x27;s right---Stan doesn&#x27;t have any online learning facilities.  It&#x27;s very hard to approximate posteriors and chain them, so we don&#x27;t try.<p>If by &quot;big data&quot;, we&#x27;re talking about too big to fit in memory, that&#x27;s right.  Stan&#x27;s fully in-memory.  Compute can be distributed and GPU-powered for matrix ops, but all of the data and parameters and the core autodiff expression graph need to fit in memory.<p>For &quot;medium data&quot;, Stan&#x27;s adaptive Hamiltonian Monte Carlo sampling is much more efficient and scalable to complex models and higher dimensions than Gibbs or Metropolis.  I&#x27;m fitting a Covid prevalence model using a custom trend-following and mean-reverting second-order autoregression model over 400 distinct regions with weekly data that has 5M data points and 10K parameters and adjusts for sensitivity and specificity of various tests taken.  It fits in a single thread using MCMC in 24 hours or so, but we can fit the model with variational inference in a couple minutes.  Although variational inference often produces reasonable point estimates in bigger data settings, it doesn&#x27;t reasonably quantify uncertainty.  I&#x27;m also working on a genomics model for differential expression of splice variants that involves 120K measurements and just as many parameters to deal with overdispersion of biological replicates in a control and treatment group.  We&#x27;re using variational inference and it fits in a couple minutes for the comparitiver event probabilities we need to estimate.')