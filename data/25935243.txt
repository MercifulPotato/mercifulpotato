Item(by='refactor_master', descendants=None, kids=None, score=None, time=1611787788, title=None, item_type='comment', url=None, parent=25934434, text='It does indeed sound like it’s written the wrong way around.<p>Random information = high entropy. A common word is therefore low entropy.<p>Here’s a good analogy I found:<p>&gt; Informally, the amount of information in an email is proportional to the amount of “surprise” its reading causes. For example, if an email is simply a repeat of an earlier email, then it is not informative at all. On the other hand, if say the email reveals the outcome of a cliff-hanger election, then it is highly informative. Similarly, the information in a variable is tied to the amount of surprise that value of the variable causes when revealed. Shannon’s entropy quantifies the amount of information in a variable, thus providing the foundation for a theory around the notion of information.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1405.2061.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1405.2061.pdf</a>')