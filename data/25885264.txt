Item(by='GregarianChild', descendants=None, kids=[25885460, 25885544, 25885449, 25885507], score=None, time=1611429931, title=None, item_type='comment', url=None, parent=25884808, text='In what sense are neural nets <i>not</i> rigorous?<p>The whole pipeline from Pytorch or TensorFlow and Python to LLVM, to\nGPUs or TPUs is <i>absolutely</i> rigorous. Much more rigorous,\nin fact, than normal, hand-written mathematics, as you find it in\ne.g. a typical <i>Annals of Mathematics</i> publication, or mathematical textbook!<p>I think what you really have in mind is a simple model of modern deep\nlearning that is not fully accurate, but still useful!<p>Let me argue by analogy. You are looking for something\nthat is to deep learning what the lambda-calculus is to the Haskell\ncompiler. One of the main simplifications in\nprogramming language theory is replacing finite precision arithmetic\n(which is painfully complex) with mathematical integers and real\nnumbers (which are much simpler). Would a theory\nof deep learning based on mathematical reals be\nvaluable in a theory of deep learning? The stunning success of\nfloating point formats like bfloat16 [1] suggests otherwise, since\narithmetic precision in deep learning is closely connected to important learning\nphenomena such as overfitting and regularisation.<p>I am tempted to be provocative and say that you are really looking for   <i>less rigour</i>!<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bfloat16_floating-point_format" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bfloat16_floating-point_format</a>')