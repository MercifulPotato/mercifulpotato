Item(by='dragontamer', descendants=None, kids=[25268169], score=None, time=1606846397, title=None, item_type='comment', url=None, parent=25267896, text='Well... I didn&#x27;t describe the C++11 memory model above. I had a gross simplification, because I didn&#x27;t account for how the C++11 memory model acts &quot;relative to a variable&quot;. (And this &quot;variable&quot; is typically the mutex itself).<p>I don&#x27;t know much about the Linux Kernel, but I gave the document a brief read-over (<a href="https:&#x2F;&#x2F;www.kernel.org&#x2F;doc&#x2F;Documentation&#x2F;memory-barriers.txt" rel="nofollow">https:&#x2F;&#x2F;www.kernel.org&#x2F;doc&#x2F;Documentation&#x2F;memory-barriers.txt</a>).<p>My understanding is that WRITE_ONCE &#x2F; READ_ONCE are meant for this &quot;relative to a variable&quot; issue. Its _precisely_ the issue I ignored in my post above.<p>All C++11 atomics are &quot;relative to a variable&quot;. There are typically no memory-barriers floating around by themselves (there can be, but, you probably don&#x27;t need the free-floating memory barriers to get the job done).<p>So you wouldn&#x27;t write &quot;acquire_barrier()&quot; in C++11. You&#x27;d write &quot;atomic_var.store(value, memory_order_release)&quot;, saying that the half-barrier is relative to atomic_var itself.<p>----------<p><pre><code>    a();\n    b();\n    while(val = atomic_swap(spinlock, 1, acquire_consistency), val!= 0) hyperthread_yield(); &#x2F;&#x2F; half-barrier, write 1 into the spinlock while atomically reading its previous value\n    c();\n    d();\n    e();\n    atomic_store(spinlock, 0, release_consistency); &#x2F;&#x2F; Half barrier, 0 means we&#x27;re done with the lock\n    f(); \n    g();\n</code></pre>\nSo the C++ acquire&#x2F;release model is always relative to a variable, commonly the spinlock.<p>This means that &quot;c, d, and e&quot; are protected by the spinlock (or whatever synchronization variable you&#x27;re working with). Moving a or b &quot;inside the lock&quot; is fine, because that&#x27;s the &quot;unlocked region&quot;, and the higher-level programmer is fine with &quot;any order&quot; outside of the locked region.<p>Note: this means that c(), d(), and e() are free to be rearranged as necessary. For example:<p><pre><code>    while(val = atomic_swap(spinlock, 1, acquire_consistency), val!= 0) hyperthread_yield(); &#x2F;&#x2F; half-barrier, write 1 into the spinlock while atomically reading its previous value\n    for(int i=0; i&lt;100; i++){\n      value+=i;\n    }\n    atomic_store(spinlock, 0, release_consistency); &#x2F;&#x2F; Half barrier, 0 means we&#x27;re done with the lock\n</code></pre>\nThe optimizer is allowed to reorder the values inside into:<p><pre><code>    while(val = atomic_swap(spinlock, 1, acquire_consistency), val!= 0) hyperthread_yield(); &#x2F;&#x2F; half-barrier, write 1 into the spinlock while atomically reading its previous value\n\n    for(int i=99; i&gt;=0; i--){ &#x2F;&#x2F; decrement-and-test form is faster on many processors\n      value+=i;\n    }\n\n    atomic_store(spinlock, 0, release_consistency); &#x2F;&#x2F; Half barrier, 0 means we&#x27;re done with the lock\n</code></pre>\nIts the ordering &quot;relative&quot; to the spinlock that needs to be kept. Not the order of any of the other loads or stores that happen. As long as all value+=i stores are done &quot;before&quot; the atomic_store(spinlock) command, and &quot;after&quot; the atomic_swap(spinlock) command, all reorderings are valid.<p>So reordering from &quot;value+=0, value+=1, ... value+=99&quot; into &quot;value+=99, value+=98... value+=0&quot; is an allowable optimization.<p>----------<p>It seems like WRITE_ONCE &#x2F; READ_ONCE was written for DEC_Alpha, which is far weaker (less guarantees about order) than even ARM. DEC_Alpha was the first popular multicore system, but its memory model allowed a huge number of reorderings.<p>WRITE_ONCE &#x2F; READ_ONCE probably compile into no-ops on ARM or x86. I&#x27;m not 100% sure, but that&#x27;d be my guess. I think the last 20-years of CPU design has overall said that the DEC_Alpha&#x27;s reorderings were just too confusing to handle in the general case, so CPU designers &#x2F; low-level programmers just avoid that situation entirely.<p>&quot;dependent memory accesses&quot; is very similar to the confusing language of memory_order_consume. Which is again: a model almost no one understands, and almost no C++ compiler implements. :-) So we can probably ignore that.')