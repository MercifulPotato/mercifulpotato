Item(by='moyix', descendants=None, kids=None, score=None, time=1611430021, title=None, item_type='comment', url=None, parent=25885110, text='Linux (Ubuntu 20.04) + Cuda 11.2. For the backend I use PyTorch; Tensorflow has some nice optimizations (like XLA, which uses LLVM to JIT optimized code for the GPU), but I found it very painful to get working reliably, and most of the language modeling stuff I&#x27;ve seen uses PyTorch.<p>For the language model training itself I&#x27;ve been experimenting with a few different things. I started off with Huggingface because it&#x27;s very easy to get up and running, and I still use its tokenizers library to do BPE training on the C source dataset (though there are still some hitches there – other libraries expect slightly different formats for the tokenizer model, like using different ways to represent the &lt;|endoftext|&gt; marker).<p>After prototyping the C language model training at home, I tried moving the training up to NYU&#x27;s HPC cluster, which has a bunch of 4xV100 and 4xRTX8000 nodes (mainly because the sound of two powerful GPU fans running at 100% gets a bit old after a while). Unfortunately I discovered that with larger models the GPU-GPU communication overhead can be prohibitive (most of the cluster nodes only support P2P GPU communication over PCIe, which is a <i>lot</i> slower than NVLink), and Huggingface&#x27;s implementation actually performed <i>worse</i> on multiple GPUs than on two 3090s with NVLink (I opened an issue track it here <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;9371" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;9371</a> ).<p>Currently I&#x27;m working on getting DeepSpeed running so that I can hopefully get better scaling even in the absence of a fast GPU-GPU interconnect. This is again a little bit annoying, because it seems like every framework wants a slightly different way of representing the tokenizer and training data – I&#x27;ve had to preprocess the dataset in about 4 different ways (plain text, loose JSON, npy (for DeepSpeed), and a custom indexed binary format for Megatron-LM). I&#x27;m also hoping to try out Huggingface&#x27;s recently-released DeepSpeed integration, which (if it works) would be a really nice combination of usability and performance: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;zero-deepspeed-fairscale" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;zero-deepspeed-fairscale</a><p>As for other software stack hitches: so, so many. The main one is just managing the different versions of CUDA. The 3090 is only supported starting with CUDA 11.1, but many packages and frameworks only support 11.0 at best. And some of the newer things like DeepSpeed use PyTorch extensions, which require you to have the exact version of CUDA around that was used to build PyTorch. So I&#x27;ve had to do a fair bit of compiling packages from source rather than relying on prebuilt packages.<p>The path of least resistance here is probably to use the NVIDIA NGC containers, but it took NVIDIA more than a month to get them updated after the 3090 was released, and I find working inside containers for everything inconvenient anyway (I hate losing my bash history, and I always accidentally end up losing data or local changes when I exit a container).<p>Anyway, this ended up being a bit more rambling than I intended, but it was helpful to write it all down and maybe it&#x27;ll help someone else avoid some stumbling blocks :)')