Item(by='cs702', descendants=None, kids=None, score=None, time=1603228294, title=None, item_type='comment', url=None, parent=24835336, text='TL;DR: For high-dimensional models (say, with millions to billions of parameters), there&#x27;s always a good set parameters nearby, and when we start descending towards it, we are highly unlikely to get stuck, because almost always there&#x27;s at least one path down along at least one among of all those dimensions -- i.e., there are no local optima. Once we&#x27;ve stumbled upon a good set of parameters, as measured by validation, we can stop.<p>These intuitions are consistent with my experience... but I think there&#x27;s more to deep learning.<p>For instance, these intuitions fail to explain &quot;weird&quot; phenomena, such as &quot;double descent&quot; and &quot;interpolation thresholds&quot;:<p>* <a href="https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;deep-double-descent&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;deep-double-descent&#x2F;</a><p>* <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1809.09349" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1809.09349</a><p>* <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1812.11118" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1812.11118</a><p>* See also: <a href="http:&#x2F;&#x2F;www.stat.cmu.edu&#x2F;~ryantibs&#x2F;papers&#x2F;lsinter.pdf" rel="nofollow">http:&#x2F;&#x2F;www.stat.cmu.edu&#x2F;~ryantibs&#x2F;papers&#x2F;lsinter.pdf</a><p>We still don&#x27;t fully understand why stochastic gradient descent works so well in so many domains.')