Item(by='qsort', descendants=None, kids=None, score=None, time=1607188105, title=None, item_type='comment', url=None, parent=25315840, text='&gt; But I don&#x27;t think it&#x27;s just a buzzword<p>I don&#x27;t think it&#x27;s just a buzzword either, in the same sense that &#x27;AI&#x27; is not <i>just</i> a buzzword. But both &quot;AI&quot; and &quot;explainable&quot; are <i>also</i> buzzwords, or at least often used as such.<p>I have no objections to the example you made, some models are &quot;obviously&quot; more explainable than others. I&#x27;m simply refuting the claim that some models are <i>inherently</i> more explainable, because the entire concept starts falling apart when you take it out of its mathematical context. For example, it&#x27;s easy to see what a DT does when it&#x27;s small, but larger DTs are as &quot;unexplainable&quot; as a NN.<p>&gt; trying to enforce political control<p>I&#x27;m not against political control per se, some political control is well-justified; but I find it pretty suspicious that everyone is jumping on this train when there are more glaring issues, (for example, if and when a branch of the government or a government-controlled agency decides to use a ML model, they should be required to declare that they are using it and make the source code public, so that it can be cross-examined) and my guess is that it&#x27;s because &quot;explainability&quot; provides a nice narrative, unlike other concerns.')