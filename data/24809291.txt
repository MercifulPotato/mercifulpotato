Item(by='wokwokwok', descendants=None, kids=[24809456], score=None, time=1602935227, title=None, item_type='comment', url=None, parent=24809035, text='Have a look at the google drive files if you click on the data link; it&#x27;s a lot more than 2 photos.<p>eg. That dinosaur skeleton is derived from 60 photos. The drumkit comes from ~100.<p>...so it&#x27;s not magic, it&#x27;s very close to what you get from standard photogrammetry. The big part of this is that it isn&#x27;t representing the scene as block of voxels like some other approaches.<p>&gt; The biggest practical tradeoffs between these methods are time versus space.<p>&gt; LLFF produces a large 3D voxel grid for every input image, resulting in enormous storage requirements (over 15GB for one “Realistic Synthetic” scene).<p>&gt; Our method requires only 5 MB for the network weights (a relative compression of 3000× compared to LLFF), which is even less memory than the input images alone for a single scene from any of our datasets.<p>Anyway, so... if you could do the same sort of thing with a similar accuracy to non-images for a &#x27;neural representation database&#x27;, that&#x27;d be pretty neat.')