Item(by='bonoboTP', descendants=None, kids=[24995201], score=None, time=1604518176, title=None, item_type='comment', url=None, parent=24991357, text='Be careful, the narrative that you need tons of hardware to do anything is pushed by companies selling licenses to cloud APIs and cloud training platforms. In reality people routinely publish new results at AI conferences while only having access to a few GPUs. Not everything is about hardware. There are thousands and thousands of AI papers. Not all of them are about scaling up some model like GPT-3.<p>Yes, there is a lot of hype, a lot of misuse of ML by people who don&#x27;t understand it. Yes, in some cases it makes no sense to use a deep neural net if logistic regression (which is a single-layer NN) will work. This is generally the case when your features are already very processed, curated and meaningful as in your earthquake example. But you will never solve e.g. image or speech understanding with logistic regression.<p>Perception on high-dimensional, complex, messy real-world signals is where deep nets excel. And they have unparalleled performance there, they blow everything out of the water. It&#x27;s hard to overemphasize this. The last decade has turned many applications from does-not-work into works. A zero-one transition. Not 10% better, but it was <i>hopeless</i> and now it is &quot;quite nice&quot;. Still not divine perfection, but it&#x27;s far from just being hype.<p>It can be hard for outsiders to appreciate this, but before 2010 or so, everything in computer vision was very fragile for real-world applicability. The algorithms only worked in one specific setting, like one particular assembly line, with all parameters hand tuned in the pipeline. You needed a lot of experience and intuition to configure your feature extraction pipeline and the regression method and it was <i>still</i> very fragile. Much of research was only demonstrated on toy examples, synthetic data, or only on a few images. The shift to huge datasets is also something from the last 10-15 years. Before that people cherry-picked example, where their hand-crafted algorithm with hand-tuned parameters finally produced something resembling an okay result. I&#x27;m only exaggerating a little. These things really didn&#x27;t work. Many of the concepts were already in place, but it still didn&#x27;t work.')