Item(by='PartiallyTyped', descendants=None, kids=None, score=None, time=1612184603, title=None, item_type='comment', url=None, parent=25988083, text='Makes sense really. From ML; the test data are an unbiased estimate of the performance of a model and you can use it get PAC (probably approximately correct) estimations of your performance. There are three (4) powers at play here, the confidence [1], the error bound [2], and the number of data points (and model complexity [3] but that is not relevant when we compare a single model against test data). Your performance is exponential in the number of samples you have (meaning more data gives you exponentially better bounds and more confidence), alternatively, for a fixed sample, you can have either high confidence, but weak bound, or small bound but low confidence. When we consider the training data as an estimate of the expected performance, we also take into consideration the class complexity.<p>The moment you touch the test data you have tainted it because your brain can do a crude search on the space of solutions and reconcile them after the fact. This is why any preprocessing and cleaning is parameterized on the training set and then applied to the data, that also includes the class of models you can consider and is why you never select final models based on their performance on the test set; it becomes biased.<p>[1] probability of getting a good model.<p>[2] how much the empirical performance underestimates the expected performance.<p>[3] the number of models you can select from a particular class of models.')