Item(by='jcranmer', descendants=None, kids=[25326410], score=None, time=1607228998, title=None, item_type='comment', url=None, parent=25320675, text='&gt; Defining signed overflow requires sticking more instructions for every integer addition, since the platform is not guaranteed to actually overflow when you want it to so the program will have to detect this and force the overflow to happen. Major slowdowns.<p>That is most definitely <i>not</i> true. All modern hardware is 2&#x27;s complement arithmetic, and the corresponding signed integer arithmetic operation will silently overflow without complaint.<p>&gt; Lots of loop stuff also depends on non-overflowing signed integers. If integers can shrink on addition then you cannot safely do a lot of unrolling or fancier stuff based on affine loop transformations.<p>Not really. Only certain particular patterns force weirdness on loops were overflow well-defined, and these aren&#x27;t the most common pattern (a basic for (int i = 0; i &lt; N; i++) loop doesn&#x27;t need overflow to be undefined to have loop optimizations kick in). Even if you hit a case where it can be annoying (such as the poor man&#x27;s multidimensional array access, because C lacks such niceties), you can multiversion the loop to fall back to an unoptimized loop in cases where overflow would have kicked in and otherwise use the optimized loop.<p>Where I&#x27;ve seen the most issues with signed versus unsigned is when LLVM decides to take a loop with a signed 32-bit index, promote the index variable to 64-bit (using zero-extension for all the related values), and then retain a signed 32-bit comparison for the loop bound. The unsigned version of the code had fewer problems. So much for signed overflow being good for optimizations.')