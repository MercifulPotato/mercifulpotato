Item(by='benjismith', descendants=None, kids=None, score=None, time=1608360260, title=None, item_type='comment', url=None, parent=25473924, text='I respect your skepticism :)<p>It&#x27;s easy to imagine exceptions to the idea of a simple numerical word-scoring algorithm...<p>Of course, a word like &quot;bad&quot; might be used ironically, or in some other slang-sense, with a different literal meaning on the page...<p>But that&#x27;s totally fine. In principle, the word2vec algorithm is designed to cope with ambiguities like that.<p>When you analyze billions of words of prose, you can build a model of word-associativity that captures the superposition of all those different word-senses, and the contexts where they tend to appear on the page.<p>After a big crazy machine-learning process, each word is modeled as a vector in 300-dimensional space, with a vast network of associations and relationships between the other words in the vector-space, based on the way those words are used together in typical English grammar.<p>When we score the emotional valence of a particular word, we use a &quot;word-vector&quot; technique where those ambiguities are basically already priced into the scoring calculation. Words with a &quot;less ambiguous&quot; sentiment score (joy, paradise, ..., agony, depression) have their lack-of-ambiguity baked into the formula already.<p>Extreme scores are reserved for words with unambiguous intensity.<p>But the important thing is: we&#x27;re not really as concerned about the numerical scores of individual words as we are with the shifting balance of those sentiment scores over the course of a long document.<p>It&#x27;s not a perfect way of scoring sentiment of individual words, but it&#x27;s REALLY reliable for estimating the basic structure of a narrative.')