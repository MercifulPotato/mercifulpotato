Item(by='ravi-delia', descendants=None, kids=[25023974], score=None, time=1604821126, title=None, item_type='comment', url=None, parent=25023647, text='I was going to snarkily make a point about that figure only making sense in the context of grammatical english text, with the example of a randomly chosen 7 letter word being one of 32,909 options.<p>log(32,909) is about 15. Which is of course 1.5 bits per character, and that&#x27;s generously assuming that a person will actually randomly pick from all those potential options.  So sorry for what I almost commented, but now I have to wonder why the entropy of a word on its own is so close to the entropy of a word in a sentence.  Am I missing something? Is it just coincidence?')