Item(by='JacobiX', descendants=None, kids=None, score=None, time=1605013880, title=None, item_type='comment', url=None, parent=25044989, text='Throughout human history, you can find many discoveries that were made before understanding why they work in the first place:  we know exactly how the neural networks work, but we donâ€™t know why they are so effective, maybe because of the lack of a theoretical understanding of deep learning and complex neural networks.\nFor this particular case, BERT is the result of incremental enhancements of existing architectures and training procedures. Fundamentally, BERT is a sequence prediction algorithm, and historically, the sequence prediction models were based on complex recurrent or convolutional neural networks. Experiences showed that the best performing models were those having an attention mechanism (the concept of directing the focus on some words or sentences). Some researchers proposed a new simple network architecture, based solely on attention mechanisms, without complex recurrence or convolutions! and they showed that some of these models achieve state-of-the-art performances while being more parallelizable and requiring significantly less time to train.')