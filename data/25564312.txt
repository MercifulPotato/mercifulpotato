Item(by='krick', descendants=None, kids=[25573149], score=None, time=1609196407, title=None, item_type='comment', url=None, parent=25564010, text='This will never, ever happen. English Wikipedia (w&#x2F;o pictures) copressed is measly 20 GB. It is hard to quantify &quot;all books ever written&quot;, but I have kept copies of some online libraries large enough that they for sure have pretty much every book you can remember and 10 000 you never heard of for every single book you can remember. It&#x27;s not that much, you can fit it on 1 or 2 regular HDDs.<p>Now, I did it because I&#x27;m <i>that type of guy</i>. There&#x27;s not that many people who actually do this bullshit, even though it&#x27;s perfectly doable.<p>So why don&#x27;t they? Because it doesn&#x27;t make much sense, if you aren&#x27;t afraid of upcoming nuclear winter. Wikipedia is updated and improved every day. You only <i>sometimes</i> want to refer to something old, but you <i>nearly always</i> want to check out something new. Petabytes of video are uploaded to Youtube every year. Probably TB&#x2F;day wouldn&#x27;t be an overestimation for audio on Spotify. All data is being updated constantly.<p>Also, the above is valid for pretty aggressive data compression. Is aggressively compressed data what we want? No. 2h video compressed into about 500 MB was totally fine 15 years ago. If I download a 2h movie today, it&#x27;s normally around 20 GB. And by no means it&#x27;s uncompressed.<p>Seriously, by now you should know for a fact, that if one believes there&#x27;s such thing as &quot;too much storage space&quot; — he&#x27;s stuck in the 80s.<p>And even if there would be such thing — realistically, a cluster of nodes in Google&#x27;s datacenters can find you a book or a video you are looking for way faster than the most perfect HDD you could theoretically have locally. So, again, normal people wouldn&#x27;t want to have all this stuff even if they could.')