Item(by='leereeves', descendants=None, kids=None, score=None, time=1602070311, title=None, item_type='comment', url=None, parent=24705399, text='Powerful analogy, but analogies are dangerous. They can obscure what&#x27;s really happening.<p>In this case, by analogy, Ferrari made the comparison to the super cheap SUV from 2003. That is, OpenAI compared GPT3 to BERT on the SuperGLUE benchmark, in the paper announcing GPT3 [1].<p>They did so to demonstrate GPT3&#x27;s ability to learn a new task given only a few examples of the task (&quot;few-shot learning&quot;). The limited amount of task-specific training data was a signficant handicap that GPT3 was able to overcome, like a Ferrari towing a two ton trailer outperforming an old SUV towing nothing.<p>What this paper claims is that encoder type models can also achieve few-shot learning. The headline should be &quot;AI training method achieves few-shot learning with 99.9% fewer parameters than GPT3.&quot; That&#x27;s the innovation here, not outperforming GPT3 on a benchmark that GPT3 isn&#x27;t particularly good at.<p>1: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2005.14165.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2005.14165.pdf</a>')