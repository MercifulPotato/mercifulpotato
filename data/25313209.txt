Item(by='einarfd', descendants=None, kids=[25313239, 25313620, 25313288], score=None, time=1607162099, title=None, item_type='comment', url=None, parent=25313000, text='&gt;-- Has she calculated the oposite alternatives? People driving to libraries, to search for something?<p>Driving to the library, that is the alternative to training these models, really?<p>&gt; -- Ok, basically she is pissed as AI is picking normal language, that people use, and not using the vocabulary that is in vogue on certain political circles. Basically censure, and forced speech.<p>If you had a model trained on a large corpus of data from the pre civil war southern American states, it would have been deeply racist, and would even view black people as possible property. If you had one that was trained on data from the 1950 it would be less racist but still problematic viewed by people from today. \nIs there really something special with today, that removes these kind of concerns with a model trained with current data?<p>&gt; I think social engineering b.s. should be kept as far away as possible from science. This is turning true ai research into a masquerade to push certain political agendas.<p>It seems to me that it would be impossible to do any social science research, and specifically any research on racism. With this kind of attitude.<p>Some of the concerns brought up in the paper seems less consequential than others. Especially the pollution one seems weak to me, that doesn&#x27;t make it false, and fair enough that is was brought up. I find the racism issue a lot more problematic, and is something I&#x27;ve run up to working on deep learning solutions myself. Even if it worked fine for my group, and most of our customers, that is definitely not fun, and something practitioners should consider when building these things.')