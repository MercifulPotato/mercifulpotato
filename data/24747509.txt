Item(by='burntsushi', descendants=None, kids=None, score=None, time=1602437596, title=None, item_type='comment', url=None, parent=24746057, text='I posted this comment on lobste.rs when this came up:[1]<p>It kind of looks like Julia’s CSV parser is cheating: <a href="https:&#x2F;&#x2F;github.com&#x2F;JuliaData&#x2F;CSV.jl&#x2F;blob&#x2F;9f6ef108d195f85daa535d23d398253a7ca52e20&#x2F;src&#x2F;detection.jl#L304-L309" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;JuliaData&#x2F;CSV.jl&#x2F;blob&#x2F;9f6ef108d195f85daa5...</a><p>It’s doing parallel parsing, but I’m pretty sure their technique won’t work for all inputs. Namely, they try to hop around the CSV data and chunk it up, and then parse each chunk in a separate thread AIUI. But you can’t do this in general because of quoting. If you read the code around where I linked, you can see they try to be a bit speculative and avoid common failures (“now we read the next 5 rows and see if we get the right # of columns”), but that isn’t going to be universally correct.<p>It might be a fair trade off to make, since CSV data that fails there is probably quite rare. But either I’m misunderstanding their optimization or they aren’t being transparent about it. I don’t see this downside anywhere in the README or the benchmark article.<p>[1] - <a href="https:&#x2F;&#x2F;lobste.rs&#x2F;s&#x2F;zksa0f&#x2F;loading_csv_file_at_speed_limit_nvme#c_y2fkbj" rel="nofollow">https:&#x2F;&#x2F;lobste.rs&#x2F;s&#x2F;zksa0f&#x2F;loading_csv_file_at_speed_limit_n...</a>')