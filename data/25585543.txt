Item(by='morlockabove', descendants=None, kids=[25594318], score=None, time=1609359259, title=None, item_type='comment', url=None, parent=25585316, text='Unless the AI was formed under a selection mechanism that rewarded something like kin preference, there&#x27;s no reason to think it would <i>care</i>.  The orthogonality principle is that you could have any level of intelligence furthering any particular goal&#x2F;utility function; there isn&#x27;t some level of computational complexity where e.g. a paperclip maximiser&#x27;s utility function magically changes.<p>In the particular case of humans, our utility function(s) is(are) so complex that what we think of as our &#x27;true values&#x27; can change (because our True values are some inscrutable tug-of-war between all the parts of the brain), and also we&#x27;re hacked together by evolution, so just blindly trying to make a human smarter might change their values (or turn them mad, or cause seizures, or...).<p>But this doesn&#x27;t have to be the case in general for intelligent agents.  In principle, you can build an AI whose terminal values remain stable as it improves its intelligence.  (If this is impossible, we&#x27;re doomed.)  So unless you explicitly built an AI to care about all humans, there&#x27;s no reason to think it magically would.')