Item(by='atq2119', descendants=None, kids=[25236367, 25236149, 25236081, 25236611], score=None, time=1606548465, title=None, item_type='comment', url=None, parent=25235433, text='<i>This is what happens when a huge proportion of your die isn&#x27;t doing instruction decoding.</i><p>That&#x27;s a misconception. Yes, decoding x86 is somewhat more complicated, as far as I&#x27;m aware that&#x27;s mostly because instruction length differs at a byte granularity. Still, the area dedicated to it simply isn&#x27;t that large on those huge out of order designs.<p>I&#x27;m sure the instruction encoding plays <i>some</i> role, but I suspect what we&#x27;re seeing is rather down to consistently good micro-architecture execution over the years and Apple being ahead of everybody else on manufacturing process.<p>Compared to the x86 processors that exist today, M1 also benefits from having the memory in-package.<p><i>Intel tried to be brave with Itanium, but failed because compilers weren&#x27;t up to the job at the time.</i><p>Compilers &quot;failed&quot; because VLIW is fundamentally not a particularly useful idea. The main problem for general purpose single-thread performance is memory latency, and VLIW just doesn&#x27;t help there.')