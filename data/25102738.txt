Item(by='derefr', descendants=None, kids=None, score=None, time=1605460474, title=None, item_type='comment', url=None, parent=25097470, text='Oh, certainly, you can make causal inference under uncertainty. But then, rather than privileging a single hypothesis just because it&#x27;s the first one you intuitively generated, you should probably instead:<p>1. generate a good few hypotheses;<p>2. rank them by their seeming plausibility;<p>3. discard the ones below some cut-off of &quot;being worth the mental effort&quot;;<p>4. and then try to construct isolated contingent mental models of <i>each</i> of the worlds following from each of the hypotheses being true, making sure to store them in your mind <i>as</i> contingent models, rather than as &quot;facts-in-the-world.&quot;<p>You can reason with inferences over contingent facts, but—especially in situations where several of the hypotheses you generate are <i>equally</i> plausible—it&#x27;s very useful to sort of mentally &quot;tag&quot; those contingent-facts as contingent, so that you&#x27;ll realize when you&#x27;re using them in your reasoning; back up; and &quot;drive down <i>all</i> the roads&quot; (i.e. work out what the answer would be under <i>all</i> the contingent models you&#x27;re holding onto) instead of just one.<p>However, if after some <i>actual effort</i> there&#x27;s only one plausible hypothesis you can think of, then sure, just update on it directly. If there&#x27;s only one contingent world, keeping it tagged as &quot;contingent&quot; in your mental model isn&#x27;t doing any useful work for you. You can just learn it, and then unlearn it later if it&#x27;s not true. (And, of course, that comes up all the time in regular life. Some things really are just &quot;predictable.&quot;)<p>-----<p>In domains that are about <i>resolving</i> uncertainty — like scientific research — I would say it&#x27;s pretty unlikely that you&#x27;ll ever run into an <i>interesting</i> hypothesis (i.e. the kind you get a grant to study) that is <i>so</i> plausible that its alternatives — or even its null hypothesis! — can be entirely mentally discarded in advance of doing the experiment.<p><i>But</i>, on the other hand, this doesn&#x27;t matter so much; science is nice because it actually is quite tolerant of its participants&#x27; mental models being all over the place! &quot;Scientific rigor&quot; is externalized to the <i>scientific process</i> (enforced by peer review) — sort of like rigor in programming can be externalized to the language, and enforced by the compiler. There doesn&#x27;t need to be much of anything happening within the minds of the researchers. (Thus incrementalism, scientific positivism, etc.)<p>But this isn&#x27;t true once you leave the realm of process rigor, and enter the realm of regular people deciding what they should do when they <i>read about</i> scientific studies: how they should—or shouldn&#x27;t!—seek to apply the &quot;potential facts&quot; they hear about from these studies in their everyday lives.<p>This is especially relevant in areas where non-scientists are closely following — and attempting to operationalize — the cutting-edge of scientific research, where there is not enough aggregate evidence to <i>prove</i> much. In such domains, it&#x27;s the consumer of the science that needs good epistemic hygiene, not the scientists themselves. (Good examples of such areas: nootropics; sports nutrition; macroeconomics; and, amusingly, software engineering.)')