Item(by='sangnoir', descendants=None, kids=[25151857], score=None, time=1605797143, title=None, item_type='comment', url=None, parent=25148417, text='Tensorflow is accelerated just fine on AMD thanks to ROCm - everything you&#x27;re praising Apple for doing, AMD has done for their hardware. You&#x27;re disparaging AMD for not doing something they have actually done. There was even a tensorflow blog article announcing upstreaming AMD support, <i>in 2018</i>[0]<p>&gt; At this point, it is basically a sealed deal - if you&#x27;re even remotely dabbling in data science, you better be working on a Mac.<p>This is hilarious. You might have missed that the benchmark was missing Nvidia (or even AMD) graphics cards; I can&#x27;t think of a lower bar for comparing ML performance than against Intel GPUs - perhaps Intel CPUs? While Apple has brilliant engineering, the M1 cannot possibly outperform the obscene number of transistors Nvidia &amp; AMD throw at the task, even in older, mid-range cards. Not to mention power dissipation.<p>If you&#x27;re dabbling, you&#x27;re better off with Google&#x27;s Colab[1] which has (free) hardware acceleration which is roughly on par with my 3-year-old RX580 for my Tensorflow projects. Colab will work on anything that can run a browser.<p>0. <a href="https:&#x2F;&#x2F;blog.tensorflow.org&#x2F;2018&#x2F;08&#x2F;amd-rocm-gpu-support-for-tensorflow.html" rel="nofollow">https:&#x2F;&#x2F;blog.tensorflow.org&#x2F;2018&#x2F;08&#x2F;amd-rocm-gpu-support-for...</a><p>1. <a href="https:&#x2F;&#x2F;colab.research.google.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;colab.research.google.com&#x2F;</a>')