Item(by='lukeplato', descendants=None, kids=None, score=None, time=1609888533, title=None, item_type='comment', url=None, parent=25651367, text='Is this kind of happening with the CLIP classifier [1] to rank the generated images?<p>&gt; Similar to the rejection sampling used in VQVAE-2, we use CLIP to rerank the top 32 of 512 samples for each caption in all of the interactive visuals. This procedure can also be seen as a kind of language-guided search16, and can have a dramatic impact on sample quality.<p>&gt; CLIP pre-trains an image encoder and a text encoder to predict which images were paired with which texts in our dataset. We then use this behavior to turn CLIP into a zero-shot classifier. We convert all of a dataset’s classes into captions such as “a photo of a dog” and predict the class of the caption CLIP estimates best pairs with a given image.<p>[1] <a href="https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;clip&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;clip&#x2F;</a>')