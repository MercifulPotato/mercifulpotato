Item(by='bumby', descendants=None, kids=None, score=None, time=1609884350, title=None, item_type='comment', url=None, parent=25650206, text='This is fair. Single point of failures denote a severity (loss of mission) but risk is defined as severity x probability. To extend your example to the extreme, everything on the JWST has a failure probability &gt; 0, even the systems with redundancies.<p>What ends up happening is each failure is plotted on a severity x probability matrix with a resulting score. Depending on the requirements, the exact probability category thresholds may be defined quantitatively (e.g. 1&#x2F;10,000 chance) or they may be more qualitative (e.g., ‘likely’ vs. ‘may happen’ vs. ‘improbable’). The resulting risk has to accepted by someone (usually a program or project manager). To your point, they have obviously found each to be an acceptable (but perhaps uncomfortable) risk. Some projects will define the acceptable level of risk at the requirements stage to drive design (e.g., no single points of failure that can result in loss of crew). With all that said, it is still an abnormally high number of single point failures in a complex system and reliability is generally not linearly related to complexity (just think about what happens to cyclomatic complexity as you add more variables and the resulting number of test cases you need for full coverage)<p>In my opinion, the real dangerous part of this process is that humans are notoriously poor at thinking in probabilistic terms. Challenger, Columbia, EVA 23 etc. are all attributable, in part, to bad assessments of probability. Human biases are all but guaranteed to affect these probabilities and thus affect our understanding of the associated risk.')