Item(by='stevesimmons', descendants=None, kids=[25051929, 25052481, 25057024, 25053391, 25053963], score=None, time=1605041940, title=None, item_type='comment', url=None, parent=25051415, text='Arrow + Parquet is brilliant!<p>Right now I&#x27;m writing tools in Python (Python!) to analyse several 100TB datasets in S3. Each dataset is made up of 1000+ 6GB parquet files (tables UNLOADed from AWS Redshift db). Parquet&#x27;s columnar compression gives a 15x reduction in on-disk size. Parquet also stores chunk metadata at the end of each file, allowing reads to skip over most data that isn&#x27;t relevant.<p>And once in memory, the Arrow format gives zero-copy compatibility with Numpy and Pandas.<p>If you try this with Python, make sure you use the latest 2.0.0 version of PyArrow [1]. Two other interesting libraries for manipulating PyArrow Tables and ChunkedArrays are fletcher [2] and graphique[3].<p>[1] I use: conda install -c conda-forge pyarrow python-snappy<p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;xhochy&#x2F;fletcher" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;xhochy&#x2F;fletcher</a><p>[3] <a href="https:&#x2F;&#x2F;github.com&#x2F;coady&#x2F;graphique" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;coady&#x2F;graphique</a>')