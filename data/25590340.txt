Item(by='Smerity', descendants=None, kids=None, score=None, time=1609398562, title=None, item_type='comment', url=None, parent=25586456, text='I work in the space and was impressed with NN-512 as there&#x27;s a painful gap in inference cost between CPU and GPU that doesn&#x27;t have to exist. Intel and AMD are really missing a boat here, most other companies have enough cash they just go to GPUs, academics rarely sling low level code even in CUDA let alone AVX-512, and other than Fabrice Bellard&#x27;s work few I&#x27;ve seen few go that low level.<p>My suggestion would be to focus on an initial use case where a very limited low cost &#x2F; high efficiency CPU model can provide massive advantage. NN-512 should be the framework that expands from that Redis like core. The limited use case tactic is what I&#x27;m focusing on[1], mainly as I have a particular application and have less technical brilliance than yourself so need to focus ;)<p>An aged but still relevant example is the early word2vec work which was (and still is) frequently better to throw onto CPUs than GPUs. A well tuned implementation is not only advantageous on CPU but can win out in many scenarios where cost &#x2F; latency &#x2F; ... are important.<p>Congrats on the project though! I&#x27;d be curious for your thoughts for the future if you ever want to chat =]<p>[1]: Initial experiments written up as a tutorial with Rust and ISPC for a specific CPU based NN task - <a href="https:&#x2F;&#x2F;state.smerity.com&#x2F;smerity&#x2F;state&#x2F;01E8RNH7HRRJT2A63NSX3N6SP1" rel="nofollow">https:&#x2F;&#x2F;state.smerity.com&#x2F;smerity&#x2F;state&#x2F;01E8RNH7HRRJT2A63NSX...</a>')