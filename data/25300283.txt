Item(by='stfwn', descendants=None, kids=[25301173, 25301400, 25302408, 25301623], score=None, time=1607070984, title=None, item_type='comment', url=None, parent=25298426, text='In June I worked on a comparison of the original NeRF [1] to a state of the art proprietary photogrammetry method.<p>The photogrammetry method could process ~80GB worth of 24MP photos into a micrometer-level accurate 3D model in about 8 hours, while the fastest NeRF implementations available took the same time to train a model on just 46 pictures at 0.2MP. A funny extrapolation from a handful of datapoints was that it would have taken 1406 hours or about two months to train a NeRF at a resolution of 24MP, assuming it would converge at all. PixelNeRF improves an aspect that was already great (the number of photos required) but does not seem to tackle this complexity problem.<p>Another problem is this: the representation of the learned scene is entirely abstract, contained within the weights of the neural networks that make up the NeRF. The space itself cannot be meaningfully inspected -- it must be probed and examined by its input&#x2F;output pairs. The NeRF takes as its input a 3D location plus a viewing direction, and the output is a color radiance in that direction and a density (which depends only on the 3D location). So to generate a 2D image you emit camera rays into the NeRF from a specific hypothetical camera position, direction, focal length and sensor resolution, get the NeRF&#x27;s output for many different points along all the camera rays and compute an image based on it (volume rendering).<p>This is fine as long as the NeRF is available and there are no time constraints, but does not seem workable for real-time graphics rendering like in gaming&#x2F;VR. So the NeRF should probably be rendered into a traditional 3D model ahead of time. Afaik this is an open problem that I&#x27;ve only seen solved by using a combination of marching cubes to extract the scene geometry and then rendering colors from normal vectors. In this process, continuity, spatial density and directional color radiance, three of the most important contributions of the NeRF design, are entirely lost.<p>I would be very interested to see papers that tackle higher resolution spaces at feasible training times and faster novel view rendering times. It would be amazing to have NeRF-based graphics engines that can make up spaces out of layers of NeRFs, all probed in real-time.<p>[1]: <a href="https:&#x2F;&#x2F;www.matthewtancik.com&#x2F;nerf" rel="nofollow">https:&#x2F;&#x2F;www.matthewtancik.com&#x2F;nerf</a>')