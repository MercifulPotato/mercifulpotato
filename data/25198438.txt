Item(by='mumblemumble', descendants=None, kids=None, score=None, time=1606226405, title=None, item_type='comment', url=None, parent=25197116, text='&gt; Cynically, neural networks are easier as you don&#x27;t really have to think about your model. Give some examples with some classes and you&#x27;re done.<p>This way of thinking about it leads directly to things like statistical redlining.<p>It&#x27;s also not specific to neural networks. I take a similar approach with logistic regression. Except that I like to replace the &quot;and you&#x27;re done&quot; step with, &quot;and you&#x27;re ready to analyze the parameters to double check that the model is doing what you hope it is.&quot; Even when linear models need some help, and I need to do a little feature engineering first, I find that the feature transformations needed to get a good result are generally obvious enough if I actually understand what data I&#x27;m using. (Which, if you&#x27;re doing this at work, is a precondition of getting started, anyway. IMNSHO, doing data science in the absence of domain expertise is professional malpractice.)<p>There is no, &quot;and you&#x27;re done&quot; step, outside of Kaggle competitions or school homework. Because machine learning models in production need ongoing maintenance to ensure they&#x27;re still doing what you think they&#x27;re doing. See, for example, <a href="https:&#x2F;&#x2F;research.google&#x2F;pubs&#x2F;pub43146&#x2F;" rel="nofollow">https:&#x2F;&#x2F;research.google&#x2F;pubs&#x2F;pub43146&#x2F;</a>')