Item(by='rsync', descendants=None, kids=[25360952, 25362242], score=None, time=1607532129, title=None, item_type='comment', url=None, parent=25360164, text='Some background ...<p>We have a fairly robust drive-proofing procedure prior to deployment - I think we still use &#x27;badblocks&#x27; and really beat them up for 5-7 days.  It&#x27;s not enough to merely not fail - we have to see perfect returns on the SMART diagnostics and zero complaints from FreeBSD.<p>So we&#x27;re starting with a known-good population of drives.<p>Then we monitor them very closely - again with both SMART and FreeBSD&#x2F;ZFS system data and we are fairly aggressive about failing them out early if we see them start to misbehave.<p>So we don&#x27;t see things like straight up drive failures - the bad ones have already been rejected and drives that misbehave are culled out.<p>The last time I got pulled in to make a judgement call (and got a little nervous) was (IIRC) we had a 15 drive vdev that had a drive failure <i>and</i> a candidate for early removal that had started to misbehave and the choice was made to <i>yank them both</i> because why do two big long resilvers ... and then during the subsequent resilver a <i>third</i> drive, while not failing, started to spit out errors.  So the resilver completed and all was well but we had a potential third failure that could have died in the resilver and then we would be running with no protection.<p>But that&#x27;s why I love raidz3 - even if that had happened, we would have needed to lose <i>yet another drive</i> to lose the vdev (and the entire pool).<p>I think the actionable recommendations here are to burn in your drives when you get them because we do, indeed, find rejects in most batches.  Also, pick an error threshold that is low and be disciplined about sticking to it - don&#x27;t let drives spin out SMART errors sporadically for months ...<p>EDIT: Here is another thought and this goes back to pre-ZFS days and old-style RAID, etc.  In normal operations we&#x27;re aggressively failing out drives that misbehave BUT if you&#x27;re in a marginal situation you need to flip that logic around - especially if the pool is in-use during resilver&#x2F;rebuild.<p>If you&#x27;ve lost 2 drives in a raidz3 (or, say, 1 drive in a RAID6) that remaining protection drive <i>even if it is failing</i> is like gold.  If you fail it out, it&#x27;s gone forever and has no relevance to the pool&#x2F;array - but if you keep it, even as it&#x27;s dying, you can either limp along OR you can even offline the pool&#x2F;array and send that last drive to recovery or clone it or whatever ... the point is, when an array goes sideways <i>every single bit of parity, no matter how poorly behaved, should be treated like gold</i>.')