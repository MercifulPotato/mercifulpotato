Item(by='shoyer', descendants=None, kids=[25887438, 25887077], score=None, time=1611443556, title=None, item_type='comment', url=None, parent=25885524, text='As someone who builds neural networks routinely, this sort of non-reproducibility sounds troubling to me. We expect small differences for floating point arithmetic between platforms, but integer math is typically exact.<p>This is all the more concerning for 8-bit quantized arithmetic, where off-by-one means a relative error of about half a percent. If a <i>individual layers</i> in a quantized neural net have off-by-one errors with a consistent bias, I can imagine these errors accumulating into significant losses in model quality in deep networks. There isn&#x27;t a huge margin for error in quantized neural nets.<p>One concern about the article: it uses the word &quot;non-deterministic&quot; in a slightly misleading way. I assume any specific hardware is still expected to produce <i>consistent</i> results when run twice on the same input. So it&#x27;s more non-reproducible than non-deterministic. Compensating for inconsistent arithmetic on different devices sounds much more feasible than compensating for stochastic arithmetic.')