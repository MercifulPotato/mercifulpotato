Item(by='AndrewKemendo', descendants=None, kids=[25777987, 25779627], score=None, time=1610638723, title=None, item_type='comment', url=None, parent=25776724, text='&gt;We&#x27;ve been in an exciting deep learning craze for a while, but it&#x27;s silly to expect it to last forever. Back to the grind now.<p>This was effectively my response to hardmaru when this topic came up on reddit [1]<p>Basically 2010-2018 was an open field for ML&#x2F;DL research with old(ish) methods being rapidly applied to low hanging fruit and large datasets with newly cheap compute.<p>Deepmind and others are actually making new methods but by and large are remixes of those same old approaches.<p>The majority of different research out there trying other approaches (Numenta, OpenCog, Causal Calculus, anything Schmidhuber etc...) don&#x27;t really get any love because it doesn&#x27;t fit within the mass tensorflow&#x2F;torch framework.<p>[1]<a href="https:&#x2F;&#x2F;twitter.com&#x2F;AndrewKemendo&#x2F;status&#x2F;1349387455552745473" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;AndrewKemendo&#x2F;status&#x2F;1349387455552745473</a>')