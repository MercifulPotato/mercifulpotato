Item(by='gfodor', descendants=None, kids=None, score=None, time=1604167046, title=None, item_type='comment', url=None, parent=24951199, text='The ethics of this are heavily influenced by how much you believe a) this rollout will result in a net decrease in automobile accidents and b) how much we should assume drivers will both be informed and intervene if the system missteps. If you believe both are unlikely, or you believe engineers should have a &quot;do no harm&quot; based ethics similar to medicine, this would be unacceptable. But if you believe both are highly and&#x2F;or think engineering ethics should focus on harm minimization, this makes sense to do and under certain assumptions it becomes unethical to <i>not</i> roll it out.<p>As a concrete example: if you assume that 99% of accidents that would be caused by the system will be prevented by driver interventions, and you think overall the system will reduce the likelihood of an accident by 20%, the question you have to ask yourself is: if 1 in 5 people harmed over the next 6 months we could have saved, should we avoid doing so and wait until only 1 in 200 will not be harmed, in part due to their own negligence? As you can see, the assumed probabilities matter a <i>lot</i>, so I&#x27;d be curious how one can come up with good projections. The only case where these probabilities don&#x27;t matter is if you believe that engineers ought to never create harm where none would have existed otherwise, but when it comes to self driving cars, this is an impossibility since it assumes perfect autonomy.<p>In general, if you buy into the theory that &quot;human + AI&quot; is always going to be smarter than just AI, then arguably even in a world with great autonomous driving, the best system will be one that has <i>both</i> an AI and a human. And that is what we have here, so it&#x27;s more a question of if AI quality is sufficient or if this is just a bad idea in general given the natural tendencies of humans to be poor co-pilots.')