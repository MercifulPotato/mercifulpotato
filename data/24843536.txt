Item(by='jpsamaroo', descendants=None, kids=None, score=None, time=1603238044, title=None, item_type='comment', url=None, parent=24842530, text='&gt; This is a complete novice, ill informed, question. So forgive it in advanced, but why have an AMD specific backend at all? Couldn&#x27;t you just use AMD&#x27;s HIP&#x2F;HIP-IFY tool on the CUDA backend and get an AMD friendly version out?<p>HIP and HIPify only work on C++ source code, via a Perl script. Since we start with plain Julia code, and we already have LLVM integrated into Julia&#x27;s compiler, it&#x27;s easiest to just change the LLVM &quot;target&quot; from Native to AMDGPU (or NVPTX in CUDA.jl&#x27;s case) to get native machine code, while preserving Julia&#x27;s semantics for the most part.<p>Also, interfacing to ROCR (AMD&#x27;s implementation of the Heterogeneous System Architecture or HSA runtime) was really easy when I first started on this, and codegen through Julia&#x27;s compiler and LLVM is trivial when you have CUDAnative.jl (CUDA.jl&#x27;s predecessor) to look at :)<p>I should also mention that not everything that CUDA does maps well to AMD GPU; CUDA&#x27;s streams are generally in-order (blocking), whereas AMD&#x27;s queues are non-blocking unless barriers are scheduled. Also, things like hostcall (calling a CPU function from the GPU) doesn&#x27;t have an obvious alternative with CUDA.')