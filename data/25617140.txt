Item(by='cmurf', descendants=None, kids=[25617567], score=None, time=1609626209, title=None, item_type='comment', url=None, parent=25614159, text='There&#x27;s no enough information to know if all the reported problems are the result of the same defect. But in:\n<a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;WSL&#x2F;issues&#x2F;5895" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;WSL&#x2F;issues&#x2F;5895</a><p>The first instance of a problem is:<p><pre><code>    [    1.956835] JBD2: Invalid checksum recovering block 97441 in log\n</code></pre>\nAnd that&#x27;s corruption that leads to log replay failing, i.e. rejecting it because honoring the replay in the face of checksum errors could make things much worse. Subsequently mount fails:<p><pre><code>    [   21.151232] ERROR: MountExt4:1659: mount(&#x2F;dev&#x2F;sdb) failed 5\n</code></pre>\nThat&#x27;s good because the purpose of journal replay is to make the file system consistent following a crash&#x2F;power fail. And if the file system is dirty, replay is called for, but can&#x27;t happen due to a corrupt journal, so now an fsck is required. i.e. it is in an inconsistent (you could say partly broken) state and needs repair.<p>I haven&#x27;t seen syslog&#x2F;systemd journal for other cases to know if there&#x27;s instances of ext4 log replay that succeeds, but with missing files. That&#x27;s not file system corruption, even if it leads to an inconsistent state in a git repository (or even a database). But this is still concerning, because to get a situation where log replay is clean but files are missing suggests an entire transaction was just dropped. It never made it to stable media, and even the metadata was not partially written to the ext4 journal.<p>qemu-kvm has a (host) cache setting called &quot;unsafe&quot;. Default is typically &quot;none&quot; or &quot;write back&quot;. The unsafe mode can result in file system corruption if the host crashes or has a power failure. The guest&#x27;s IO is faster with this mode, but the write ordering expected by the file system is not guaranteed if the host crashes. i.e. writes can hit stable media out of order. If the guest crashes, my experience has been that things are fine - subsequent log replay (in the guest) is successful, because the guest writes that made it to the host cache do make it to stable media by the same the guest reboots. The out of order writes don&#x27;t matter... unless the host crashes, and then it&#x27;s a big problem. The other qemu cache modes have rather different flush&#x2F;fua policies that can still keep a guest file system consistent following a host crash. But they are slower.<p>So it makes me suspicious that for performance reasons, WSL2 might be using a possibly volatile host side caching policy. Merely for additional data point, it might be interesting to try to reproduce this problem using e.g. Btrfs for the guest file system. If write order is honored and flushed to stable media appropriate for default out of the box configuration of a VM, I&#x27;d expect Btrfs never complains, but might drop up to 30s of writes. But if there&#x27;s out of order writes making it to stable media, Btrfs will also complain, I&#x27;d expect transid errors which are also a hallmark of drive firmware not consistently honoring flush&#x2F;fua and then you get a badly timed crash. (And similar for ZFS for that matter - nothing is impervious to having its write order expectations blown up.)')