Item(by='jononor', descendants=None, kids=None, score=None, time=1608390040, title=None, item_type='comment', url=None, parent=25477364, text='assert test_performance &gt; threshold ? This would only be a sanity check for big deviations though. For more subtle changes one may need to do a proper statistical test. I believe the jury is still out of how to do that properly for deep ML models - challenge is the lack of independence in CV folds and generally the compute time it takes to evaluate.<p>However, I would probably not do performance check inside a unit-testing framework. Instead treat this as quality indicators like performance benchmarks, code coverage etc. It may be a &quot;gate&quot;, that needs to pass to allow a new model into production.<p>To evaluate performance over time, one would preferably want labeled datasets for test gathered at different points in time. Which requires a (reliable) continuous labeling process.\nOne can also gather customer feedback about performance, track those as metrics.\nThese things are probably more in the &quot;monitoring&quot; part of a system, rather than unit-testing time though.')