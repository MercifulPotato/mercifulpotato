Item(by='talolard', descendants=None, kids=[25328709, 25327602], score=None, time=1607292157, title=None, item_type='comment', url=None, parent=25325084, text='50&#x2F;50 I can shed some light or am way over my head:<p>The point of the paper is that if you train a deep learning model with gradient descent then the resulting model is effectively a kernel machine, regardless of model architecture.<p>The nice thing about a kernel machine is that it is simple (just one hidden layer) and we are able to use to analyze a kernel machine more effectively and conveniently.<p>So, I think the contribution here isn&#x27;t &quot;these sets of universal aproximators are equivalent&quot;  but rather &quot;We have this effective but opauge deep learning thing, turns it it&#x27;s actual a kernel machine in retropsect so we can bring &#x27;kernel tooling&#x27; to analyze the deep learning mode&quot;')