Item(by='pmiller2', descendants=None, kids=[24866758], score=None, time=1603425739, title=None, item_type='comment', url=None, parent=24865473, text='If A is a square matrix, then a left eigenvector v is a vector such that vA = \\lambda_v v for some \\lambda_v.  Likewise, if u is a right eigenvector of A, Au = \\lambda_u u.<p>Notice that u and v cannot be equal, because they are not the same shape.  However, if v is a right eigenvector with eigenvalue \\lambda, then v^T is a left eigenvector with eigenvalue \\lambda, as well.  More or less what this means is that we tend to just ignore left eigenvalues and only use the right eigenvalues, because the math is exactly the same up to a transpose operation.<p>A matrix does not necessarily have nontrivial eigenvectors.  Think about the 0 matrix here.  But, if A is nonzero, then it must have at least one nonzero eigenvalue, hence one nontrivial eigenvector.  This is because a nonzero matrix must have at least one nonzero row and column; however, if you construct the other rows (columns) to be multiples of the first column, you end up with a matrix with only one nontrivial eigenvalue.  This also illustrates the conditions necessary for an nxn matrix A to have n linearly independent eigenvectors: the rows of A must be linearly independent.<p>All of this is covered in a decent undergrad linear algebra course.  I would suggest either finding a video course, or getting a good book and working through it, if you want to understand these things better.')