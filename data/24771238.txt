Item(by='Phemist', descendants=None, kids=None, score=None, time=1602629612, title=None, item_type='comment', url=None, parent=24765440, text='Interesting! Thanks for the write-up. In the field of Biometrics we often use the library BOB to compute scores like AUC&#x27;s and EER&#x27;s, or other measures over stuff like DET curves.<p>For some practical work I was doing, BOB (and SK learn) proved too slow for my liking.\nI am currently unable to provide much more detail, but I used a similar insight to yours (mine was the amortization of sorting costs to have a better average time complexity) in a library I built for calculating empirical, bootstrapped, confidence intervals for EER&#x27;s.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;feerci&#x2F;feerci" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;feerci&#x2F;feerci</a><p>For massive amounts of decision scores, like millions, this method can give you confidence intervals on EER&#x27;s where no other library currently in use can. Unfortunately, I have not yet found a theoretical basis for this method, and suspect it might actually break down under some trivial smaller cases. Also, DET curves and their derived measures remain a tough cookie to crack.<p>Considering AUC&#x27;s and EER&#x27;s are somewhat tightly related, I&#x27;m thinking this method could also be applied to AUC&#x27;s.')