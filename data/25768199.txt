Item(by='mattalex', descendants=None, kids=None, score=None, time=1610573326, title=None, item_type='comment', url=None, parent=25762968, text='It&#x27;s also different because Stockfish uses Alpha-beta treesearch instead of MCTS:  \nMCTS tries to deal with an explosion in search-space by only sampling very small parts of the searchspace and relying on a very good heuristic to guid that search process. In this case it is crucial to find the most relevant subset of the tree to explore it, so spending more time on your policy makes sense.<p>Alpha-beta pruning however always explores the entire searchtree systematically (up to a certain depth using itd) and prunes the searchtree by discarding bad moves. In this case you don&#x27;t need as good of an evaluation function because you search the entire tree anyways. Rather you need the function to be fast, as it is evaluated on many more states.<p>In general AB-pruning only needs the heuristic for estimating the tail-end of the tree and for sorting the states based on usefulness, while MCTS uses all the above plus guiding the whole search process.   \nSpending tons of time on the heuristic is not useful as even the optimal search order can only double your searchdepth. Don&#x27;t get me wrong that still a lot (especially considering exponential blowup) but MCTS can surpass this depth easily. The disadvantage is that MCTS loses a lot of the guarantees of AB-pruning and tends to &quot;play down to his opponent&quot; when trained using self-play because the exploration order is entirely determined by the policy.')