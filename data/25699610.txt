Item(by='KMag', descendants=None, kids=[25700073], score=None, time=1610192920, title=None, item_type='comment', url=None, parent=25697289, text='Yes, the DEC Alpha AXP was a beast of a chip family.  The Alpha design team made nearly as few guarantees as possible in order to leave nearly as much room for optimization as possible.  The Alpha&#x27;s lax memory model provided the least-common denominator upon which the Java memory model is based.  A stronger Java memory model would have forced a JVM on the Alpha to use a lot more atomic operations.<p>All processors (or at least all processors I&#x27;m aware of) will make a core observe its own writes in the order they appear in the machine code.  That is, a core by itself will be unable to determine if the processor performs out-of-order memory operations.  If the machine code says Core A makes writes A0 and then A1, it will always appear to Core A that A0 happened before A1.  As far as I know, all processors also ensure that all processors will agree to a single globally consistent observed ordering of all atomic reads and atomic writes.  (I can&#x27;t imagine what atomic reads and writes would even mean if they didn&#x27;t at least mean this.)<p>On top of the basic minimum guarantees, x86 and x86_64 (as well as some SPARC implementations, etc.) have a Total Store Ordering memory model: if Core A makes write A0 followed by A1, and Core B makes write B0 followed by B1, the two cores may disagree about whether A0 or B0 happened first, but whey will always agree that A0 happened before A1 and B0 before B1, even if none of the writes are atomic.<p>In a more relaxed memory model like the SPARC specification or Aarch64 specification (and I think RISC-V), if the machine code says Core A makes write A0 before A1, Core B might see A1, but not yet see A0, unless A0 was an atomic write.  If Core B can see a given atomic write from Core A, it&#x27;s also guaranteed that Core B can see all writes (atomic and non-atomic) that Core A thinks it made before that atomic write.<p>With the DEC Alpha, the hardware designers left themselves almost the maximum amount of flexibility that made any semantic sense: if Core B makes an atomic read, then that read (and any reads coming after it in machine code order) is guaranteed to see the latest atomic write from Core A, and all writes that came before that atomic write in machine code order.  On the Alpha, you can think of it as all of the cores having unordered output buffers and unordered input buffers, where atomic writes flush the core&#x27;s output buffer and atomic reads flush the input buffer.  All other guarantees are off.  (Note that even under this very lax memory model, as long as a mutex acquisition involves an atomic read and an atomic write, and a mutex release involves an atomic write, you&#x27;ll still get correct behavior if you protect all reads and writes of shared mutable state with mutexes.  A reader&#x27;s atomic read in mutex acquisition guarantees that all reads while holding the mutex will see all writes made before another thread released the mutex.)  This might be slightly wrong, but it&#x27;s roughly what I remember of the Alpha memory model.<p>The thing that confused some programmers with the Alpha is that with most memory models, if one thread makes a ton of atomic writes, and another thread makes a ton of non-atomic reads, the reading thread will still never see the writes in a different order than what the writer thought it wrote.  There&#x27;s no such guarantee on Alpha.<p>On a side note, the Alpha team was also pretty brutal about only allowing instructions that were easy for compilers to generate and showed a performance improvement in simulations on some meaningful benchmark.  The first generation of the Alphas didn&#x27;t even have single-byte loads or stores and relied on compilers to perform single-byte operations by bit manipulation on 32-bit and 64-bit loads and stores.<p>Many of the Alpha design people went on to the AMD K6 III (first AMD chips to give Intel a run for their money in the desktop gaming market), the PASemi PWRFicient (acqui-hired by Apple to start their A-series &#x2F; Apple Silicon team), AMD Ryzen, etc.)<p>When I bought my first computer in the fall of 1997, the fastest Intel desktop processors were 300 MHz PIIs.  DEC Alphas at the time were running at 500 MHz, and had more instructions per clock, particularly in the floating point unit.  The Cray T3E supercomputer used DEC Alphas for good reason.')