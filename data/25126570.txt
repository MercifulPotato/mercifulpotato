Item(by='ithkuil', descendants=None, kids=None, score=None, time=1605634145, title=None, item_type='comment', url=None, parent=25098661, text='I&#x27;ll try. On pipelining:<p>Most operation above a certain level of complexity can be broken down in individual smaller steps that\nhave to happen in order and each step takes some time. Once one such step is done, the next step is performed etc.<p>If the logic for each of those steps is implemented in hardware, once a step is done executing, the logic just sits there idle, waiting for the next operation (composed of smaller steps) to be executed.<p>Pipelining is a technique that allows to make use of that otherwise idle logic to start performing the first step of the next operation.<p>For example, let&#x27;s consider this small program:<p><pre><code>    r3 = r1 + r2\n    r2 = r3 + r1\n    r1 = r3 + r4\n</code></pre>\nA hypothetical CPU would execute this instruction in 12 clock cycles:<p><pre><code>    load instruction 1\n    decode instruction 1\n    perform r1 + r2\n    write result in r3\n\n    load instruction 1\n    decode instruction 2\n    perform r3 + r1\n    write result in r2\n\n    load instruction 1\n    decode instruction 3\n    perform r3 + r4\n    write result in r1\n\n</code></pre>\nA pipelined execution looks like this:<p><pre><code>    load instruction 1 \n    decode instruction 1 | load instruction 1  \n    perform r1 + r2      | decode instruction 2 | load instruction 3\n    write result in r3   | perform r3 + r1      | decode instruction 3\n                           write result in r2   | perform r3 + r4\n                                                  write result in r1\n</code></pre>\nand completes in 6 cycles. Once the pipeline is running at full capacity, it produces 1 result\nper clock cycle, achieving a 4x speedup over the naive approach above.<p>This example architecture has a pipeline depth of 4.<p>The execution is still strictly in order. The idea is that you can start doing some work for the next\ninstruction before the previous one is fully completed.<p>For example, you can decode the next instruction after you decoded the current one.\nAlso, you can perform a computation on the next instruction as soon as you have computed the results\nof the previous one, even before you actually have written the results in the actual destination register,\nprovided there is additional logic that ships the result of the current operation as an operand of the arithmetic unit for the next cycle.<p>The maximum duration of each step is bound by the clock period. The faster the clock, the less time\nyou have for a single step. The deeper the pipeline, the faster the clock can tick and still produce\none operation per clock tick. We&#x27;ll see next some of the many things that prevents us from just riding \nthis idea to the extreme and deepening the pipeline to ludicrous amounts and harness ludicrous clock speeds.<p>You may have noticed that this dummy architecture here has been carefully designed so that the next instruction can consume\nthe result of the previous instruction if the instruction stream so requires.<p>Imagine a different architecture that is divided in 6 steps, where the + operation itself is pipelined in two steps.<p><pre><code>    load instruction 1\n    decode instruction 1\n    access r1, r2\n    start +\n    continue +\n    write result in r3\n\n    load instruction 2\n    decode instruction 2\n    access r3, r1\n    start +\n    continue +\n    write result in r2\n\n    load instruction 3\n    decode instruction 3\n    access r3, r4\n    start +\n    continue +\n    write result in r1\n\n\n    load instruction 1\n    decode instruction 1 | load instruction 1 \n    access r1, r2        | decode instruction 2 | load instruction 3\n    start +              | access r3, r1        | decode instruction 3\n    continue +           | start +  (HAZARD!)   | access r3, r4\n    write result in r3   | continue +           | start +\n                           write result in r2   | continue +\n                                                  write result in r1\n</code></pre>\nNow, here the second instructions depends on r3 which is has not yet finished computing!\nThis is known as a data hazard. A common way out is to introduce a pipeline &quot;stall&quot;, i.e. \na no-operation step is injected in the pipeline to resolve the hazard.<p><pre><code>    decode instruction 1\n    access r1, r2        | decode instruction 2\n    start +              | access r3, r1        | decode instruction 3\n    continue +           | nop                  | nop\n    write result in r3   | start +              | access r3, r4\n                           continue +           | start +\n                           write result in r2   | continue +\n                                                  write result in r1\n</code></pre>\nThis short stall is not a full pipeline flush.\nOk, so we saw a data hazard, but so far all this has been pretty straightforward.<p>So far we were looking at a linear instruction stream. Let&#x27;s see what happens when you have a branch:<p><pre><code>    r3 = r1 + r2\n    call func1\n    r1 = r3 + r4\n</code></pre>\nwhere func1 is:<p><pre><code>    r2 = r3 + r1\n    return\n</code></pre>\nLet&#x27;s see what our dummy 4-stage pipeline machine would do:<p><pre><code>    load instruction 1                   \n    decode instruction 1 | load instruction 2  \n    perform r1 + r2      | decode instruction 2    | load instruction 3\n    write result in r3   | perform pc + 1          | decode instruction 3 (HAZARD!!)\n                         | write pc=func1, ra=pc+1 | perform r3 + r4      (^^^^^^^^)\n                                                   | write result in r1   (^^^^^^^^)\n\n</code></pre>\ncalling a function basically means setting the program counter (aka instruction pointer) register\nto point to a different location instead of the next instruction. In order to return from the function\nyou also need to save the return address somewhere (some CPUs use a &quot;link&quot; register, some use the stack,\nhere I used a return address &quot;ra&quot; register); the return address is the address of the next instruction\nin the instruction stream before jumping to the function body.<p>As you can see, once we set the program counter and tell the CPU that the program flow continues from\nanother place, the work the CPU has started doing in the pipeline becomes invalid.<p><pre><code>    load instruction 1\n    decode instruction 1 | load instruction 2\n    perform r1 + r2      | decode instruction 2    | load instruction 3\n    write result in r3   | perform pc+1, &amp;func1    | nop\n                         | write pc=func1, ra=pc+1 | load ins func1.1\n                                                   | decode ins func1.1 | load ins func1.2\n                                                   | perform r3 + r1    | decode ins func1.2\n                                                   | write result in r2 | read ra\n                                                                        | write pc=ra\n                                                  \n</code></pre>\nJumping to a different address thus incurs in additional delay,\nbecause we don&#x27;t know yet where the jump is taking us until we have decoded the instruction\nand performed the necessary control flow adjustments.<p>In our dummy example this adds 2 cycle latency. Not all is lost\nsince we can still do a little bit of pipelining, but for all intents and purposes this is a pipeline flush.<p>Some architectures (especially the older RISCs but it&#x27;s still quite common in DSPs) work around this problem by introducing\none or more &quot;branch delay slots&quot;; these are instructions that appear physically after a call&#x2F;jump&#x2F;branch instruction\nbut get executed before the branch is actually taken.<p>A equivalent program for such an architecture would look like:<p><pre><code>    call func1\n    r3 = r1 + r2\n    r1 = r3 + r4\n</code></pre>\nwhere func1 is:<p><pre><code>    return\n    r2 = r3 + r1\n</code></pre>\nAnd be executed as<p><pre><code>    load instruction 1\n    decode instruction 1    | load instruction 2 \n    perform pc+1, &amp;func     | decode instruction 2 | load ins func1.1   |\n    write pc=func1, ra=pc+1 | perform r1 + r2      | decode ins func1.1 | load ins func1.2\n                              write result in r3   | read ra            | decode ins func1.2\n                                                   | write pc=ra        | perform r3 + r1\n                                                                        | write r2\n</code></pre>\nBy avoiding the pipeline flush we can now complete the same task in 7 cycles compared to the 9 cycles shown above.\nBranch delay slots are no panacea though and modern CPU have largely moved away from them, favouring other techniques\nto predict whether and where a branch is likely to happen. I choose to mention branch delay slots in order to illustrate that pipeline\nstalls are not inherently necessary whenever branches are involved.<p>Stalls are ultimately caused by data dependencies. The program counter is just data, on which the instruction stream itself depends on.<p>(In the real world obviously things get much more complicated but you get the idea)')