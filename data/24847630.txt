Item(by='MrPowers', descendants=None, kids=[24848241], score=None, time=1603286742, title=None, item_type='comment', url=None, parent=24845332, text='Here&#x27;s the Scala Spark style guide: <a href="https:&#x2F;&#x2F;github.com&#x2F;MrPowers&#x2F;spark-style-guide" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;MrPowers&#x2F;spark-style-guide</a><p>The chispa README also provides a lot of useful info on how to properly write PySpark code: <a href="https:&#x2F;&#x2F;github.com&#x2F;MrPowers&#x2F;chispa" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;MrPowers&#x2F;chispa</a><p>Scala is easier than Python for Spark because it allows functions with multiple argument lists and isn&#x27;t whitespace sensitive.  Both are great &amp; Spark is a lot of fun.<p>Some specific notes:<p>&gt; Doing a select at the beginning of a PySpark transform, or before returning, is considered good practice<p>Manual selects ensure column pruning is performed (column pruning only works for columnar file formats like Parquet).  Spark does this automatically and always manually selecting may not be practical.  Explicitly pruning columns is required for Pandas and Dask.<p>&gt; Be careful with joins! If you perform a left join, and the right side has multiple matches for a key, that row will be duplicated as many times as there are matches<p>When performing joins, the first thing to think about is if a broadcast join is possible.  Joins on clusters are hard.  Then it&#x27;s good to think about using a data stores that allows for predicate pushdown aggregations.')