Item(by='einarfd', descendants=None, kids=None, score=None, time=1607169604, title=None, item_type='comment', url=None, parent=25313453, text='&gt; Either your goal is to model reality and real life, or your goal is to model your idealization of what you feel real life should be.<p>&gt; If you pick option #2 then your model does not reflect real life.<p>These deep learning models built by corporations are not scientific models, they are engineering solutions, built to solve problems. Reflecting the real world is only useful if it furthers what the company wants to solve. If they for example remove swear words from their training set, that will make them a less accurate model of the world, but make them more useful for building solutions. But it is probably a trade of they would be happy with. We&#x27;ve also seen example of risk scoring application for felons that seem to end up doing racial profiling, because that is what the data seem to indicate makes sense. But that&#x27;s deeply problematic and runs counter to laws in some places and seem ethically problematic (<a href="https:&#x2F;&#x2F;www.theverge.com&#x2F;2020&#x2F;6&#x2F;24&#x2F;21301465&#x2F;ai-machine-learning-racist-crime-prediction-coalition-critical-technology-springer-study" rel="nofollow">https:&#x2F;&#x2F;www.theverge.com&#x2F;2020&#x2F;6&#x2F;24&#x2F;21301465&#x2F;ai-machine-learn...</a>).<p>&gt; Bias is by definition the way the model returns results that don&#x27;t match the real world and real life, in frequency and in proportion.<p>Getting a good data set without bias is hard, even if you crawl the whole internet like Google does. Not everything is on the internet and there are systematic drivers that make some part of the human condition over represented (English, science, the views of the affluent and educated), and some under presented (small languages, the discourse of people behind the Chinese great firewall, the poor). So just getting a ginormous data set does not fix bias.<p>&gt; If your goal is to use your model to manipulate and control society based on your own personal criteria, by manipulating it to return results that distort the real world and real life, then call it something else, because bias is not it.<p>Positive bias is absolutely something that we use, and while it might seem sinister it does not have to be. The example I&#x27;m most familiar with a facial recognition technology. Most groups building that ends up with a model that is better at some groups than others. Asian research groups often end up with models that does well with asians and worse with whites, while European groups  usually end up with the reverse. In some sense these results do reflect the reality of these groups, most people in Europe is white and most people Asian is asians, so that you training sets ends up like that is not surprising. But no one is happy with these kind of results, and everyone wants to fix that.<p>To bring it back to speech and text models, let&#x27;s say you are building a customer service solutions incorporating a deep learning model, the reality might be that you current customer service representatives treat blacks (or people who use &quot;black&quot; dialects), worse than people who sound white. An accurate model built on this data set will then also do that. But is that acceptable? I hope most companies would want to fix that, and be fine with adding some positive bias in their solution.')