Item(by='golergka', descendants=None, kids=[25551725, 25552288], score=None, time=1609082405, title=None, item_type='comment', url=None, parent=25542011, text='I only took a beginner course in ML over 5 years ago, so this probably is a stupid question, but does this mean, the trained GPT-2 model encodes the source text into it&#x27;s parameters somehow? Is this resilient â€” will it still remember it if we randomize the weights just a little tiny bit? Will it remember it if we clear a small portion of parameters?<p>Does the human memory work the same way?')