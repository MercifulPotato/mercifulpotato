Item(by='qayxc', descendants=None, kids=None, score=None, time=1605047943, title=None, item_type='comment', url=None, parent=25051741, text='That depends entirely on the hardware of both the ML accelerator and the GPU in question, as well as model architecture, -data and -size.<p>Unfortunately Apple was very vague when they described the method that yielded the claimed &quot;9x faster ML&quot; performance.<p>They compared the results using an &quot;Action Classification Model&quot; (size? data types? dataset- and batch size?) between an 8-core i7 and their M1 SoC. It isn&#x27;t clear whether they&#x27;re referring to training or inference and if it took place on the CPU or the SoC&#x27;s iGPU and no GPU was mentioned anywhere either.<p>So until an independent 3rd party review is available, your question cannot be answered. 9x with dedicated hardware over a thermally- and power constrained CPU is no surprise, though.<p>Even the notoriously weak previous generation Intel SoCs could deliver up to 7.73x improvement when using the iGPU [1] with certain models. As you can see in the source, some models don&#x27;t even benefit from GPU acceleration (at least as far as Intel&#x27;s previous gen SoCs are concerned).<p>In the end, Apple&#x27;s hardware isn&#x27;t magic (even if they will say otherwise;) and more power will translate into higher performance so their SoC will be inferior to high-power GPUs running compute shaders.<p>[1] <a href="https:&#x2F;&#x2F;software.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;develop&#x2F;articles&#x2F;accelerate-deep-learning-inference-with-integrated-intel-processor-graphics-rev-2-0.html" rel="nofollow">https:&#x2F;&#x2F;software.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;develop&#x2F;article...</a>')