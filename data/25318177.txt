Item(by='6d65', descendants=None, kids=None, score=None, time=1607201239, title=None, item_type='comment', url=None, parent=25314066, text='One of the few things I remember from watching fastai course a few years ago, is to train a model with smaller data. For example, instead of 2k×2k images, downscale them to for ex 400×400. A network with a better design should still learn fastest on smaller data.<p>Also, one can train the network to a good accuracy, then change the input layer, and unfreeze the inner layers, that way the network will have a head start.<p>Not sure how universal this principle is, but it seemed reasonable, if I remember it correctly, of course.<p>The approach described in the article looks very smart. Also could be handy for integration testing of ML frameworks. I&#x27;ve been working on my own DL framework, and this data set looks like a good way to test the training and inference pipelines E2E.')