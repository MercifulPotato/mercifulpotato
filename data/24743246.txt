Item(by='maxov', descendants=None, kids=None, score=None, time=1602380767, title=None, item_type='comment', url=None, parent=24742854, text='This answer is probably biased by what I’ve needed to use recently, but I think spectral methods are the most generally useful. E.g. spectra of symmetric matrices, SVD, Courant-Fischer theorem. You may not need this in all of, or even most of, practical ML, but knowing these things are prerequisites in my mind for understanding PCA, CCA, LDA&#x2F;QDA, multivariate Gaussians (which are foundational in probabilistic interpretations of ML), and covariance. A good understanding of inner products also conditions you to understanding kernels better.<p>You need something a bit different for SVMs. The linear algebra there is basically the geometry of planes and half spaces. Also for optimization you need some different things, but those are typically taught under the moniker of convex analysis, not linear algebra.<p>In specific, my approach to many multivariate estimation problems starts with “take the SVD, are there any properties you can use afterwards?”')