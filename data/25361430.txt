Item(by='ghshephard', descendants=None, kids=[25361528], score=None, time=1607536600, title=None, item_type='comment', url=None, parent=25360747, text='I mostly agree with all of what you&#x27;ve said here.  In our case, it&#x27;s not unusual for a single customer environment to surge to 200-300 instances of an underlying compute server, and then scale back down to 20-30 at steady state.   With 30 customer environments, you might have customers running from anywhere as low as 15 containers to as many as 500+, with a lot of dynamic flux depending on data ingestion and ETL.<p>K8S is in flux, so you still have to have a few top-end SRE types to manage your kube environment - the acceleration &#x2F; maturity of the ecosystem is incredible though, so, sometime in the next 3-4 years, we&#x27;ll start to see things get standardized enough that the wizardry required to keep it running will become a more commodity skill set.<p>And, more importantly, most of the ecosystem is fairly identical between azure&#x2F;google&#x2F;AWS - so porting or going multi-cloud is usually a weeks effort if that&#x27;s something you want to do.<p>By &quot;Moving up the Stack&quot; - Of course I understand that cgroups&#x2F;linux underpins it all - it&#x27;s just that we&#x27;re not using linux system binaries to manage the containers directly.<p>I mean tasks like process, storage, memory, CPU, resource utilization isn&#x27;t something we tweak&#x2F;query with OS commands, rather we&#x27;re sending request&#x2F;limit configurations to kube, and let it worry about managing the resources, relying on PromQL to monitor resource utilization, etc...')