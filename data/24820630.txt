Item(by='alquemist', descendants=None, kids=[24820676, 24820669, 24820868, 24821678, 24820955, 24820858, 24820928, 24820670], score=None, time=1603051195, title=None, item_type='comment', url=None, parent=24820298, text='The title is misleading. The core technique still uses 60,000 images from MNIST, but &#x27;distills&#x27; them into 10 images that contain the information from the original 60,000. The 10 &#x27;distilled&#x27; images look nothing like digits. Learning a complex model from 10 (later reduced to 2) &#x27;distilled&#x27; number arrays <i>is</i> an interesting research idea, but it has little to do with reducing the size of the input dataset. Arguably the heavy lifting part of the learning process moved from training the model to generating the distilled dataset. There is also some unconvincing discussion around synthetic datasets, though it remains fully unclear how these synthetic datasets have anything to do with real world scenarios.<p>&gt; In a previous paper, MIT researchers had introduced a technique to “distill” giant data sets into tiny ones, and as a proof of concept, they had compressed MNIST down to only 10 images.')