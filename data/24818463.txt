Item(by='fritzo', descendants=None, kids=None, score=None, time=1603034290, title=None, item_type='comment', url=None, parent=24817249, text='I pursued this question in grad school [1], leaning towards symbolic logic:<p>Suppose we want to learn a &quot;natural&quot; programming language. Training data would be example programs that we believe should be easy to express in any language. Since each of those programs will be expressed in a particular language, we&#x27;ll need a notion of program equivalence across languages. As a toy framework let &quot;language&quot; mean a basis of combinators in pure lambda calculus; this is convenient because we have a (Hilbert-Post-)complete theory of behavioral equivalence among programs (H*, see Barendregt&#x27;s classic book), and because the combinatory basis problem has been well studied since 1950s. Applying machine learning we can try to &quot;fit a combinatory basis to data&quot; in the sense of finding a finite weighted set of combinators, giving more weight to language primitives with shorter spelling. Our loss function will be the Kolmogorov complexity of our training data, actually gradients will be better if we use a softmax-Kolmogorov complexity. I used gradient descent to update weights of existing language primitives, and used greedy sparse dictionary learning to propose new language primitives. Most of the work was in proving equivalence and approximating Kolmogorov complexity.<p>It was a cute experiment, but hopelessly far from practical.<p>[1] <a href="http:&#x2F;&#x2F;fritzo.org&#x2F;thesis.pdf" rel="nofollow">http:&#x2F;&#x2F;fritzo.org&#x2F;thesis.pdf</a> (2009)')