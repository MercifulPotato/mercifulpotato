Item(by='kordlessagain', descendants=None, kids=[25835874], score=None, time=1611004111, title=None, item_type='comment', url=None, parent=25825003, text='From 2019: <a href="https:&#x2F;&#x2F;heartbeat.fritz.ai&#x2F;deep-learning-has-a-size-problem-ea601304cd8" rel="nofollow">https:&#x2F;&#x2F;heartbeat.fritz.ai&#x2F;deep-learning-has-a-size-problem-...</a><p>&gt; Earlier this year, researchers at NVIDIA announced MegatronLM, a massive transformer model with 8.3 billion parameters (24 times larger than BERT)<p>&gt; The parameters alone weigh in at just over 33 GB on disk. Training the final model took 512 V100 GPUs running continuously for 9.2 days.<p>Running this model on a &quot;regular&quot; machine at some useful rate is probably not possible at this time.')