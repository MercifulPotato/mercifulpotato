Item(by='jensneuse', descendants=None, kids=[25015524, 25015560], score=None, time=1604761458, title=None, item_type='comment', url=None, parent=25015246, text='Client side caching with a normalized cache implementation is very hard to get right. I see why you would want that feature and if it were simple to implement I&#x27;d always want to use it. However I think we can get away with a solution that is a lot simpler than normalized caching. With persisted Queries we can apply the &quot;stale while revalidate&quot; pattern. This means we can invalidate each individual page and would have to re-fetch each page in case something updated. This is some overhead but the user experience is still very good. Normalized caching in the client can get super hairy with nested data. In addition, normalized caching adds a lot of business logic to the client which makes it hard to understand the actual source of truth. From a mental model it&#x27;s a lot simpler if the server dictates the state and the client doesn&#x27;t override it. If you allow the client to have a normalized cache the source of truth is shared between client and server. This might lead to bugs and generally makes the code more complicated than it needs to be. Is it really that bad to re-fetch a table? I guess most of the time it&#x27;s not. I&#x27;ve written a blog post on the topic if you want to expand on it further: <a href="https:&#x2F;&#x2F;wundergraph.com&#x2F;blog&#x2F;2020&#x2F;09&#x2F;11&#x2F;the-case-against-normalized-caching-in-graphql" rel="nofollow">https:&#x2F;&#x2F;wundergraph.com&#x2F;blog&#x2F;2020&#x2F;09&#x2F;11&#x2F;the-case-against-nor...</a>')