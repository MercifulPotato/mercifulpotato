Item(by='ajtulloch', descendants=None, kids=None, score=None, time=1607287904, title=None, item_type='comment', url=None, parent=25314830, text='For everyone saying &quot;oh wow, we can go back to SVMs now, huge speedups, etc&quot; - that&#x27;s more than a little premature. This is purely a formal equivalence, but not very useful computationally.<p>The math here is pretty much first-year undergraduate level calculus, and it&#x27;s worth going through section 2 since it&#x27;s quite clearly written (thanks Prof Domingos).<p>Essentially, what the author does is show that any model trained with &quot;infinitely-small step size full-batch gradient descent&quot; (i.e. a model following a gradient flow) can be written in a &quot;kernelized&quot; form<p><pre><code>   y(x) = \\sum_{(x_i, y_i) \\in L} a_i K(x, x_i) + b.\n\n</code></pre>\nThe intuition most people have for SVMs is that the constants a_i are, well, <i>constant</i>, that the a_i are sparse, and that the kernel function K(x, x_i) is cheap to compute (partly why it&#x27;s called the &#x27;kernel trick&#x27;).<p>However, none of those properties apply here, which is why this isn&#x27;t particularly exciting computationally.\nThe &quot;trick&quot; is that both a_i and K(x, x_i) involve path integrals along the gradient flow for the input x, and so doing a single inference is approximately as expensive as training the model from scratch (if I understand this correctly).')