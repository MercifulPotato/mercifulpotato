Item(by='qayxc', descendants=None, kids=None, score=None, time=1610628063, title=None, item_type='comment', url=None, parent=25774929, text='Instead of arguing back and forth, how about a test case instead?<p>Pretraining BERT takes 44 minutes on 1024 V100 GPUs [1]<p>This requires dedicated instances, since shared instances won&#x27;t be able to get to peak performance if only because of the &quot;noisy neighbour&quot;-effect.<p>At GCP, a V100 costs $2.48&#x2F;h [2], so Microsoft&#x27;s experiment would&#x27;ve cost $2,539.52.<p>Smaller providers offer the same GPU at just $1.375&#x2F;h [3], so a reasonable lower limit would be around $1,408.<p>For a single BERT pretraining, provided highly optimised workflows and distributed training scripts are already at hand, renting a GPU for single training tasks seems to be the way to go.<p>The cost of V100-equivalent end-user hardware (we don&#x27;t need to run in a datacentre, dedicated workstations will do), is about $6,000 (e.g. a Quadro RTX 6000), provided you don&#x27;t need double precision. The card will have equal FP32 performance, lower TGP and VRAM that sits between the 16 GB and 32 GB version of the V100.<p>Workstation hardware to go with such card will cost about $2,000, so $8,000 are a reasonable cost estimation.\nThe cost of electricity varies between regions, but in the EU the average non-household price is about 0.13€&#x2F;kWh [4].<p>Pretraining BERT therefore costs an estimated 1024 h * 0.13€&#x2F;kWh * 0.5 kW ≈ 57€ in electricity (power consumption estimated from TGP + typical power consumptions of an Intel Xeon workstation from my own measurements when training models).<p>In order to get the break-even point we can use the following equation: t * $1,408 = $8,000 + t * $69, which results in t = 8,000&#x2F;(1408-69) or t &gt; 5.<p>In short, if you pretrain BERT 6 times, you safe money by BUYING a workstation and running it locally over renting cloud GPUs from a reasonably cheap provider.<p>This example only concerns BERT, but you can use the same reasoning for any model that you know the required compute time and VRAM requirements of.<p>This only concerns training, too - inference is a whole different can of worms entirely.<p>[1] <a href="https:&#x2F;&#x2F;www.deepspeed.ai&#x2F;news&#x2F;2020&#x2F;05&#x2F;27&#x2F;fastest-bert-training.html" rel="nofollow">https:&#x2F;&#x2F;www.deepspeed.ai&#x2F;news&#x2F;2020&#x2F;05&#x2F;27&#x2F;fastest-bert-traini...</a><p>[2] <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;gpus-pricing" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;gpus-pricing</a><p>[3] <a href="https:&#x2F;&#x2F;www.exoscale.com&#x2F;syslog&#x2F;new-tesla-v100-gpu-offering&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.exoscale.com&#x2F;syslog&#x2F;new-tesla-v100-gpu-offering&#x2F;</a><p>[4] <a href="https:&#x2F;&#x2F;ec.europa.eu&#x2F;eurostat&#x2F;statistics-explained&#x2F;index.php&#x2F;Electricity_price_statistics#Electricity_prices_for_non-household_consumers" rel="nofollow">https:&#x2F;&#x2F;ec.europa.eu&#x2F;eurostat&#x2F;statistics-explained&#x2F;index.php...</a>')