Item(by='thesz', descendants=None, kids=[25903851], score=None, time=1611581501, title=None, item_type='comment', url=None, parent=25899286, text='Their slides <a href="https:&#x2F;&#x2F;pgm.di.unipi.it&#x2F;slides-pgm-index-vldb.pdf" rel="nofollow">https:&#x2F;&#x2F;pgm.di.unipi.it&#x2F;slides-pgm-index-vldb.pdf</a> about PGM index, page 21.<p>They stop at page size of 1024 <i>bytes</i> - that indicates they are tested in-memory situation. And, which is worse, their compression ratio advantage almost halves when block size is doubled. Thus, what about B-tree with blocks of 16K or even 256K?<p>Also, what about log-structured merge trees where bigger levels can use bigger pages and, which is quite important, these bigger levels can be constructed using (partial) data scan. These bigger levels can (and should) be immutable, which enables simple byte slicing of keys and RLE compression.<p>So, where&#x27;s a comparison with more or less contemporary data structures and algorithms? Why beat half a century old data structure using settings of said data structure that favors your approach?<p>My former colleague once said &quot;give your baseline some love and it will surprise you&quot;. I see no love for B-trees in the PGM work.')