Item(by='mryab', descendants=None, kids=None, score=None, time=1610976333, title=None, item_type='comment', url=None, parent=25819965, text='Not directly related, but the Learning@home [1] project aims to achieve precisely that goal of public, volunteer-trained neural networks. The idea is that you can host separate &quot;experts,&quot; or parts of your model (akin to Google&#x27;s recent Switch Transformers paper) on separate computers.<p>This way, you never have to synchronize the weights of the entire model across the participants â€” you only need to send the gradients&#x2F;activations to a set of peers. Slow connections are mitigated with asynchronous SGD and unreliable&#x2F;disconnected experts can be discarded, which makes it more suitable for Internet-like networks.<p>Disclaimer: I work on this project. We&#x27;re currently implementing a prototype, but it&#x27;s not yet GPT-3 sized. Some issues like LR scheduling (crucial for Transformer convergence) and shared parameter averaging (for gating etc.) are tricky to implement for decentralized training over the Internet.<p>[1] <a href="https:&#x2F;&#x2F;learning-at-home.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;learning-at-home.github.io&#x2F;</a>')