Item(by='mistermann', descendants=None, kids=None, score=None, time=1602270513, title=None, item_type='comment', url=None, parent=24732807, text='&gt; Its impossible to construct a world view without axioms.<p>As a binary, perhaps.  But is the ability for the human mind to alter the manner in which it forms axioms fixed? <i>How would we know</i>?<p>&gt; Its impractical to construct a usefully large model of the world in the tiny slice of time afforded us without accepting far more uncritically based on prior performance of agent, apparent validity, credentials, social position etc etc and only discarding or doubting when given reason to.<p>I am not recommending that anyone stops &quot;accepting far more uncritically based on prior performance of agent, apparent validity, credentials, social position etc etc&quot;, I am asking people to not label unknown or axiomatic beliefs as ~&quot;known for certain to be be true&#x2F;false - any new or conflicting information is therefore false&quot;).  I recommend using the history of physics or medicine as one&#x27;s guide when pondering this.<p>&gt; Everyone is mostly a somewhat crappy model based on time and brain constraints. People can be viewed as a graph of agents collecting information about subjects along with meta information about other agents needed to construct a much larger pool of information.<p>This is exactly the style of abstract, systems analysis thinking that I believe the world needs far more of.<p>&gt; Conspiracy theorists who come into a discussion with a default disbelieve position on a particular agent are likely to spend much more time finding fault and may actually be more accurate at identifying correct faults the same way you are I are more apt to correctly identify a variety of conspiracy theorists as kooks and find immediate fault with their arguments.<p>From an abstract perspective, this same behavior can be seen in all human beings, regardless of their community affiliations - it seems to be innate subconscious heuristics that enable it.  The frequency and magnitude may vary per community, and adjusting one&#x27;s heuristics accordingly is both reasonable and logical, but if one&#x27;s heuristic is to assume that membership in a group <i>necessarily</i> implies certain things, without exception, then you have opened yourself up to a future of erroneous thinking.<p>For a specific <i>instance of</i> this at the object level (people who have skin colour different than one&#x27;s own, and to what degree it should be considered in your decision making), this advice seems easy to understand and uncontroversial.  But when you simply change the dimension of categorization (to membership in a group on something other than skin color), might a fundamental change in <i>thinking style</i> occur?  And might the mind <i>energetically defend</i> that belief (but also not want to get too deeply into a discussion about <i>just why is it</i> that it so aggressively defends this <i>particular</i> belief, while for others it has little more than indifference.  Is there perhaps something &quot;special but unseen&quot; about this one?  How might one know the answer to that question?)<p>&gt; Unfortunately kooks are apt to have a broken model of whom is trustworthy and to have built up a large collection of incorrect facts. A particular challenge is the fact that they are possessed of a large collection of &quot;facts&quot; that they aren&#x27;t learned enough to have come up with in the first place and don&#x27;t really have an accurate model of.<p>Again, what you are saying applies to all human beings, and always has.  It is innate.  Compare current ~&quot;consensus beliefs&quot; in &#x27;USA 2020&#x27;, to consensus beliefs held in various countries throughout the world - are there disagreements between groups?  Is one group always right, and the other always wrong?  <i>By what means does one know the answer, 100% of the time?</i>  Or, instead of comparing to other countries, compare to prior periods in US history, and ask the same question.<p>&gt; See the moon landing truthers as a particular example. It&#x27;s trivial to collect &quot;facts&quot; that require only a small amount of bad understanding to &quot;get&quot; but require a substantial understanding of science to actually refute. Since they can&#x27;t build an accurate enough model of the world to actually understand they would have to accept the expertise of others as valid in order to correct their model of the world. Having already rejected such there is no hope for them.<p>This seems fairly true.  So, are we to form certain conclusions based on these facts?  Are moon landing truthers a big problem? Are the attributes of this conspiracy representative of the attributes of all ideas from that community, and then via simple logic we know(!) that the proper heuristic to form is to immediately reject all ideas from that community?  If that isn&#x27;t what you&#x27;re saying, what <i>specific</i> idea is it that you are intending to communicate with this example?<p>&gt; These &quot;facts&quot; act like prions corrupting their model. When they see people providing true and valid info they are predisposed to discard that agents information as corrupted because it contradicts prior beliefs. Since their prior beliefs predispose them to believe bad agents and disbelieve good agents their model inevitably gets worse until it is unrecoverable.<p>Do you believe that this is a 100% one way street?  Do you hold the belief that <i>literally every single idea in the conspiracy community is 100% wrong</i>, and where there are conflicting ideas, the corresponding theory in the mainstream world is 100% correct (known(!) to be correct, as opposed to axiomatic correctness)?  If so, upon what actual evidence does your belief system rest?<p>All sorts of the (perceived to be) &quot;facts&quot; on many matters, are simply not facts.  There are a massive number of questions in the world, <i>for which the answer is literally not known</i>.  I am suggesting that people <i>start</i> distinguishing, <i>explicitly</i>, between True, False, and Unknown.  I should probably also point out an accompanying and often unrealized idea:  <i>certainty is not a pre-requisite for action</i>.  Of course we all know this <i>at the abstract level</i>, but let&#x27;s not forget this at the object level.  Abstract knowledge being seemingly inaccessible from object level cognitive processes can be regularly observed, and can sometimes result in extremely harmful outcomes - I am suggesting that we keep such things in the forefront of our minds.<p>The main question I would like to put to people is simple:  are you willing, and able, to differentiate between True, False, and Unknown?  Both in a binary &quot;matter of principle&quot; sense, and also as a &quot;to what degree are you able to, with high skill, in constant, reliably consistent, real time operations&quot; sense?  (And, consider how you know whether the answers you are giving are actually correct, and not estimates, or intentions?)<p>And if the answer to the first question is &quot;No&quot;, or &lt;downvote&gt;, ask yourself whether choosing <i>wilful</i> ignorance is a good idea, and consider whether there may be some sort of unseen force involved in making this choice - in a programming forum of all places.<p>EDIT:  I would also like to restate an idea from my prior comment, in hopes of catalyzing more in-depth <i>abstract</i> thinking:<p>&gt;&gt; Members from both communities will take offence (usually &quot;quite&quot; passionately) at some portion of the above, and attempt to rebut the assertion in the standard form:<p>&gt;&gt; [rhetoric, narrative, &quot;logic&quot;, &quot;common sense&quot;, &quot;facts&quot;&#x2F;axioms&#x2F;intuitions presented as facts] + [and then therefore we shall conclude...]&quot;<p>&gt;&gt; ...but there will almost always be a flaw in the respective rebuttals: invalid epistemology.')