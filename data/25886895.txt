Item(by='joe_the_user', descendants=None, kids=None, score=None, time=1611442388, title=None, item_type='comment', url=None, parent=25884808, text='One thing to consider is the distinction between deep neural networks as mathematical objects and machine learning as currently practiced.<p>Lately, there have been quite a few theories on neural networks as ideal nonlinear approximators [1]. Similarly, people have shown many ways that gradient descent can tend to reach a global maximum of regularized curve-closeness[2]. Which is to say, if your development cycle is: gather-data, train, test, deploy, we know this approximates the data almost ideally; you can&#x27;t really do much better than a deep network.<p>But we know in practice, when deployed, that deep neural networks actually have many limitations (compared to our intuitions or compared to human performance, etc). There are some obvious explanations. Of course, they&#x27;re limited by our ability to gather data and by the biases of the data. But even more, they&#x27;re limited to situations where you have large chunks of unchanging data that you can extrapolate from.<p>Given that deeps are more or less perfect for the train-test-deploy cycle, it seems like the problem is with this cycle itself. And it&#x27;s easy to see human beings somehow acting &quot;intelligently&quot; without using this cycle. So figuring out an alternative to this might be something to look at.<p>[1] For example: Nonlinear Approximation and (Deep) ReLU Networks I. Daubechies, et al <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1905.02199" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1905.02199</a><p>[2] For example: Gradient descent optimizes over-parameterized deep ReLU networks Difan Zou et al <a href="https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s10994-019-05839-6" rel="nofollow">https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s10994-019-05839-6</a>')