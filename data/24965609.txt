Item(by='hrktb', descendants=None, kids=[24966073, 24969046, 24965628], score=None, time=1604303797, title=None, item_type='comment', url=None, parent=24964800, text='I don’t understand this part:<p>&gt; In the current version, model inference is executed on the client’s CPU for low power consumption and widest device coverage.<p>Naively I would think model inference done server side would have the lower CPU power (from the client point of view) and widest device coverage (client does nothing more), what am I missing ?')