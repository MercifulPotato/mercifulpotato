Item(by='mirdaki', descendants=None, kids=[24848713, 24836334], score=None, time=1603157901, title=None, item_type='comment', url=None, parent=24826951, text='I&#x27;m very excited to see this discussed! Even without pressure from government entities, I would certainly want some sort of way to prevent something like child abuse content from being uploaded to my server, so I&#x27;d love for a system like this to work well.<p>1. Would the system discussed have anyway to diminish &quot;cancel culture&quot; like attacks? For instance, if a user were to say something egregious like &quot;Empire Strikes Back is bad&quot;, what is preventing especially enthusiastic fans from labeling them with having uploaded child abuse content. Presumably that would be a good way to get that user banned from most popular servers. Is there a way to &quot;appeal&quot; this is the moderator or server host is a bad actor? Is having a good enough reputation before an incident like that enough to not be blocked everywhere?\n2. Is there a way we could test this outside of Matrix? It seems useful to social platforms, particularly other decentralized ones. If it could be done in a platform independent way, it could be a nice open source collaboration (granted, how people interact on different platforms might be enough to make that impossible to generalize effectively). I also ask, because this sort of thing is really dependent on how people interact, so &quot;play testing&quot; it might be beneficial to getting a more realistic design going early on. Even doing something low-tech might find some issues that should be addressed.<p>Is there a room discussing this in particular? I&#x27;d be interested in participating.')