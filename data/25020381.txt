Item(by='beagle3', descendants=None, kids=None, score=None, time=1604789144, title=None, item_type='comment', url=None, parent=25020096, text='Yes. I haven&#x27;t touched this since 1995, so I had to refresh my memory. I was indeed, talking about Sprecher&#x27;s modification. Back when I studied this, the proofs I found were not constructive.<p>I was unaware, but apparently Gribel gave a constructive proof in 2009 (link from Wikipedia article about KA rep theorem). I would have to read it and hope I am not too rusty to understand it before I could really ponder your question...<p>But I could offer two places I would have looked:<p>1. The approximation is of a continuous function, and such approximations (e.g. chebychev, bernstein) usually require that you be able to sample the function at specific points - but learning usually gives you training data that does not correspond to those specific points. It&#x27;s possible that construction fails here somehow.<p>2. The approximation is too hard in practice. This is the too often the case for Breiman&#x27;s beautiful ACE (Alternating Conditional Expectation) which, if you squint hard enough, looks like a two-layer network where each neuron has its own transfer function.  The algorithm is incredibly simple in theory, but very hard to use in practice.')