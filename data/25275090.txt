Item(by='KMag', descendants=None, kids=None, score=None, time=1606904374, title=None, item_type='comment', url=None, parent=25267443, text='Interesting, though the overhead of nanosleep vs. futex wait with non-NULL timeout should be dwarfed by the context switch.  I can see where if you have extreme latency and&#x2F;or throughput constraints on the writer side of the ring buffer, you couldn&#x27;t afford the extra atomic read and occasional futex wake on the writer side to wake the consumer.<p>&gt; but being written by only one process, ownership of cache lines never changes, so hardware locks are not engaged.<p>Which cache coherency protocol are you referring to where cache lines are &quot;locked&quot;?  I&#x27;m aware that early multiprocessor systems locked the whole memory bus for atomic operations, but my understanding is that most modern processors use advanced variants on the MESI protocol[0].  In the MOESI variant, an atomic write (or any write, for that matter) would put the cache line in the O state in the writing core&#x27;s cache.  If we had to label cache line states as either &quot;locked&quot; or &quot;unlocked&quot;, the M, O, and E states would be &quot;locked&quot;, and S and I would be &quot;unlocked&quot;.<p>Your knowledge of cache coherency protocols is probably much better than mine, so I&#x27;m probably just misinterpreting your shorthand jargon for &quot;locked&quot; cache lines.  By &quot;hardware locks are not engaged&quot; do you just mean that the O state doesn&#x27;t ping-pong between cores?<p>[0] <a href="http:&#x2F;&#x2F;www.rdrop.com&#x2F;users&#x2F;paulmck&#x2F;scalability&#x2F;paper&#x2F;whymb.2010.07.23a.pdf" rel="nofollow">http:&#x2F;&#x2F;www.rdrop.com&#x2F;users&#x2F;paulmck&#x2F;scalability&#x2F;paper&#x2F;whymb.2...</a>')