Item(by='btown', descendants=None, kids=None, score=None, time=1610549106, title=None, item_type='comment', url=None, parent=25735175, text='&gt; Maybe you should reconsider who you follow if the people you follow wallow in hate and bigotry? Why doesn&#x27;t the party of individual responsibility doesn&#x27;t seem to understand you can just stop following the hate? Seek out and follow reasonable voices.<p>There are a LOT of facets to an answer to that, but one that is <i>entirely</i> enabled by the technology industry is the concept of an algorithmic news feed promoting <i>unsolicited</i> content from not-yet-followed sources, <i>especially</i> when that algorithm is based on a function of &quot;all of your activity over an extended period of time&quot; and &quot;the activity of people you interact with but whose views you never explicitly endorsed.&quot;<p>You <i>cannot</i> simply &quot;just stop following the hate&quot; when the hate is repeatedly shoved down your throat daily in conspicuous &quot;join this group&quot; ads in Facebook. And this is far, far more prominent than we had believed, per <a href="https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;opinion-platforms-must-pay-for-their-role-in-the-insurrection&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;opinion-platforms-must-pay-for-t...</a> :<p>&gt; Facebookâ€™s own research revealed that 64 percent of the time a person joins an extremist Facebook Group, they do so because the platform recommended it. Facebook has also acknowledged that pages and groups associated with QAnon extremism had at least 3 million members, meaning Facebook helped radicalize 2 million people.<p>The frustrating thing about conversations about censorship on platforms is that you&#x27;re not starting from a baseline of neutrality, because the algorithm itself is picking winners even if it wasn&#x27;t coded that way. Any algorithm that takes engagement-after-clicking-a-promotion as an input will start to take advantage of people who are more susceptible to extremist content. Knowing what we know now, does the continued deployment of those algorithms constitute <i>intentional</i> corporate speech that emphasizes extremism? And in that case, would the removal or manual tuning of said corporate speech really be the censorship we&#x27;re worried about?')