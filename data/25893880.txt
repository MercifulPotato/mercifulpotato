Item(by='moyix', descendants=None, kids=None, score=None, time=1611509865, title=None, item_type='comment', url=None, parent=25886577, text='For large models it does help! The training loop for multiple GPUs with data parallelism is roughly:<p>1. Split the data up<p>2. Do a forward and backward pass on each GPU individually<p>3. Compute the average of the gradients and update the model on each GPU<p>4. Repeat<p>For step 3 you need to send the gradients from each GPU somewhere, and then send back either the averaged gradient or the updated model weights. So when the model is large (say, 3GB for GPT 774M!) that&#x27;s a lot of GPU-GPU communication!<p>You&#x27;re right that for the vast majority of ML cases, the models are small enough that the synchronization cost is negligible, though.<p>I wrote up some benchmarks here:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;9371" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;9371</a>')