Item(by='datameta', descendants=None, kids=[24758140], score=None, time=1602529101, title=None, item_type='comment', url=None, parent=24757405, text='Sure. I saw talks by NXP and Renesas at the ARM Dev Summit last week that demonstrated the use of the Glow ML compiler [0] and CMSIS-NN Library [1] to squeeze a Wake Word NN [2] from 20KB to 14KB (with much greater reduction possible on larger ML models). Both Tensorflow Lite and Pytorch is supported in this toolchain.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;glow" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;glow</a><p>[1] <a href="https:&#x2F;&#x2F;arm-software.github.io&#x2F;CMSIS_5&#x2F;NN&#x2F;html&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;arm-software.github.io&#x2F;CMSIS_5&#x2F;NN&#x2F;html&#x2F;index.html</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;tensorflow&#x2F;tree&#x2F;master&#x2F;tensorflow&#x2F;lite&#x2F;micro&#x2F;examples&#x2F;micro_speech" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;tensorflow&#x2F;tree&#x2F;master&#x2F;tensorf...</a>')