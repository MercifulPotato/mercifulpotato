Item(by='xiphias2', descendants=None, kids=[24906539], score=None, time=1603798983, title=None, item_type='comment', url=None, parent=24904642, text='Julia is improving, but I wouldn&#x27;t compare it to BLAS&#x2F;LAPACK, cuBLAS is a much better comparision.<p>PyTorch uses tensor cores, supports mixed precision multiplication and bfloat16 representation.<p>Julia&#x27;s implementation in CUDA.jl doesn&#x27;t even use shared memory, which is important, as using that 10MB cache can reduce the needed main memory bandwidth significantly.<p>Also the new 30xx NVIDIA series supports sparse matrix multiplication, which is great if you have ReLU operations in your data processing for example. I&#x27;m not sure if PyTorch has support for that, but I&#x27;m sure it will be added, just like how mixed precision multiplication was added this year.<p>We&#x27;re talking about an order of magnitude difference if you add all these small things together that PyTorch &#x2F; cuBLAS supports. Of course Julia can call into these libraries, but then all the nice optimizations that make Julia Julia go away.<p>I had to think really hard of these things actually, because I love the Julia language, but the more I see ML pipelines beating the previous state-of-the-art models by 5x, the more I understand why an interpreted language with a great linear algebra library won the data processing race (I wish it would have been Ruby though).')