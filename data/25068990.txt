Item(by='YeGoblynQueenne', descendants=None, kids=[25069341], score=None, time=1605183400, title=None, item_type='comment', url=None, parent=25068678, text='&gt;&gt; It seems to me that if the programmer understands the algorithm, then the\nalgorithm is limited by the programmer!<p>Well, a learning algorithm is always going to be limited by something. In AI\nliterature any such something is grouped under &quot;inductive bias&quot; and there is\nno machine learning algorithm that doesn&#x27;t incorporate some kind of inductive\nbias. Bayesian learners have their priors, distance learners have their\ndisance functions, Support Vector Machines in particular have their kernels,\nand of course neural networks have their intricate architectures painstakingly\nhand-engineered and fine-tuned to a particular domain, or even a specific\ndataset [1]. Indeed it is probably impossible to have learning without\ninductive bias [2] [3].<p>The opinion in the short piece above is representative of a current trend in\nmachine learning, of taking &quot;the human out of the loop&quot;, which loosely\ntranslates in trying to learn everything end-to-end, only from examples, while\npretending that no attempt is made at any point to guide the learner to find\na consistent hypothesis that explains the examples.<p>Unfotunately, in practicce, this ideal remains a fantasy. All the progress\nachieved with deep learning in the last few years would not have been possible\nwithout the discovery (by chance or concerted effort) of good biases that are\nconducive to learning in specific domains, e.g. convolutional layers for image\nrecognition, or Long-Short Term Memory cells for sequence learning, etc.<p>And where did these good biases come from? Why, from human programmers. Humans\nourselves most likely come equipped with very strong, very useful biases.\nWe&#x27;ve used those biases and our ability to generalise to come up with powerful\nabstractions, such as the laws of physics or mathematics. It took us literally\nthousands of years to amass this fortune of knowledge (possibly millions,\nduring our evolution).<p>Why would we not use those finely-honed biases of ours and all the knowledge\nwe&#x27;ve collected to kickstart a new form of intelligence? After all, when we\nwant fire, we don&#x27;t sit around waiting for thunder to strike a tree, anymore.\nWe can start fires on our own. <i>Because</i> of our intelligence- because to build\nupon prior knowledge is intelligent.<p>__________________<p>[1] See for example the Neural Network Zoo, a collection of neural net\narchitectures:<p><a href="https:&#x2F;&#x2F;www.asimovinstitute.org&#x2F;neural-network-zoo&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.asimovinstitute.org&#x2F;neural-network-zoo&#x2F;</a><p>Or most of the deep learning literature published in the last few years, where\nevery little architectural tweak is presented as a major breakthrough.<p>[2] Famously argued by Tom Mitchell in &quot;The Need for Biases in Learning\nGenearlizations&quot;:<p><a href="https:&#x2F;&#x2F;www.semanticscholar.org&#x2F;paper&#x2F;The-Need-for-Biases-in-Learning-Generalizations-Mitchell&#x2F;6cf35ec34efa592f83e3a1b748aea14957fc784a?p2df" rel="nofollow">https:&#x2F;&#x2F;www.semanticscholar.org&#x2F;paper&#x2F;The-Need-for-Biases-in...</a><p>[3] But also see the discussion between Yan Le Cunn and Christopher Manning:<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=fKk9KhGRBdI" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=fKk9KhGRBdI</a>')