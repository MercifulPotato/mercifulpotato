Item(by='chordalkeyboard', descendants=None, kids=[24963876], score=None, time=1604276258, title=None, item_type='comment', url=None, parent=24963009, text='&gt; I think what you’re saying leads to much, much more harm than good. 99% of the time, defending things that are “one step away” from communities with widely known hate-based agendas is actually just legitimizing those abusive views and is not contributing some sort of “intellectual, good faith exploration of the fringe” or anything like that.<p>The point is that what is &quot;one step away&quot; is often subjective and relies on a web of inferences that the original speaker may not share. The views that are downvoted for being &quot;too close&quot; to &quot;hate-based agendas&quot; may not actually be close at all, merely shared by a variety of people, some of which also have hate-based views and some who do not. In other words it may be a mistake to infer that the view is too close to a &quot;bad idea&quot; if the idea itself cannot be labeled as bad. Refusing to discuss ideas that are legitimate objects of discourse because they are &quot;too close&quot; to bad ideas prevents reasonable-minded people from responding to those ideas if they are incorrect and drives discussion of these ideas to areas that have been taken over by extremists.<p>&gt; If there was a reliable way to detect the good faith attempts, then maybe, but it is so noise-jammed by the people taking hate or bigotry and trying to legitimize it under some false pretense of open inquiry that it’s not practically achievable.<p>The remedy for this is to treat assume good faith and interact on the object level, explaining why an idea may be good or bad, and explicitly draw the boundary between the acceptable idea they put forth and the unacceptable idea that you suspect they really mean.<p>&gt; As for people using memes or tropes as stereotypes to reject them, I think that is super, super uncommon.<p>Actually I see it all the time.<p>&gt; Especially with communities like MRA, which is even seen federally as a domestic terror group, it is not at all an unfair generalization to reject the discourse commonly associated with that community.<p>How then could we ever discuss reform of child support or divorce laws if any argument that suggests that something might be unfair for men pattern-matches to &quot;federally [designated] domestic terror&quot; and is therefore forbidden? Actually, how does being federally-designated as terror make it somehow forbidden to talk about? What if the feds wrongly designated something as terror, how could we talk about that?<p>&gt;  it is not at all an unfair generalization to reject the discourse commonly associated with that community. It doesn’t deserve any further due process or open forum - it’s just hate speech. I don’t think it’s fair to act as though instant rejection of that sort of community is unfair.<p>So do you see how you have identified discourse that you associate with a community as the same as that community? What if your association is the part that&#x27;s incorrect? What if a bad community discusses things that are both good and bad? This guilt by association is exactly the problem here. Observe that if an extremist community has somehow obtained a good idea, and you refuse to permit discussion of that idea because of its association with that community, you have given people who want to discuss that idea no choice but to discuss it with extremists. This occurs <i>even if the idea is itself bad but the person wants to explain why it is bad.</i><p>&gt; It’s not simplistic reductionism based on an in-group, it’s really actually based on analysis of the group’s core tenets and goals and a widely agreed perspective that it’s inappropriate.<p>Actually I think its based on pattern-matching and inference, both processes that cover a lot of terrain because they tolerate error in results.')