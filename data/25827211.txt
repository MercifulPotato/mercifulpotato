Item(by='froh', descendants=None, kids=[25828828], score=None, time=1611007393, title=None, item_type='comment', url=None, parent=25826445, text='&gt; How large are these datasets that have attracted so many research papers?<p>&gt; The NASA dataset contains 93 rows (that is not a typo, there is no power-of-ten missing), COCOMO 63 rows, Desharnais 81 rows, and ISBSG is licensed by the International Software Benchmarking Standards Group (academics can apply for a limited time use for research purposes, i.e., not pay the $3,000 annual subscription). The China dataset contains 499 rows, and is sometimes used (there is no mention of a supercomputer being required for this amount of data ;-).<p>&gt; Why are researchers involved in software effort estimation feeding tiny datasets from the 1980s-1990s into machine learning algorithms?<p>&gt; Grant money. Research projects are more likely to be funded if they use a trendy technique, and for the last decade machine learning has been the trendiest technique in software engineering research. What data is available to learn from? Those estimation datasets that were flogged to death in the 1990s using non-machine learning techniques, e.g., regression.<p>Is this telling me that most theories about &quot;sw estimation best practices&quot; are cargo cults o-O ?')