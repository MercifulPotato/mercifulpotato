Item(by='screye', descendants=None, kids=[25361228, 25350470, 25350291], score=None, time=1607455394, title=None, item_type='comment', url=None, parent=25346456, text='The pair of these papers: (Don&#x27;t read them in full.)<p>1.Attention is not explanation (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1902.10186" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1902.10186</a>)<p>2.Attention is not not Explanation (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1908.04626" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1908.04626</a>)<p>Goes to show the complete lack of agreement between researchers in the explainability space. Most popular packages (allen NLP, google LIT, Captum) use saliency based methods (Integrated gradients) or Attention. The community has fundamental disagreements on whether they capture anything equivalent to importance as humans would understand it.<p>An entire community of fairness, ethics and Computational social science is built on top of conclusions using these methods. It is a shame that so much money is poured into these fields, but there does not seem to be as strong a thrust to explore the most fundamental questions themselves.<p>(my 2 cents: I like SHAP and the stuff coming out of Bin Yu and Tommi Jakkola&#x27;s labs better..but my opinion too is based in intuition without any real rigor)')