Item(by='sliken', descendants=None, kids=None, score=None, time=1606290835, title=None, item_type='comment', url=None, parent=25206654, text='Depends.  Cachelines are typically 64-128 bytes long and sometimes depending on various factors that might be across on memory channel, or spread across multiple memory channels, somewhat like a RAID-0 disk.  I&#x27;ve seen servers (opterons I believe) that would allow mapping memory per channel or across channels based on settings in BIOS.  Generally non-NUMA aware OS ran better with stripped memory and NUMA aware OSs ran better non-stripped.<p>So striping a caching line across multiple channels goes increase bandwidth, but not by much.  If the dram latency is 70ns (not uncommon) and your memory is running at 3.2 GHz on a single 64 bit wide channel you get 128 bytes in 16 transfers.  16 transfers at 3.2GHz = 5ns.  So you get a cache line back in 75ns.  With 2 64 bit channels you can get 2 cache lines per 75ns.<p>So now with a 128 bit wide channel (twice the bandwidth) you wait 70ns then get 8 transfers @ 3.2GHz = 2.5ns.  So you get a cache line back in 72.5ns.  Clearly not a big difference.<p>So the question becomes for a complicated OS with a ton of cores do you want one cacheline per 72.5ns (the stripped config) or two cachlines per 75ns (the non-stripped config).<p>In the 16 bit 8 channel (assuming the same bus speed and latency) you get 8 cacheline per 90ns.  However not sure what magic apple has but I&#x27;m seeing very low memory latencies on the M1, on the order of 33ns!  With all cores busy I&#x27;m seeing cacheline througput of a cacheline per 11ns or so.')