Item(by='AtlasBarfed', descendants=None, kids=[25961894, 25961751], score=None, time=1611947376, title=None, item_type='comment', url=None, parent=25958012, text='scylladb had a blogpost once about how surprisingly small amounts of cpu time are available to process packets at the modern highest speed networks like 40gbit and the like.<p>I can&#x27;t find it now. I think they were trying to say that cassandra can&#x27;t keep up because of the JVM overhead and you need to be close to metal for extreme performance.<p>This is similar. Huge amounts of flooding I&#x2F;O from modern PCIx SSDs really closes the traditional gap between CPU and &quot;disk&quot;.<p>The biggest limiter in cloud right now is the EBS&#x2F;SAN. Sure you can use local storage in AWS if you don&#x27;t mind it disappearing, but while gp3 is an improvement, it pales to stuff like this.<p>Also, this is fascinating:<p>&quot;Take the write speeds with a grain of salt, as TLC &amp; QLC cards have slower multi-bit writes into the main NAND area, but may have some DIMM memory for buffering writes and&#x2F;or a “TurboWrite buffer” (as Samsung calls it) that uses part of the SSDs NAND as faster SLC storage. It’s done by issuing single-bit “SLC-like” writes into TLC area. So, once you’ve filled up the “SLC” TurboWrite buffer at 5000 MB&#x2F;s, you’ll be bottlenecked by the TLC “main area” at 2000 MB&#x2F;s (on the 1 TB disks).&quot;<p>I didn&#x27;t know controllers could swap between TLC&#x2F;QLC and SLC.')