Item(by='blackbear_', descendants=None, kids=None, score=None, time=1602187493, title=None, item_type='comment', url=None, parent=24719670, text='Not sure why we need another one of these, it does not add anything compared to every other tutorial there&#x27;s around, it only makes things more confusing with imprecision and errors, in particular:<p>1. &quot;Machine learning attempts to extract new knowledge from a large set of pre-processed data loaded into the system. Programmers need to formulate the rules for the machine, and it learns based on them&quot; - plain wrong, machine learning _learns_ the rules, that&#x27;s the whole point.<p>2. &quot;The main architectures of deep learning are: [...] recurrent neural networks, [...] and recursive neural networks&quot; - recursive NNs include recurrent NNs, and almost nobody uses the term &quot;recursive&quot;.<p>3. Comparing DL to ML makes no sense. It&#x27;s like comparing apples to fruits. Not to mention that almost every point is questionable in some way or the other: low quality datasets give low quality predictors, regardless, ANNs can be even more sensitive to raw data compared to some other ML algorithms, and not all ML models that are not ANNs are interpretable.<p>4. &quot;Deep learning doesnâ€™t rely on human expertise as much as traditional machine learning&quot; - of course it does, simply because DL is _not_ the right thing to do in _most_ cases.<p>5. &quot;in many cases, deep learning cannot be substituted.&quot; - see previous point.<p>6. &quot;An artificial neural network is heavily inspired by the structure of a human brain&quot; - the connection is tenuous at best.<p>7. &quot;Every neural network consists of [...], and functions&quot; - activations is the right term.<p>8. &quot;Neurons only operate numbers in the range [0,1] or [-1,1].&quot; - wrong.<p>9. &quot;There are a lot of activation functions, we will consider the most common ones: linear, sigmoid and hyperbolic tangent&quot; - linear activations are used only in very specific situations. And the relu family has been the most commonly used activation for years.<p>And I am only half-way.<p>I am really sorry to be like this but this is very low quality material.')