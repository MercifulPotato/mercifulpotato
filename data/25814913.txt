Item(by='CaptArmchair', descendants=None, kids=None, score=None, time=1610917296, title=None, item_type='comment', url=None, parent=25814429, text='The original intention of the Web - as devised by Tim Berners Lee - was to share documents in a research context. The idea of a URI or URL was to uniquely identify documents, but also to link them as hypertext. So you could jump from document to document.<p>Then Netscape came. They were the first having a primitive idea of the potential of the Web beyond simply linking pages.<p>The Dotcom bubble of 1999 and the first boom of e-commerce was exactly because in the few years preceding, everyone was scrambling to get dominate this Web thing. This included Microsoft, Sun and so on.<p>Now, nothing of this is new.<p>The entire idea of &quot;Rich Web Applications&quot; is as old as the Web. Silverlight, Java Web Applets, Flash,... They were all about this idea of building applications that could be loaded - and more importantly: controlled - via the Web.<p>Over the past 15 years, Google succeeded in dominating the browser market, and building a set of API&#x27;s that made it easy to build such Rich We Applications without needing extra plugins or 3rd party sandboxes. You can just do it using Javscript, HTML and CSS.<p>And while that&#x27;s not a bad thing in itself, the problem is that an entire generation of engineers has created an entire layer of abstractions on top of the browser engine in order to reinvent the exact same things which already existed back in the &#x27;90s. Only now, instead of running separate native programs and applications, everything is now corralled into a single browser environment... which tends to be entirely controlled by Google, if you use anything powered by Chromium.<p>And this often includes how simple text documents are consumed. The text you get doesn&#x27;t arrive as a HTML body in a HTTP request, it arrives as a chunk - or chunks - of JSON and needs to get parsed and assembled by layers of javascript until the assembled code can be fed to a browser engine... which will parse that once again in order to paint it on the browser canvas.<p>None of that is truly necessary when it comes to plain text. Heck, none of that is even necessary to render a single image on the browser canvas.<p>Plenty of major, highly visible, high traffic outlets like newspapers or media use these layers of complexity to publish text. Why? Because it gives them control over your experience and what you can do with the content e.g. paywalls, DRM, advertising, intricate metrics,...<p>Now, plenty of the Web still offers plain HTML and CSS, just like 20 years ago. And that&#x27;s awesome. But that&#x27;s a long tail of websites which largely remains in the dark since the vast majority of Internet users have been corralled into a set of centralized services that tend to promote links to these highly visible outlets through their recommendation engines.')