Item(by='qsort', descendants=None, kids=None, score=None, time=1602617587, title=None, item_type='comment', url=None, parent=24768918, text='&gt; In the past, when a construct like &#x27;intelligence&#x27; has been hard to pin down, science moves on--leaves it to &#x27;philosophy&#x27; and works with formal definitions.<p>Yes, that&#x27;s part of the point, your wording is better than mine. If we&#x27;re sticking to a purely historical perspective, this is definitely what happened. Most disciplines that today we (rightfully) regard as fully independent, originally splintered off philosophy (the most obvious examples are mathematics and physics, but even something like economics, in spite of having become more formalized recently, undoubtedly originates from moral philosophy).<p>&gt; You seem to be sweeping something important under the rug because it&#x27;s hard to pin down, and saying that this is what science has done in the past--and if so, you&#x27;re right.<p>I won&#x27;t deny that strictly speaking there is a bit of inductivism at play here. Historically, the scientific approach  of limiting the domain of discourse to a tractable subset has been so much more productive and successful than any alternative that my, as it were, &quot;bayesian prior&quot;, is that we should replicate the same approach if possible at all.<p>&gt; So from my perspective, it doesn&#x27;t matter that whatever &#x27;intelligence&#x27; is, is hard to pin down: it&#x27;s still got to be figured out, whether or not it&#x27;s difficult.<p>This is a reasonable position, but wouldn&#x27;t you agree that it&#x27;s more of a &quot;moral intuition&quot; (not that there&#x27;s anything wrong with that!) than a position regarding how ML results ought to be interpreted? As such I have no real counterpoints to offer, except perhaps an utilitarian point of view: are you really sure that banging your head against this very specific wall is the most productive thing to do?<p>Most problems I see with AI arise from either flat out using the models incorrectly (i.e. mathematically wrong, not ethically wrong, which is what I was pointing out in my previous comments) or from already familiar &quot;political&quot; problems, i.e. incentives, transparency, privacy, openness of the decision-making process.<p>The good news is that none of that is new. The bad news is that our track record as a species on problems of that kind is abysmal. I doubt that, in any case, trying to halt scientific progress makes sense.')