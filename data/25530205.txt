Item(by='LunaSea', descendants=None, kids=[25530978, 25530582], score=None, time=1608835817, title=None, item_type='comment', url=None, parent=25530069, text='I disagree. If you take one of the examples mentioned in said MOOC, which is the bias in word embeddings that makes vector arithmetic go from &quot;doctor&quot; to &quot;nurse&quot; if you replace &quot;male&quot; by &quot;female&quot;.<p>I agree that it would be nice if the returned vector would be &quot;doctor&quot; in both cases but the embedding code (the implementation) or the embedding algorithm (theory) have no idea about gender, ethics or moral.<p>Here the bias comes from the datasets the AI trained on.<p>The bias of those datasets comes from society writing texts in a biased way.<p>So the solution to fixing this &quot;bias&quot; is fixing the language used in society which is not an AI problem nor a dataset problem.')