Item(by='kelnos', descendants=None, kids=None, score=None, time=1602740187, title=None, item_type='comment', url=None, parent=24781490, text='Most of the data science &#x2F; ML code I&#x27;ve seen reminds me of academic researcher code.  Hack and experiment until it kinda works, never write tests because what&#x27;s the point, it works on my machine and that&#x27;s good enough.<p>For a ML-heavy project I ended up being the &quot;productionize this stuff&quot; guy.  My favorite was a bunch of python code that depended on a library that under the hood would spin up <i>a new JVM instance</i> to do some NLP work every time it was called (or whatever incorrect way they were using it caused this to happen).  And they wanted to do this in the context of a synchronous HTTP request&#x2F;response cycle.<p>My overall impression was that these were incredibly smart people, but they had no interest in understanding the context in which their code was going to run, or in understanding business requirements around reliability or maintainability.  They just wanted to hack on cool projects that they found interesting.<p>Given that, it doesn&#x27;t really surprise me that a lot of ML&#x2F;data science people don&#x27;t &quot;get&quot; accepted software development processes.  There were some who made the leap and learned to understand why some of these practices are useful and tend to give you a better product, but they seem to unfortunately be in the minority.  This is a pretty young field&#x2F;subfield, and I think it&#x27;s to be expected that practices are pretty raw and underdeveloped.')