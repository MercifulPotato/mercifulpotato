Item(by='azhenley', descendants=None, kids=None, score=None, time=1607544215, title=None, item_type='comment', url=None, parent=25362375, text='In response to this article&#x27;s discussion of automating some tasks away, back when I was at Microsoft we studied the effects of an automated code reviewer on team collaboration. Our automated code reviewer utilized a family of analyzers (e.g., static, dynamic, binary, security, and dependency analyzers, along with best practice linters), unit test results, and feedback from the build system.<p>To summarize, a <i>lot</i> of effort during code reviews is spent on superficial details while missing high-level design discussions and bugs. Anything that can get rid of those superficial problems (i.e., linters and program analyzers) is helpful, but we found that it was even better to optionally show the analyzer&#x27;s warnings to the entire team , not just an individual prior to the code review. Showing the warnings to the entire team let them discuss them (and they often did) and decide as a group what should be done.<p>Findings from our project: <a href="https:&#x2F;&#x2F;web.eecs.utk.edu&#x2F;~azh&#x2F;pubs&#x2F;Henley2018CHI_CFar.pdf" rel="nofollow">https:&#x2F;&#x2F;web.eecs.utk.edu&#x2F;~azh&#x2F;pubs&#x2F;Henley2018CHI_CFar.pdf</a>')