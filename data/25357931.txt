Item(by='j-pb', descendants=None, kids=None, score=None, time=1607518006, title=None, item_type='comment', url=None, parent=25350068, text='Hey Arjun,\nThanks for taking the time to reply!\nI didn&#x27;t mean to suggest that this was Franks opinion,\nI merely explained to the other user, who seemed to have a more negative attitude towards Materialize, why I am extremely excited about it, namely Frank being the CTO, and him having an extremely good track record in terms of research and code.<p>I think the general gist of &quot;use an OLTP database as your write model if you don&#x27;t absolutely know what you&#x27;re doing&quot; is completely sane advice, however I think there are far more nuanced (and in a sense also honest) arguments that can be made.<p>I think the architecture you&#x27;ve sketched is over engineered for what you&#x27;d need for the task. So here&#x27;s what I&#x27;d build for your inventory example, IFF the inventory was managed for a company with a multi Terra Items Inventory that absolutely NEEDS horizontal scaling:<p>One event topic in Kafka, partitioned along the ID of the different stores whose inventory is to be managed.\nThis makes the (arguably) strong assumption that inventory can never move between stores, but for e.g. a Grocery chain that&#x27;s extremely realistic.<p>We have one writer per store ID partition, which generates the events and enforces _serialisability_, with a hot writer failover that keeps up do date and a STONITH mechanism connecting the two. All writing REST calls &#x2F; GraphQL mutations for its store-ID range, go directly to that node.<p>The node serves all requests from memory, out of an immutable Data-structure, e.g. Immutable.js, Clojure Maps, Datascript, or an embedded DB that supports transactions and rollbacks, like SQLite.<p>Whenever a write occurs, the writer generates the appropriate events, applies them to its internal state, validates that all invariants are met, and then emits the events to Kafka.\nKafka acknowledging the write is potentially much quicker than acknowledging an OLTP transaction, because Kafka only needs to get the events into the memory of 3+ machines, instead of written to disk on 1+ machine (I&#x27;m ignoring OLTP validation overhead here because our writer already did that).\nAlso your default failure resistance is much higher than what most OLTP systems provide in their default configuration (e.g. equivalent to Postgres synchronous replication).<p>Note that the critical section doesn&#x27;t actually have to be the whole &quot;generate event -&gt; apply -&gt; validate -&gt; commit to kafka&quot; code. You can optimistically generate events, apply them, and then invalidate and retry all other attempts once one of them commits to Kafka. However that also introduces coordination overhead that might be better served mindlessly working off requests one by one.<p>Once the write has been acknowledged by Kafka, you swap the variable&#x2F;global&#x2F;atom with the new immutable state or commit the transaction, and continue with the next incoming request.<p>All the other (reading) request are handled by various views on the Kafka Topic (the one causing the inconsistencies in the article). They might be lagging behind a bit, but that&#x27;s totally fine as all writing operations have to go through the invariant enforcing write model anyways.\nSo they&#x27;re allowed to be slow-ish, or have varying QOS in terms of freshness.<p>The advantage of this architecture is that you have few moving parts, but those are nicely decomplected as Rich Hickey would say, you have use the minimal state for writing which fits into memory and caches, you get 0 lock congestion on writes (no locks), you make the boundaries for transactions explicit and gain absolutely free reign on constraints within them, and you get serialisability for your events, which is super easy to reason about mentally.\nPlus you don&#x27;t get performance penalties for recomputing table views for view models. (If you don&#x27;t use change capture and Materialize already, which one should of course ;] )<p>The two generals problem dictates that you can&#x27;t have more than a single writer in a distributed system for a single &quot;transactional domain&quot; anyways. All our consensus protocols are fundamentally leader election. The same is true for OLTP databases internally (threads, table&#x2F;row locks e.t.c.), so if you can&#x27;t handle the load on a single 0 overhead writer that just takes care of its small transactional domain, then your database will run into the exact same issues, probably earlier.<p>Another advantage of this that so far has gone unmentioned is that if allows you to provide global identifiers for the state in your partitions that can be communicated in side-effect-full interactions with the outside world.\nIf your external service allows you to store a tiny bit of metadata with each effect-full API call then you can include the offset of the current event and thus state.\nThat way you can subsume the external state and transactional domain into the transactional domain of your partition.<p>Now, I think that&#x27;s a much more reasonable architecture, that at least doesn&#x27;t have any of the consistency issues.\nSo let&#x27;s take it apart and show why the general populace is much better served with an OLTP database:<p>- Kafka is an Ops nightmare. The setup of a Cluster requires A LOT of configuration. Also Zookeper urgh, they&#x27;re luckily trying to get rid of it, but I think they only dropped it this year, and I&#x27;m not sure how mature it is.<p>- You absolutely 100% need immutable Data-structures, or something else that manages Transactions for you inside the writer. You DO NOT want to manually rollback changes in your write model. Defensive copying is a clutch, slow, and error prone (cue JS, urgh...: {...state}).<p>- Your write model NEEDS to fit into memory. That thing is the needles eye that all your data has to go through. If you run the single event loop variant, latency during event application WILL break you.\nIf you do the optimistic concurrency variant performing validation checks might be as or more expensive than recomputing the events from scratch.<p>- Be VERY weary of communications with external services that happen in your write model. They introduce latency, and they break your transactional boundary that you set up before. To be fair OLTPs also suffer from this, because it&#x27;s distributed consistency with more than one writer and arbitrary invariants which this universe simply doesn&#x27;t allow for.<p>- As mentioned before, it&#x27;s possible to optimistically generate and apply events thanks to the persistent data structures, but that is also logic you have to maintain, and which is essentially a very naive and simple embedded OLTP, also be weary of what you think improves performance vs. what actually improves it.\nIt might be better to have 1 cool core, than 16 really hot ones that do wasted work.<p>- If you don&#x27;t choose your transactional domains well, or the requirements change, you&#x27;re potentially in deep trouble. You can&#x27;t transact across domains&#x2F;partitions, if you do, they&#x27;re the same domain, and potentially overload a single writer.<p>- Transactional domains are actually not as simple as they&#x27;re often portrayed. They can nest and intersect. You&#x27;ll always need a single writer, but that writer can delegate responsibility, which might be a much cheaper operation than the work itself.\nTake bank accounts as an example. You still need a single writer&#x2F;leader to decide which account ID&#x27;s are currently in a transaction with each other, but if two accounts are currently free that single writer can tie them into a single transactional domain and delegate it to a different node, which will perform the transaction and write and return control to the &quot;transaction manager&quot;.\nA different name for such a transaction manager is an OLTP (with Row-Level locking).<p>- You won&#x27;t find as many tutorials, if you&#x27;re not comfortable reading scientific papers, or at least academic grade books like Martin Kleppmanns &quot;Designing Data Intensive Applications&quot; don&#x27;t go there.<p>- You probably won&#x27;t scale beyond what a single OLTP DB can provide anyways. Choose tech that is easy to use and gives you as many guarantees as possible if you can. With change capture you can also do retroactive event analytics and views, but you don&#x27;t have to code up a write-model (and associated framework, because let&#x27;s be honest this stuff is still cutting edge, and really shines in bespoke custom solutions for bespoke custom problems).<p>Having such an architecture under your belt is a super useful tool that can be applied to a lot of really interesting and hard problems, but that doesn&#x27;t mean that it should be used indiscriminately.<p>Afterthought; I&#x27;ve seen so many people use an OLTP but then perform multiple transactions inside a single request handler, just because that&#x27;s what their ORM was set to. So I&#x27;m just happy about any thinking that people spend on transactions and consistency in their systems, in whatever shape or form, and I think making the concepts explicit instead of hiding them in a complex (and for many unapproachable) OLTP&#x2F;RDBMS monster helps with that (if Kafka is less of a monster is another story).<p>I think it&#x27;s also important to not underestimate the programmer convenience that working with language native (persistent) data-structures has. The writer itself in its naive implementation is something that one can understand in full, and not relying on opaque and transaction breaking ORMs is a huge win.<p>PS:\nPlz start a something collaboratively with ObservableHQ, having reactive notebook based dashboards over reactive Postgres queries would be so, so, so, so awesome!')