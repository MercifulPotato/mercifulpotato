Item(by='fpgaminer', descendants=None, kids=[25768573, 25772581], score=None, time=1610570972, title=None, item_type='comment', url=None, parent=25767307, text='I love Star Trek, and that being a possible future is nice to entertain, but doesn&#x27;t take into account an AI singularity.  The latter, to some approximation, seems more likely to me.  And that puts a Moore&#x27;s law on intelligence.  Not just for us, but for any potential space faring race.  And it doesn&#x27;t have to be just &quot;AI&quot;; as soon as a race has the capability to recursively improve their own intelligence they&#x27;ll hit that exponential curve too.<p>So by the time a race gets space faring and solves FTL travel, it&#x27;s likely they&#x27;re at least a good step onto that exponential intelligence curve.  Hence a vague estimate of &quot;tens of thousands of orders of magnitude&quot;.<p>For example, as is we should be capable of building a GPT-human within 20 years (1).  About the same time frame that, if we put all our effort into it, it would take for us to barely colonize another planet in our solar system.  Given another 20 years and we&#x27;ve got  an AI that&#x27;s 1000x more intelligent than us.<p>I see our trajectory as hitting that exponential curve before we even get out of this solar system.  So I just imagine any potential &quot;visitors&quot; to our planet are much further along.<p>(1) This is calculated using Moore&#x27;s law: how many doubles of compute will it take before GPT-3 can be naively scaled to the estimated number of parameters of a human brain.  That doesn&#x27;t necessarily imply human level intelligence, but our studies so far indicate that there&#x27;s strong reason to believe that GPT-human will be something approximating 1000x smarter than GPT-3.  Human or not, that&#x27;s terrifyingly intelligent.  Remember that Transformer like architectures are the ones that &quot;solved&quot; protein folding last year.  And none of this calculation takes into account the potential for continued improvements to architectures and efficiency over that time span.  So while I don&#x27;t think GPT-human will start an AI apocalypse in 20 years, I do think GPT-human will be better than every other human on this planet at doing ... AI research.  And that&#x27;s where the spark of singularity begins.')