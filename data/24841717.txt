Item(by='mumblemumble', descendants=None, kids=[24841964], score=None, time=1603223689, title=None, item_type='comment', url=None, parent=24840697, text='It doesn&#x27;t necessarily seem that surprising to me.<p>If I can be <i>really</i> hand-wavy about it -- A lot of deep neural networks achieve their nonlinearity in a very constrained way. They stack linear models on top of each other, and the nonlinearity comes from using a a relatively simple nonlinear function such as logistic or tanh to scale the models&#x27; outputs before feeding them into the next one. (Without that step, you&#x27;d just have a linear combination of linear functions, which would itself be linear.)<p>That&#x27;s a pretty constrained form of nonlinearity compared to polynomial regression, which tries to directly fit some high-order polynomial. I don&#x27;t have anything like the math chops to prove it this, but I believe that means that the neural network is going to tend to favor a relatively smoother decision boundary, whereas polynomial regression is a naturally high variance sort of affair.')