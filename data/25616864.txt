Item(by='lostdog', descendants=None, kids=None, score=None, time=1609624055, title=None, item_type='comment', url=None, parent=25616555, text='I work in an area where high variance is very expected and unavoidable. Here&#x27;s what we do:<p>In your PR, you link to the tool showing the performance diff of your PR. The tool shows the absolute and relative differences of performance from the base version of code. It also tracks the variance of each metric over time, so it can kind of guess which metrics have degraded, though this doesn&#x27;t work consistently. The tool tries to highlight the likely degraded metrics so the engineer can better understand what went wrong.<p>If the metrics are better, great! Merge it! If they are worse, the key is to discuss them (quickly in Slack), and decide if they are just from the variance, a necessary performance degradation, or a problem in the code. Typically it&#x27;s straightforward: the decreased metrics either are unrelated to the change or they are worth looking into.<p>The key here is not to make the system too rigid. Good code changes cannot be slowed down. Performance issues need to be caught. The approvers need to be fast, and to mostly trust the engineers to care enough to notice and fix the issues themselves.<p>We also check the performance diffs weekly to catch hidden regressions.<p>IF YOUR ORGANIZATION DOES NOT VALUE AND REWARD PERFORMANCE IMPROVEMENTS, NONE OF THIS WILL WORK. Your engineers will see the real incentive system, and resist performance improvements. Personally, I don&#x27;t believe that Atlassian cares at all about performance, otherwise it never would have gotten this bad. Engineers <i>love</i> making things faster, and if they&#x27;ve stopped optimizing performance it&#x27;s usually because the company discourages it.')