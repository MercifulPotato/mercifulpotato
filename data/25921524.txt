Item(by='YeGoblynQueenne', descendants=None, kids=None, score=None, time=1611698188, title=None, item_type='comment', url=None, parent=25919006, text='From the title I was sure that this would be some horrible misuse of &quot;machine\nlearning&quot; to stand-in for &quot;AI&quot; but while Shannon himself never uses the term in\nthe video, the title seems accurate enough.<p>To summarise: a model mouse, named Theseus, is moved around a maze by a system\nof electromagnets placed under the maze and guided by an electromechanical\ncomputer (made with telephone relays). The computer is from Bell Labs and given\nthe date listed for the video (&quot;early 1950&#x27;s&quot;) it may be a Model VI.<p>The program that drives the mouse is a combination of a search algorithm with\nsome kind of modular memory. The mouse first finds its way around the maze by\nsuccessivly trying a set of four moves at right angles. The sequence seems to\nbe: move forward; if that&#x27;s impossible, turn 90Â° CW; repeat. This eventually\nfinds a path to the end of the maze, where a goal is placed (possibly magnetic).\nIn more modern terms, the program learns a plan to navigate the maze. Once the\nplan is learned it can be re-used any number of times to solve the maze without\nany more searching.<p>The interesting thing is that, if the maze changes, the program only needs to\nreplace <i>part</i> of its learned plan. Shannon demonstrates this by moving around\nsome of the walls of the maze, which are modular segments. Theseus initially\nenters its searching routine, until it finds its way to the part of the maze\nthat it knows already. After that, it&#x27;s smooth sailing, following the plan it\nalready knows.<p>I have no idea what that technique is that does that. If I guessed, I&#x27;d guess\nthat the program has a model of its world -a map of the maze- or, more\ninterestingly, it builds one as it goes along. When the maze changes, the\nprogram updates its model, but it is able to update only as much of the model as\ncorresponds to the part of the world that has really changed. This suggests some\nkind of object permanence assumption, an encoding of the knowledge that if the\nstate of some entity in the world is not known to have changed, then it has not\nchanged. Or, in other words, some solution to the frame problem (which was\nactually described about a decade later than Shannon&#x27;s demonstration).<p>Note well that modern AI systems very rarely exhibit this ability. For instance,\ndeep neural networks <i>cannot update their models</i> - not partly, not wholesale.\nLike I say, I have no idea what the approach is that is demonstrated. It may be\nsomething well-known but rarely used these days. It may be something that\nShannon came up with before John McCarthy fathered AI and so it never really\ncaught on as an AI approach. If anyone knows, please do tell, I&#x27;m very curious.<p>In any case &quot;Theseus&quot; (rather, its AI) seems to exhibit what McCarthy called\n&quot;elaboration tolerance&quot;, making formal representations of facts that can be\nmodified easily:<p><a href="http:&#x2F;&#x2F;jmc.stanford.edu&#x2F;articles&#x2F;elaboration.html" rel="nofollow">http:&#x2F;&#x2F;jmc.stanford.edu&#x2F;articles&#x2F;elaboration.html</a>')