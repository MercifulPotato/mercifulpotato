Item(by='jiggawatts', descendants=None, kids=None, score=None, time=1608351832, title=None, item_type='comment', url=None, parent=25474294, text='Ahhh... the joy of enterprise monitoring systems that do exactly nothing by default, and are very helpful in avoiding any further recurrences of one-time issues. At a nominal fee, of course.<p>The golden rule of both backups and monitoring is: <i>There are no time machines.</i><p>It&#x27;s not helpful to find out after the fact that a default-off alert or metric threshold alarm could have avoided the issue. It&#x27;s not helpful to blame the user for not knowing every one of <i>thousands</i> of metrics they &quot;should&quot; be monitoring. How would they know until they get burnt at least once?<p>Even if they <i>do get burnt</i>, how would they know which of the metrics they <i>weren&#x27;t capturing</i> could have been useful it was <i>was captured?</i><p>That&#x27;s not a rhetorical question!<p>Fundamentally the issue is this: Practically no enterprise monitoring system stores data efficiently enough to capture all metrics, so instead they simply... don&#x27;t.<p>Instead, these &quot;solutions&quot; trade a moderately difficult storage compression problem at the service provider end for a <i>physically impossible</i> time travel problem on the consumer end.<p>Just blame the user for not knowing ahead what disasters they will face! Job done! No need to figure out columnar compression, that would take actual engineering work for a couple of guys. But why bother when it&#x27;s soooo much easier to just dump some JSON into a storage account or S3 bucket and bill the customer for every metric. Mmm... dollars per metric per month. That&#x27;s the ticket to a nice robust revenue stream!<p>Apologies if I sound salty, but I&#x27;ve traced the root cause of outage after outage back to lazy vendors writing MVP monitoring systems that do literally nothing useful out of the box. These vendors simply <i>refuse</i> to store data efficiently enough to capture all relevant metrics to that I can check what happened without needing a TARDIS. Why would they when monitoring is a <i>revenue stream</i> that they measure in gigabytes?<p>PS: An ordinary Windows desktop has on the order of 10,000 to 50,000 performance counter metrics that it tracks. However, with even <i>light</i> compression, that&#x27;s barely a few gigabytes for a <i>year</i> of logging every metric every second. I&#x27;ve written code to do this personally. Name me a cloud vendor that can approach this within an order of magnitude without an eye-watering bill every month.')