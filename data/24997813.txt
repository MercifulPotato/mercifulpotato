Item(by='albertzeyer', descendants=None, kids=[24999712, 24999348, 24999402], score=None, time=1604580901, title=None, item_type='comment', url=None, parent=24995696, text='This is quite interesting.<p>In the beginning, there is this statement (assumption):<p>&gt; when processing natural language, we can bound the maximum number of things weâ€™ll need to store on the stack<p>Why is that? Is it because there is a limit N and all natural sentences ever are less than N words, and thus you also have a bound on the stack depth? But that&#x27;s not strictly true, just highly unlikely, right? In principle, for every sentence of length N, I can easily add a word, and have length N+1. In a similar way, I can also always make the stack deeper.<p>This discussion reminds me in general about whether RNNs are turing complete or not. There is a famous paper stating that they are (On the computational power of neural nets, Siegelmann &amp; Sontag, 1992). However, that uses infinite precision rational numbers for weights and activations. Once you have finite precision (standard floats), it has finite memory. Then stack depths are bounded of course, and also it is not Turing complete anymore.<p>Some interesting discussion about this is also here (own question, and also has some own answer, among others):\n<a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;2990277&#x2F;how-useful-is-turing-completeness-are-neural-nets-turing-complete" rel="nofollow">https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;2990277&#x2F;how-useful-is-tu...</a>')