Item(by='simias', descendants=None, kids=None, score=None, time=1611420917, title=None, item_type='comment', url=None, parent=25883499, text='&gt;How else would you lazy-load a database of (say) 32GB into memory, almost instantly?<p>By using an existing database engine that will do it for me. If you need to deal with that amount of data and performance is really important you have a lot more to worry about than having to use unsafe blocks to map your data structures.<p>Maybe we just have different experiences and work on different types of projects but I feel like being able to seamlessly dump and restore binary data transparently is both very difficult to implement reliably and quite niche.<p>Note in particular that machine representation is not necessarily the most optimal way to store data. For instance any kind of Vec or String in rust will use 3 usize to store length, capacity and the data pointer which on 64 bit architectures is 24 bytes. If you store many small strings and vectors it adds up to a huge amount of waste. Enum variants are also 64 bits on 64 bit architectures if I recall correctly.<p>For instance I use bincode with serde to serialize data between instances of my application, bincode maps almost 1:1 the objects with their binary representation. I noticed that by implementing a trivial RLE encoding scheme on top of bincode for running zeroes I can divide the average message size by a factor 2 to 3. And bincode only encodes length, not capacity.<p>My point being that I&#x27;m not sure that 32GB of memory-mapped data would necessarily load faster than &lt;16GB of lightly serialized data. Of course in some cases it might, but that&#x27;s sort of my point, you really need to know what you&#x27;re doing if you decide to do this.')