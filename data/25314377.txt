Item(by='sillysaurusx', descendants=None, kids=[25314594], score=None, time=1607174858, title=None, item_type='comment', url=None, parent=25314233, text='The weird part is, the paper didn&#x27;t seem to be saying that. It was mostly about energy usage, and bias in language models. The reviewer described it as &quot;anodyne,&quot; which seems apt: <a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;MachineLearning&#x2F;comments&#x2F;k69eq0&#x2F;n_the_abstract_of_the_paper_that_led_to_timnit&#x2F;" rel="nofollow">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;MachineLearning&#x2F;comments&#x2F;k69eq0&#x2F;n_t...</a><p>I&#x27;d totally agree with you. That would indeed be ridiculous. But... It&#x27;s strange... each time a new argument pops up, I dig into the new detail, and surprise: it seems to have a straightforward, boring answer. &quot;This is a pretty standard paper. Google wouldn&#x27;t have been hurt reputation-wise by letting it through. And we should probably be thinking more about energy usage and bias. She didn&#x27;t namedrop all relevant research, but there doesn&#x27;t seem to be anything here to demand a retraction over.&quot;<p>It only gets stranger when you take this into account, too. From the journal reviewer:<p><i>However, the authors had (and still have) many weeks to update the paper before publication. The email from Google implies (but carefully does not state) that the only solution was the paper&#x27;s retraction. That was not the case.</i><p>In some parallel universe, Google could re-hire her, Jeff and her could sit down and hammer out the paper, send the updated version, and there would still be two weeks to make even more edits. Isn&#x27;t the point of the edit window to address these problems?<p>What really got my attention though, was that she informed everyone months ago that her and her coauthors were writing this paper. She wasn&#x27;t working on some hit piece of a research paper. It&#x27;s just ... a standard survey of the current ML scene circa 2021. I read the abstract and go &quot;Yup, we use a shitload of energy. Yup, we should have better tools for filtering training data -- I&#x27;ve wanted this for myself. Where&#x27;s the bombshell?&quot;<p>For all the fuss people are making, you&#x27;d expect the paper to be arguing that we should stop doing AI for the betterment of humanity, or <i>something</i> weird. But it&#x27;s nothing like that.')