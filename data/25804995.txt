Item(by='lordnacho', descendants=None, kids=[25811785], score=None, time=1610827370, title=None, item_type='comment', url=None, parent=25803288, text='The thing that didn&#x27;t click for me was precisely the &quot;try to count the operations&quot; thing that the author mentions. In fact it&#x27;s the wrong road to go down, it isn&#x27;t the point of big-o, and yet that&#x27;s how you&#x27;re invited to think about it when the issue is presented in college. It&#x27;s only natural to think &quot;oh let&#x27;s look at all the operations and add them up&quot;.<p>I think of it pretty simply, but not in that formal big-theta&#x2F;big-omega kind of way, because you want to just quickly have an idea of whether you&#x27;ll write a really slow piece of code in general, not some best or worst case.<p>The question is simply what growth model dominates the increase of time&#x2F;space for the algo for each of the inputs? Imagine the algo is already processing millions of input A, and you now increase A by a factor of 10, 100, etc.<p>This melts away all the setup costs, nothing machine specific like cache size matters, anything that isn&#x27;t the dominating factor is swamped, and all you&#x27;re left thinking about is probably how some loop expands. You also don&#x27;t need to think about what coefficient that dominating term has, which you would if you tried to write an equation that took all the operations into account.')