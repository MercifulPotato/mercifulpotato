Item(by='codekilla', descendants=None, kids=[25077855, 25077485], score=None, time=1605226652, title=None, item_type='comment', url=None, parent=25077081, text='&gt; AGI is another path to a post-existential-threat future for humanity but probably has the greatest risks because it accelerates virtually all of the existing risky technology and climate trends<p>No, it is not. The reason is this: <i>it is science fiction</i>. This makes it no less, or no more, dangerous than anything else I can make up with a quasi realistic chance of existing in the near term future.<p>When thermonuclear bombs were detonated, they thought there was a possibility that the reaction would get away from them, and incinerate the atmosphere. The first test, they were scared this actually happened as the explosion carried on longer than they expected.<p>The point? They actually assigned a probability (non zero) to this event, <i>and then carried on with the test anyway</i>. So I could say that a hydrogen bomb, using some iteration of technology just beyond our grasp is likely to kill <i>every human on earth</i> in the near future. Am I right? Who knows..but in conjecturing this I am no more or &#x27;less wrong&#x27; than anyone saying AGI is the <i>greatest</i> threat.')