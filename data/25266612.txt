Item(by='kllrnohj', descendants=None, kids=[25268207], score=None, time=1606840035, title=None, item_type='comment', url=None, parent=25262182, text='As I said, in most scenarios this doesn&#x27;t matter, so I&#x27;m not sure why you&#x27;re pointing at an average scenario as some sort of counter-argument?<p>There&#x27;s a single memory controller on the M1. The CPU, GPU, neural net engine, etc... all share that single controller (hence how &quot;unified memory&quot; is achieved). Given the theoretical maximum throughput of the M1&#x27;s memory controller is 68GB&#x2F;s, and that the CPU can hit around 68GB&#x2F;s in a memcpy, I&#x27;m not sure what you&#x27;re expecting? If you hammer the GPU at the same time, it must by design share that single 68GB&#x2F;s pipe to the memory. There&#x27;s not a secondary dedicated pipe for it to use. So the bandwidth must necessarily be split in a multi-use scenario, there&#x27;s no other option here.<p>Think of it like a network switch. You can put 5 computers behind a gigabit switch and 99% of the time nobody will ever have an issue. At any given point you can speedtest on one of them and see a full 1gbit&#x2F;s on it. But if you speedtest on all of them simultaneously you of course won&#x27;t see each one getting 1gbit&#x2F;s since there&#x27;s only a single 1gbit&#x2F;s connection upstream. Same thing here, just with a 68GB&#x2F;s 8x16-bit channel LPDDR4X memory controller.<p>The only way you can get full-throughput CPU &amp; GPU memory performance is if you physically dedicated memory modules to the CPU &amp; GPU independently (such as with a typical desktop PC using discrete graphics). Otherwise they must compete for the shared resource by definition of being shared.')