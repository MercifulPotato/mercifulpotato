Item(by='anotherProd', descendants=None, kids=[24857218, 24854006, 24852428, 24853943], score=None, time=1603314037, title=None, item_type='comment', url=None, parent=24851239, text='As someone who&#x27;s a software engineer (not data scientist) but is interested in consciousness and by extension AGI and dabbled in some ML algorithms, I find it surprising how often I see the sentiments of AGI being possible or impossible using some sort of algorithm.<p>Obviously I could be missing some great breadth and depth of research (there&#x27;s definitely a lot I don&#x27;t know) but from what I&#x27;ve read &quot;we have no idea&quot; is a pretty accurate description with how far we&#x27;ve come when it comes to consciousness, and I would imagine even less for the newer field of AI&#x2F;AGI (consciousness has been around for a while P: and our theories have mostly sidestepped this real world phenomenon).<p>&gt; &quot;The idea of using &quot;rewards&quot; as a learning mechanism and a path to actual cognition is just wrong, full stop.&quot;<p>This to me is a huge red flag (mostly of ego&#x2F;hubris). I think if we rephrased the goal to not talk about &quot;AGI&quot; and maybe around quantitative things like the things you&#x27;ve listed (&quot;computational efficiency&quot;, likelihood of being stuck in local minimimas, etc) then I&#x27;d happily concede that we should be looking at &quot;X&quot; and not &quot;Y&quot; but unless I&#x27;ve missed something, again likely, when we&#x27;re talking about AGI, we&#x27;re talking about consciousness (epiphenomenon  that come about through physical&#x2F;deterministic interactions). A quick way to gut check myself here is twisting what you state is not a good place to start &quot;ML&#x2F;AI field... gets stuck in local minima&quot; and ask myself is it possible that local minima (which we consider &quot;bad&quot; for current&#x2F;traditional tasks) could be necessary for consciousness ?  I think the widely accepted answer to this is currently &quot;We don&#x27;t know&quot;.<p>If I think that achieving AGI is going to be similar to what the algorithms and architecture we currently use (where the likelihood of being stuck in a local minima is something we can look at) then sure, your opinions stand. But that is just a guess and unless I&#x27;m mistaken AGI hasn&#x27;t been achieved because we don&#x27;t know how to do it.<p>This isn&#x27;t to say that we should have 100% of the data before making strong judgements like this about a subject. It&#x27;s just that the subject of &quot;consciousness&quot; is a big one (I&#x27;d say THE big one) so making such strong statements about something we know we don&#x27;t know much about is interesting. &lt;- this is where I get flashbacks to SE world where a missing piece of data can really throw you off or leads to wrong assumptions and when I think about consciousness we know we don&#x27;t know a lot.')