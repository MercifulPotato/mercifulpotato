Item(by='qayxc', descendants=None, kids=None, score=None, time=1607852801, title=None, item_type='comment', url=None, parent=25404325, text='&gt; There is usually not a &quot;perfect&quot; structure, rather there are hardware, energy and time constraints to training and inference, balanced by over and under training the network.<p>The same can be said about complex simulations. The difference lies in knowing and being able to determine the limits of the system and the verification process.<p>In a simulation we can derive the accuracy of our model from parameters like numerical precision and -stability, coarseness and model used. In other words we know the function we want to model because we state it explicitly.<p>Neural nets can model any function and the challenge is to extract the learned function from the trained network, to examine its limits and correctness. This is what I understood the author meant by the &quot;operational&quot; viewpoint.<p>We can verify that a given network architecture combined with a given optimisation function will find a local minimum w.r.t a given set of training data. This can be verified and tested.<p>What&#x27;s not so easy to verify and test, however, are the properties of the modelled function as well as the function itself. That&#x27;s why we still have to rely on proxies like error metrics on fixed datasets or failure cases.<p>With a simulation on the other hand, we can easily control and predict the (quality of the-) outcome by manipulating well understood parameters (number of iterations, coarseness of the simulation, numerical precision, etc.).<p>I picked simulations as an example, because many other classes of program can be verified using formal methods since the desired results are usually known beforehand. Again, just another reason why the author talks about a distinction in terms of operations, not the fundamental type of programming.<p>I find this to be a very interesting and thought provoking idea.')