Item(by='pwuille', descendants=None, kids=[25297146, 25297405], score=None, time=1607038300, title=None, item_type='comment', url=None, parent=25296714, text='Hi, other author here.<p>0. Indeed. Though if your set consists of 100 million elements, you&#x27;ll indirectly need to have at least 27 bit elements just to be able to identify them. So for practical problems you can probably say that the sketch size does grow logarithmitically with the set sizes.<p>1. The capacity scales with the symmetric difference between the sets. So if one party has [A,B,C] and the other has [B,C,D,E], you need capacity 3 (for A,D,E). So every &quot;change&quot; to an element costs at most 2 capacity; insertions&#x2F;deletions each cost 1 capacity.<p>2. Sketch size is <i>exactly</i> (element size in bits)*capacity. So 1000 bits for a sketch with capacity of 100 10-bit elements.<p>3. I&#x27;m not really up to date here. I believe that PinSketch is the best known deterministic reconciliation (meaning that reconstruction is guaranteed to succeed if the capacity is sufficient). There are more performant algorithms and more trade-offs to make (between capacity and probility, e.g.) if you allow probabilistic recovery.<p>4. I just wrote a (slow) native Python re-implementation for demonstration&#x2F;testing purpose: <a href="https:&#x2F;&#x2F;github.com&#x2F;sipa&#x2F;minisketch&#x2F;pull&#x2F;26" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;sipa&#x2F;minisketch&#x2F;pull&#x2F;26</a>. The C++ code relies on many low-level optimizations that are hard to replicate in higher level languages, so expect a very serious performance drop.<p>1. Shouldn&#x27;t be hard to compile to WASM with emscripten, but I haven&#x27;t tried.')