Item(by='RandyRanderson', descendants=None, kids=[24843381, 24840697, 24841316, 24841133, 24843363, 24840635], score=None, time=1603216791, title=None, item_type='comment', url=None, parent=24835336, text='Certainly a much shorter way to say this: if you have enough lines you can approximate any curve within a margin. This is what large neural networks are doing.<p>Another way to look at it: most neural nets are just a bunch of polynomials stitched together. You can see this from the popularity of the relu activation function. when the relu is negative, that poly is always zero in that area. When positive it&#x27;s some poly multiplied by a const - another poly.<p>For nets that use other activation fns, they try to be linear in the area of most active input. So again they approximate a const * a ploy.')