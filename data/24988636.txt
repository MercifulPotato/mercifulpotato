Item(by='qayxc', descendants=None, kids=None, score=None, time=1604495280, title=None, item_type='comment', url=None, parent=24988453, text='The link only mentions requirements to run inference at &quot;decent speeds&quot;, without going into details about what they consider to be decent speeds.<p>In principle you can of course run any model on any hardware that has enough RAM. Whether the inference performance is acceptable depends your particular application.<p>I&#x27;d argue that for most non-interactive use cases, inference speed doesn&#x27;t really matter and the cost benefit from running on CPUs vs GPUs might be worth it.')