Item(by='KMag', descendants=None, kids=[25427872], score=None, time=1608013467, title=None, item_type='comment', url=None, parent=25426656, text='I remember reading something about Google interview ratings being uncorrelated to job performance, but that&#x27;s clearly a conditional correlation based on the person being hired.  For all we know, the ratings might be highly correlated with job performance up until the hire&#x2F;no-hire cutoff.  After all, their primary purpose is to make that binary hire&#x2F;no-hire decision.  Hopefully, the scoring system is hyper-optimized to be a good signal right around the hire&#x2F;no-hire boundary, as the scores themselves aren&#x27;t that useful for obvious hires and obvious no-hires: the scores are a decision-making tool.<p>In order to really get a good assessment of if the interview ratings were effective, they&#x27;d need to also hire some random unbiased sample of those who fail the interview process.  There are alternative ways of slicing the data to help give insight, such as looking at only those who barely passed the interview process, or looking only at the bottom 10% of performers.  However, when you&#x27;re looking at such a highly biased sample (only the small-ish percentage of people hired), it&#x27;s hard to say what the correlation is across the entire interview population.<p>At the risk of repeating myself, we don&#x27;t particularly care the predictive power of the scores across the whole range, only their predictive power across those who aren&#x27;t obvious no-hires and those who aren&#x27;t obvious hires.  That&#x27;s the range where the power of the interview scores as a decision-making tool is most important.<p>Also, if two metrics disagree, it&#x27;s not clear which one is problematic.  It&#x27;s possible that a poor correlation indicates that there&#x27;s a problem with the performance rating system.')