Item(by='PeterisP', descendants=None, kids=[25647476], score=None, time=1609855257, title=None, item_type='comment', url=None, parent=25645288, text='No, my point is that our research on algorithms dealing with uncertain rewards show that in such scenarios intentionally exploring choices (entropy maximization) is the optimal way to maximize long-term expected value.<p>I.e. it&#x27;s <i>not</i> that humans value choices over expected value, since valuing choices actually is the correct way to get larger expected value (with caveats such as how explore vs exploit tradeoff needs to be changed over time) - the message isn&#x27;t that humans &quot;pay for the perception of freedom&quot; but that human evolved values, even seemingly irrational such as &quot;need for perception of freedom&quot; are actually close to mathematically optimal behavior.')