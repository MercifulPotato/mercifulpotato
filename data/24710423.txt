Item(by='cronobo', descendants=0, kids=None, score=2, time=1602090992, title='Ask HN: Detecting silent code failures using log-based scans. Good or bad idea?', item_type='story', url=None, parent=None, text='The goal is to have more confidence in deployed code. More specifically, it&#x27;s to detect failures hard to find - anything that &quot;fails&quot;, but doesn&#x27;t crash.<p>The concept itself is pretty simple, when implementing a new feature:<p>1. log major steps of the feature (python&#x27;s logging, ruby&#x27;s logger, etc.)<p>2. Then write a simple validation scenario that &quot;scans&quot; the logs and searches for main logging entries of your feature (beginning, end, etc.).<p>3. Tell this hypothetical library&#x2F;platform to analyze logs with those validation scenarios<p>Example of scenario concept:<p>- If you find &quot;User 245 signed up&quot; in logs<p>- Not later than 30 seconds<p>- then you should also find &quot;User 245 - sent welcome email&quot;<p>Concept sounds interesting to me because:<p>- it tackles issues that are difficult to monitor currently (failures that DONT crash)<p>- it&#x27;s useful both pre-deployment and post-deployment (less effort for testing)<p>- it can be used to instrument integration&#x2F;e2e&#x2F;manual tests<p>- it works for app of any scale or architecture (monolithic or 1000 microservices) because logs can be aggregated to a single stream<p>- it&#x27;s helping to ensure business-relevant features or user-level features work OK at scale<p>Any downsides to this idea ?<p>I made a landing to detail the idea a bit more: logscan.io (don&#x27;t know if I&#x27;m allowed to post it here, will remove it otherwise)')