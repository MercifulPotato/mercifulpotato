Item(by='plaidfuji', descendants=None, kids=None, score=None, time=1604148597, title=None, item_type='comment', url=None, parent=24950569, text='&gt; A 2018 study had people who were partisan get exposed to some information on the other side. So if you’re Republican, you get to see what Hillary Clinton is saying, or if you’re a Democrat, you’re exposed to what Donald Trump is saying. And that actually made it worse.<p>The messenger matters just as much as the message. People on both sides carry strong Bayesian priors associated with Trump and Clinton, as well as with most major news outlets now, so they’re predisposed to assume words coming from messengers of “the other side” are inherently untrustable or malicious.<p>A more interesting study would be to expose partisans to identical content (an argument for something wonky, like trade policy or entitlement reform) through venues that otherwise look like their ideological stomping grounds, and watch how quickly people from both sides would agree.<p>Another part of the problem is that many of the current polarizing topics (gun control and abortion come to mind) are  rooted in fundamental moral differences that don’t allow for rational discussion of the middle ground. Either the government can take guns away or it can’t. Either people can terminate a pregnancy or they can’t. Slippery slope arguments dominate these conversations and make people afraid to give any ground.<p>This is actually why I see promise for an AI tool like GPT-3 (or -6 or -7, more like) in politics. Prompt it with content from both sides and see how it answers debate questions. Maybe it would just learn how to give political non-answers. Maybe it would just lead to the politicization of AI, which currently enjoys healthy distrust from both sides. Or maybe people find that they’re more comfortable trusting a machine than a human from the other side.')