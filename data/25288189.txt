Item(by='MayeulC', descendants=None, kids=[25288372], score=None, time=1606998372, title=None, item_type='comment', url=None, parent=25288107, text='&gt; On a large die you could (theoretically) today fit say one million Motorola 68000 Processors. If you clocked that at 5Ghz it would in theory be about 1000 times faster then a Ryzen 7 using the same power.<p>Right. Then, you have to consider how to supply power to all these transistors switching at once, which would well be in the ~10-100A range and beyond. So you have to figure out how not to have it melt (dark silicon, chiplets to have a lower power&#x2F;area ratio, etc).<p>And once you have it not melt, you have to figure out a way of making it useful: keeping the processors fed with data.<p>Because even the algorithms that crunch just a few variables need to be supplied with those, as well as instructions, etc. This ends up sucking a lot of energy, when you have to clock long lines at 5GHz times the number of CPUs times bus width (you can trade hertz for bus width). To avoid this, we use caches. Good luck keeping your CPU count at 1M with the area SRAM needs. AMD GPUs have been memory bandwidth-starved for a few years now, their latest generation features a whooping 128MB cache.<p>Adiabatic&#x2F;reversible computing is another way to get around that, but we&#x27;re not there yet, and isn&#x27;t a silver bullet either.<p>Von neumann architectures is the thing that doesn&#x27;t scale. In my opinion, we are going to go back to specialized computing, especially since Moore&#x27;s Law left us in the cold a few years ago. And RISC-V is quite extensible, which makes it relatively suitable for specialized architectures.')