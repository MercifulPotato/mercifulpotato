Item(by='dodobirdlord', descendants=None, kids=[24981662], score=None, time=1604382808, title=None, item_type='comment', url=None, parent=24976203, text='&gt; In my experience, it&#x27;s easy enough to have services which have a large blast radius themselves and can become points of failure for your entire ecosystem. I don&#x27;t find this to be a huge point of difference from a monolith, although of course it depends a lot on what you&#x27;re working on.<p>It&#x27;s a huge different because if some core critical service starts causing problems it&#x27;s almost certainly because the last binary push was bad, and you roll it back. You only have to roll back that particular service any everything starts behaving correctly again. Moreover, you probably detected the problem in the first place when the rollout of that service began by replacing a single instance of the updated service with the new binary. Monitoring picks up a spike in errors&#x2F;latency&#x2F;database-load&#x2F;whatever and the push is stopped and rolled back.<p>Monoliths have inventive ways to address this problem without having to roll the entire binary back, like pushing patches or using feature flags, but few would argue that the microservice approach to handling bad pushes isn&#x27;t superior.<p>&gt; To me that sounds like poorly written software regardless of architecture. I can&#x27;t imagine a scenario in any of the recent codebases I&#x27;ve worked on (microservices and monoliths both) where errors in what sounds like an internal CRUD tool would cause an entire production application to crash. I find it even harder to imagine if the application has a halfway decent test suite.<p>Easy enough with a sufficiently large codebase in C or C++. Somebody&#x27;s parser encounters an input that was <i>supposed</i> to never happen and now it&#x27;s off clobbering the memory of who-knows-what with garbage.')