Item(by='boulos', descendants=None, kids=[25616674], score=None, time=1609621365, title=None, item_type='comment', url=None, parent=25616372, text='Amusingly, Pixar did build the &quot;Pixar Image Computer&quot; [1] in the 80s and they keep one inside their renderfarm room in Emeryville (as a reminder).<p>Basically though, Pixar doesn&#x27;t have the scale to make custom chips (the entire Pixar and even &quot;Disney all up&quot; scale is pretty small compared to say a single Google or Amazon cluster).<p>Until <i>recently</i> GPUs also didn&#x27;t have enough memory to handle production film rendering, particularly the amount of textures used per frame (which even on CPUs are handled out-of-core with a texture cache, rather than &quot;read it all in up front somehow&quot;). I think the recent HBM-based GPUs will make this a more likely scenario, especially when&#x2F;if OptiX&#x2F;RTX gains a serious texture cache for this kind of usage. Even still, however, <i>those</i> GPUs are extremely expensive. For folks that can squeeze into the 16 GiB per card of the NVIDIA T4, it&#x27;s <i>just</i> about right.<p>tl;dr: The economics don&#x27;t work out. You&#x27;ll probably start seeing more and more studios using GPUs (particularly with RTX) for shot work, especially in VFX or shorts or simpler films, but until the memory per card (here now!) and $&#x2F;GPU (nope) is competitive it&#x27;ll be a tradeoff.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pixar_Image_Computer" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pixar_Image_Computer</a>')