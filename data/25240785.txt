Item(by='maxander', descendants=None, kids=[25242075], score=None, time=1606599551, title=None, item_type='comment', url=None, parent=25238211, text='I get why everyone is wondering whether this is autogenerated jargon, but I think the basic thrust of this is that machine learning algorithms work better as they use larger and larger vectors for representation.  I&#x27;m pretty sure is just a well-known thing in ML, although the projects listed here may be pushing vector size farther than most.<p>So for instance, a typical NLP algorithm (although not GPT-3, IIRC) might represent a word as a 500-float-long vector, which is the same as saying the algorithm considers each word as a point in 500-dimensional space.  This turns out to have weirdly useful properties, to the point where directions in this 500-dimensional space start to have semantic correspondences (e.g. [0], still one of the coolest things in ML, IMHO.)  You can&#x27;t do the same trick with a 3D space- the algorithm doesn&#x27;t have enough to work with when all it knows about a word is three numbers.<p>Another cool example- in gradient descent, you&#x27;re constantly trying to find the lowest point in a &quot;fitness landscape&quot;; in a 3D landscape, you might easily find yourself in a &quot;valley&quot; where every direction is worse than you currently are (a local minima), and you won&#x27;t know where to go.  In a 500D landscape, it&#x27;s unlikely that you&#x27;ll find yourself in a valley where <i>all 500 available directions</i> lead somewhere worse.  So the algorithm will be much less likely to get stuck, and this effect gets more robust the more dimensions you have.<p>[0] <a href="https:&#x2F;&#x2F;colah.github.io&#x2F;posts&#x2F;2014-07-NLP-RNNs-Representations&#x2F;" rel="nofollow">https:&#x2F;&#x2F;colah.github.io&#x2F;posts&#x2F;2014-07-NLP-RNNs-Representatio...</a>')