Item(by='josephg', descendants=None, kids=[25092746], score=None, time=1605363943, title=None, item_type='comment', url=None, parent=25092129, text='&gt; It makes a lot more sense if you consider it shared, concurrently accessed memory.<p>But its not concurrently accessed memory - unlike memory you often want filesystem operations to run asynchronously.<p>And even if you want to think about it like memory, we&#x27;re missing all the tools we have for managing concurrently accessed memory like atomics and granular load and store barriers. And even then only about 1 in 10000 programmers know how to use those primitives to implement correct software anyway, so thats not exactly what I&#x27;d reach for when I think of good API design.<p>But even if you want to use that analogy, the filesystem API doesn&#x27;t have APIs for granular memory barriers. Until recently all we had was fsync. Peppering your code with fsync calls destroys your performance even though it doesn&#x27;t change your program&#x27;s behaviour. And if you batch up your changes and then fsync returns an error, you have no idea which writes succeeded and which failed. inotify helps, but oh goodness does it get complicated.<p>&gt; If userspace can already implement it (when needed) based on provided kernel primitives, why should the kernel maintain two APIs side by side?<p>This is a good question. I guess I don&#x27;t really think much about caches, or processes sharing a pseudofile to communicate. For those programs, sure - the current behaviour is fine. I&#x27;m thinking about the 95% of applications out there which want to use the filesystem to save user data to disk. Those programs shouldn&#x27;t randomly end up in a corrupt state if power goes out at the wrong time. This is the core use case for the filesystem and I think its pretty reasonable for the kernel to make that case easy and correct by default, without resorting to storing all user data in every application in sqlite.<p>From Linus talking about this issue ( <a href="https:&#x2F;&#x2F;lkml.org&#x2F;lkml&#x2F;2009&#x2F;3&#x2F;25&#x2F;632" rel="nofollow">https:&#x2F;&#x2F;lkml.org&#x2F;lkml&#x2F;2009&#x2F;3&#x2F;25&#x2F;632</a> ):<p>&gt; The problem is not that we have a lot of fsync() calls. Quite the reverse. fsync() is really really rare. So is being careful in general. The number of applications that do even the _minimal_ safety-net of &quot;create new file, rename it atomically over an old one&quot; is basically zero. Almost everybody ends up rewriting files with something like open(name, O_CREAT | O_TRUNC, 0666); write(); close();<p>Basically, the reality on the ground right now is that:<p>- Its almost impossible to reliably update a saved file right now without sometimes corrupting data. From the article, researchers found sqlite was the only correct database that existed based on their survey.<p>- Most programmers don&#x27;t even bother to use fsync. Anything which starts with &quot;you can solve the trivial problem of saving a file in this super complex way&quot; is a nonstarter and will almost never be used.<p>Why should the kernel maintain an API for this? Because the default, obvious way people use the filesystem should be correct. A good API makes simple things simple and complex things possible. The posix API fails here. It makes the simple task of updating a save file almost impossible.<p>I&#x27;d be happy for an argument that the posix filesystem is too low level to be used to directly store user data. But nobody seems to be making that argument, and thats not what reality looks like. So long as the filesystem API is advertised (and used) as the standard way to persist user data, it should do that in a simple, safe and correct way.')