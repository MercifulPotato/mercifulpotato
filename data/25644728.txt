Item(by='albertzeyer', descendants=None, kids=None, score=None, time=1609848778, title=None, item_type='comment', url=None, parent=25643696, text='Yes, there are really good open source speech to text tools (automatic speech recognition (ASR) is the common name for that).<p>Kaldi (<a href="https:&#x2F;&#x2F;kaldi-asr.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;kaldi-asr.org&#x2F;</a>) is probably the most well known, and supports hybrid NN-HMM and lattice-free MMI models. Kaldi is used by many people both in research and in production.<p>Lingvo (<a href="https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;lingvo" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;lingvo</a>) is the open source version of Google speech recognition toolkit, with support mostly for end-to-end models.<p>ESPNet (<a href="https:&#x2F;&#x2F;github.com&#x2F;espnet&#x2F;espnet" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;espnet&#x2F;espnet</a>) is good and well known for end-to-end models as well.<p>RASR (<a href="https:&#x2F;&#x2F;github.com&#x2F;rwth-i6&#x2F;rasr" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;rwth-i6&#x2F;rasr</a>) + RETURNN (<a href="https:&#x2F;&#x2F;github.com&#x2F;rwth-i6&#x2F;returnn" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;rwth-i6&#x2F;returnn</a>) are very good as well, both for end-to-end models and hybrid NN-HMM, but they are for non-commercial applications only (or you need a commercial licence) (disclaimer: I work at the university chair which develops these frameworks).<p>Wav2Letter (<a href="https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;wav2letter" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;wav2letter</a>), the tool by Facebook.<p>These are probably just the most well known. There are many others as well. DeepSpeech is inferior to all the ones above, but maybe simpler.<p>Important is also the data to train these, but you will find quite some resources online for English, e.g. Tedlium, Librispeech, etc.<p>You will find lots of resources actually for ASR. Some random links:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;gooofy&#x2F;zamia-speech" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;gooofy&#x2F;zamia-speech</a><p><a href="https:&#x2F;&#x2F;commonvoice.mozilla.org&#x2F;en&#x2F;datasets" rel="nofollow">https:&#x2F;&#x2F;commonvoice.mozilla.org&#x2F;en&#x2F;datasets</a><p><a href="https:&#x2F;&#x2F;www.openslr.org&#x2F;resources.php" rel="nofollow">https:&#x2F;&#x2F;www.openslr.org&#x2F;resources.php</a><p>To add: If you want to do sth like Descript, you are mostly also interested in accurate time-stamps of the recognized text (start and end time of each spoken word). The end-to-end models are usually not so good at this (the goals is mostly to get a good word-error-rate (WER)). The conventional hybrid NN-HMM is maybe actually a better choice for this task.')