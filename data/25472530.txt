Item(by='dragontamer', descendants=None, kids=[25472604], score=None, time=1608328413, title=None, item_type='comment', url=None, parent=25472486, text='&gt; But for that bandwidth to be used efficiently, the processes on each NUMA node need to almost exclusively limit themselves to memory attached to their node -- at which point, well-written software could probably do just as good spread out over several machines that are connected by multiple 100Gb network links, and then you saved two or three bajillion dollars.<p>Wait, so latency over NUMA is too much, but you&#x27;re willing to incur a 100Gb network link? Intel&#x27;s UPI is like 40GBps (Giga-BYTEs, not bits) at sub 500ns latency.<p>100Gb Ethernet is what? 10GBps in practice? 1&#x2F;4th the bandwidth with 4x more latency (in the microseconds) or something?<p>That&#x27;s a PCIe latency penalty (x2, for the sender + the receiver). That&#x27;s a penalty for electrical -&gt; optical, and back again.<p>Any latency, or bandwidth, bound problem is going to prefer a NUMA-link rather than 100Gbps over PCIe.')