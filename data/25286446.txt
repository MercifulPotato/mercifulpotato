Item(by='compycom', descendants=None, kids=[25286697], score=None, time=1606980109, title=None, item_type='comment', url=None, parent=25285897, text='She didn&#x27;t call him a racist. She pointed out that there&#x27;s more to addressing bias than just acknowledging that it&#x27;s there. If you are going to say &quot;Well, garbage-in, garbage-out!&quot;, why do you keep putting the racist garbage in? Why do you, knowing of the problem, keep using biased sources of data, knowing of the potential harms?<p>That&#x27;s the deeper question ML research&#x2F;industry has to grapple with. When these models are deployed increasingly quickly and at scale in ways that can potentially cause massive harm, why is it okay to keep doing the same exact harmful things?<p>LeCun mentioned that there would have been opposite problem if it were trained on a dataset from Senegal. But why wasn&#x27;t it trained on a dataset from Senegal? Why do we always see these errors where white-centric datasets produce white-centric results?<p>It&#x27;s obvious that while it would be a symmetric situation a vacuum, we do not live in a vacuum. We live in a world with deep sociocultural biases in favor and against various racial groups. And it is unjust to let AI perpetuate and entrench these biases by acting at with this bias at scale.<p>Acknowledging dataset bias by itself doesn&#x27;t address the bias meaningfully. In practice, we often treat the bias as an exogenous factor when it is not, moving it outside the scope of our responsibility. But it is very much the product of our work, a reflection of our choices, values, and beliefs about what to prioritize. We can&#x27;t abdicate our responsibility for it (even if we choose not to prioritize it.)')