Item(by='lstamour', descendants=None, kids=[25965220, 25969926, 25967931], score=None, time=1611970962, title=None, item_type='comment', url=None, parent=25964501, text='So I’m not convinced this is actually “billions of colours”. Technically, it means having a 10-bit colour encoding over the wire such that you can express over a billion colours. The distinction between 8-bit and 10-bit is not actually sRGB vs HDR in the same way as dithering a GIF to a maximum of 256 colours lets you display the sRGB colour space but limits you to only 256 colours of it. <a href="https:&#x2F;&#x2F;helpx.adobe.com&#x2F;photoshop-elements&#x2F;using&#x2F;dithering-web-images.html" rel="nofollow">https:&#x2F;&#x2F;helpx.adobe.com&#x2F;photoshop-elements&#x2F;using&#x2F;dithering-w...</a><p>Similarly, you can turn on that little High Dynamic Range checkbox and get HDR but only have 16.7 million colours at your disposal because it’s output in 8-bits per colour rather than 10-bits per colour.<p>And it’s really hard to tell the difference sometimes between 8-bit HDR and 10-bit HDR. Like really hard. Like usually only visible when doing colour grading such that you need every possible nuance of data to more accurately shade and re-colour your pixels. <a href="https:&#x2F;&#x2F;youtu.be&#x2F;MyaGXdnlD6M" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;MyaGXdnlD6M</a><p>Of course I imagine there’s also good vs bad dithering and the output to the attached laptop computer screen is probably better than the multiple cables and adapters required to output to TVs and external displays, but... the easiest way to tell whether something supports billions of colours is to go into monitor preferences and look for 10-bit or 422 or 444. If you see 420 or 8-bit, technically you might still have HDR but you don’t have “billions of colours”, technically.')