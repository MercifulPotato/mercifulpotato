Item(by='tsimionescu', descendants=None, kids=[25748546], score=None, time=1610455897, title=None, item_type='comment', url=None, parent=25743866, text='&gt; A similar argument -- though he didn&#x27;t take it as far -- was made by John Searle (of Chinese Room fame). [2]<p>I have read the entire paper - thank you for the link! - and I find it either false or trivial (to use a style of observation from Chomsky). Searle is asserting that computers don&#x27;t do anything without homunculi to observe their computation, which is patently false. If I create a robot with an optical camera that detects if there is a large object near itself and uses an arm to open a door if so, the system works (or doesn&#x27;t work) regardless of any meaning that is ascribed to its computations by an observer.  It is true that the computation isn&#x27;t &quot;physical&quot; in the sense that there isn&#x27;t a particle of 0 or 1 that could be measured, but it is also impossible to describe the behavior of the system without ultimately referring to the computation it performs. So, if Searle is claiming that such a system only works (opens the door) in relation to some observer, then he is obviously wrong. If he is claiming that the physical processes that occur inside the microprocessor and actuators are the real explanation for how the system behaves, not the computational model, then he is in some sense right, but that is trivially true and no one would really contest it.<p>Furthermore, there likely is no way to actually give an accurate, formal physical model of this entire system that does not also include some kind of computational model of the algorithm it performs to  interpret the photons hitting the sensor as an image, to detect the object, to determine if the object is large enough that the door should be opened, to control the actuator that opens the door etc.<p>Basically, you can look at human beings as black boxes that take in inputs from the environment and produce output. Searle and I both agree that there exists some formal mathematical model that describes how the output the human being will give is related to the input that it gets (including all past inputs and possibly the entire evolutionary history). However, he seems to somehow believe that computation is not necessary as a part of this formal model, which I find perplexing.<p>His claims that cognitivists believe that if they successfully create a computer mimicking some aspect of human capacity that the computers IS that human capacity seems completely foreign to me, I have never seen someone truly claim something this absurd. At most, I have seen claims that if we have successfully created a computer system mimicking a human capacity, that this constitutes proof against mind&#x2F;body dualism at least for that particular capacity, which is I think relatively correct, though more formally this should be called evidence against the need for mind&#x2F;body dualism rather that actual proof.<p>&gt; because denying this would be denying our ability to engage in coherent reasoning and therefore self-defeating. So those mental states can&#x27;t be &quot;implemented&quot; solely using physical states.<p>I don&#x27;t think this holds water. A computer (the theoretical model) is, be definition, something that can perform coherent reasoning without any special internal state. A physical realization of a Turing machine can &quot;think about&quot; any kind of computational problem and come up with the same answer that a human would come up with, at least in the Chinese room sense. Yet we know that the Turing machine doesn&#x27;t have any qualia, so why should we then believe that qualia are fundamental to reason itself?<p>To me, computer science has taken out all of the wind from any kind of qualia-based representation of the human mind.<p>&gt; But in any event, I would reject the notion that any representation of &quot;the feeling of red&quot; is equivalent to the sensation itself.<p>This I agree with in some sense - the map is not the thing. Let&#x27;s assume for a moment that we have an AGI which uses regular RAM to store its internal state. Let&#x27;s also assume that the AGI claims that it is currently experiencing the feeling of seeing red. We could take a snapshot of its RAM and analyze this, and even show it to another AGI, which could recognize that some particular bit pattern is the representation of the AGI feeling of red. Still, that second AGI would not be feeling &quot;I am seeing red&quot; when analyzing this bit pattern. It could though feel &quot;I am seeing red&quot; if it copied the bit pattern into the relevant part of its own memory, even if its optic sensors were in no way receiving red light.')