Item(by='kllrnohj', descendants=None, kids=None, score=None, time=1606161397, title=None, item_type='comment', url=None, parent=25190894, text='&gt; For sure OpenGL&#x2F;DX requires more infrastructure to run unit tests than a generic block of C code. But it&#x27;s absolutely possible to &quot;unit test&quot; shader code, with buffer read-back and&#x2F;or vertex stream out, among other options.<p>Which is what I said, you can screenshot &amp; compare. But it becomes a fuzzy compare due to acceptable precision differences.<p>And it ends up being more of an integration test and not a unit test.<p>&gt; Every language has valid-per-spec differences.<p>They really don&#x27;t, but that&#x27;s not entirely what I&#x27;m talking about. I&#x27;m talking about valid <i>hardware behavior</i> differences, which doesn&#x27;t exist broadly. How a float performs in Java is well-defined and never changes. How numbers perform in <i>most</i> languages is well-defined and does not vary.<p>GPU shaders are completely different. Numbers do not have consistent behavior across differing hardware &amp; drivers. This is a highly unique situation. Even in languages where things are claimed to be variable (like the size of int in C &amp; C++), end up not actually varying, because things don&#x27;t cope well with it. Shaders don&#x27;t play any such similar games.')