Item(by='dragontamer', descendants=None, kids=[24982562], score=None, time=1604427788, title=None, item_type='comment', url=None, parent=24982093, text='&gt; A64FX is HBM2 at equivalent bandwidth to GPUs (with lower power). CCX is much finer granularity than an entire GPU, so not a direct comparison.<p>But only when using SVE512 SIMD-units, which are grossly similar to GPU SIMD units. At a minimum, SIMD reigns supreme. Even Intel only gets its max bandwidth when using AVX512 units.<p>Once you start rewriting your inner loops to run with SIMD, its not too difficult to start thinking about a dedicated SIMD-accelerator, or GPU, to do the job.<p>&gt; L3 bandwidth on EPYC is multi-TB&#x2F;s.<p>32-bytes &#x2F; infinity fabric cycle. 16-CCX per EPYC chip. 3GHz == 1.5 TBps. I dunno about &quot;multi-TB&#x2F;s&quot;, but its over 1 TBps... sure.<p>But only if used in parallel. EPYC only has 32MBs of L3 per CCX. To achieve the full bandwidth, you need to split the problem into each CCX (which asserts a MESI-like &quot;exclusive&quot; lock when writing to a L3 location: preventing other L3 caches from reading-or-writing there).<p>Even then, EPYC&#x27;s L3 cache is small compared to GPU-VRAM. Radeon VII 1TBps VRAM applies at full speed with atomics &#x2F; synchronizations (in fact, the atomics &#x2F; synchronization to L2 cache. I just don&#x27;t have L2 numbers for that GPU...). I would expect Radeon VII&#x27;s L2 cache to have more bandwidth than EPYC&#x27;s L3 cache (Indeed: the Radeon VII L2 cache is in front of a 1TBps HBM2 cluster).<p>If we traverse up the GPU cache structure, you get 10TBps+ on __shared__ or LDS RAM, which is used as atomic-synchronization points or thread-barriers within a  workgroup (a batch of up to 1024 cudaThreads).<p>As such: synchronization between threads (within a large workgroup), or even across the device, is reasonably efficient. The downside of this comparison is that 1024 cudaThreads only have access to 64kB of __shared__ RAM. So it isn&#x27;t really comparable from a size perspective.<p>------<p>Your &quot;TBps&quot; estimate on EPYC&#x27;s L3 cache however, is misleading. Because you spend significant amounts of MESI messages passing cache lines back and forth between CCX to get there. Compared to a unified L2 on the GPU (or unified VRAM), its just not really comparable.<p>EPYC L3 is somewhere between GPU L2 and GPU __shared__, in terms of memory hierarchy and complexity of use.')