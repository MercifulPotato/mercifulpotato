Item(by='Retric', descendants=None, kids=[25263987], score=None, time=1606769697, title=None, item_type='comment', url=None, parent=25257806, text='&gt; There&#x27;s no proof that uninterpretable models perform better<p>It’s a vastly larger solution space.  So it’s really the reverse that would be surprising.<p>&gt; In some sense, it&#x27;s possible to interpret any model, the effort requires just varies.<p>Models can be of arbitrarily large sizes to the point where people really can’t understand them.  How do you go about dissecting a 2 layer NN with 10^40th nodes?')