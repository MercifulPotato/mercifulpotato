Item(by='abernard1', descendants=None, kids=None, score=None, time=1603007891, title=None, item_type='comment', url=None, parent=24815980, text='The first one doesn&#x27;t even need much historical data.  Unless you have some very unoptimized periodic jobs, the last few days or something is plenty.<p>The second can be done simply on something like Dynamo, CosmosDB, or your cloud-hosted NoSQL of choice.  Heck, it can even be done on Aurora or vanilla Postgres + partitioning if it&#x27;s &lt;64TB.<p>The third can be done with any off the shelf cloud data warehouse software, at many petabyte scale.  And even then, I&#x27;m sorry, but I just don&#x27;t believe you that the product clicks over some large timeframe are historically relevant if your software and UI changes often.<p>All of these things mentioned have had extremely simple, boring solutions <i>at petabyte scale</i> for &gt;10 years, and in some cases more than that.  If you add a batch workflow manager and a streaming solution like Spark, that&#x27;s like 3-4 technologies total to cover all these cases (and many more!)')