Item(by='Isinlor', descendants=None, kids=[25325836], score=None, time=1607281342, title=None, item_type='comment', url=None, parent=25325491, text='As far as I&#x27;m aware, attention does not even attempt biological plausibility, nor was it in any way inspired by biology. The issues attention addresses are very specific to sequential nature of so called Recurrent Neural Networks. The first issue is known as exploding &#x2F; vanishing gradients - basically as you keep multiplying some vector with matrices you will either explode that vector to infinity or squeeze to zero, the same happens with derivatives. The second issue is that you can not parallelize sequential operation.  Attention address this issues by removing recurrence by using a specific invented mathematical structure. There was no name for it, but attention gives good intuition for what that mathematical structure is trying to do. Kind of like quantum chromodynamics uses the term &quot;colors&quot; in a way that has nothing to do with light, photons or even electromagnetic force.')