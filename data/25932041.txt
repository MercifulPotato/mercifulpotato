Item(by='StillBored', descendants=None, kids=None, score=None, time=1611772810, title=None, item_type='comment', url=None, parent=25926203, text='For (low) bandwidth metrics yes, for any kind of latency sensitive workload not really.<p>The extra decompression on top of the data fetch latency can be quite noticeable. Sometimes that can be offset if the compression ratio is affecting a hitrate, and thereby decreasing the latency. The problem of course is that even with 10M IOP storage devices frequently it is really latency and an inability to keep 100k requests outstanding that limit perf to one&#x27;s IO turnaround latency.<p>Put another way, compressed ram and disk are really popular in systems which are RAM constrained, or bandwidth limited because the cost of fetching 2x the data vs 1x and decompressing it is a win (think phones with emmc). The problem is that this doesn&#x27;t really make sense on high end NVMe (or for that matter desktops&#x2F;servers with a lot of RAM) where the cost of fetching 8k vs 4k is very nearly identical because the entire cost is front loaded on the initial few bytes, and after than the transfer overhead is minimal. Its even hard to justify on reasonable HD&#x2F;RAID systems too for bandwidth tests since any IO that requires a seek by all the disks will then tend to flood the interface. AKA it takes tens of ms for the first byte, but then the rest of it comes in at a few GB&#x2F;sec and decompressing at faster rates takes more than a single core.<p>edit: And to add another dimension, if the workload is already CPU bound, then the additional CPU overhead of compress&#x2F;decompress in the IO path will likely cause a noticeable hit too. I guess what a lot of people don&#x27;t understand is that a lot of modern storage systems are already compressed at the &quot;hardware&quot; layer by FTL&#x27;s&#x2F;etc.')