Item(by='zbentley', descendants=None, kids=None, score=None, time=1602089782, title=None, item_type='comment', url=None, parent=24710043, text='&gt; changing the batch size to 1 won&#x27;t help if it continues ack&#x27;ing messages without syncing to disk<p>Sorry, I may not have been clear. For many Kafka clients, setting the batchsize to 1&#x2F;interval to 0 causes every publish operation to block on the batch being flushed internally. Flushes are acknowledged by Kafka (and, like any RPC, can succeed or fail), and their success indicates that the persistence system was engaged, indicating disk persistence to a configured number of disks.<p>Other Kafka clients provide a manual &quot;flush&quot; operation which acts similarly.<p>More information about, and discussion of this phenomenon in the Python Kafka client can be found here: <a href="https:&#x2F;&#x2F;github.com&#x2F;confluentinc&#x2F;confluent-kafka-python&#x2F;issues&#x2F;137" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;confluentinc&#x2F;confluent-kafka-python&#x2F;issue...</a><p>The deceptive behavior only happens when a publish operation does <i>not</i> entail a flush. At that point, unsuspecting client code may have assume that data left over the network when it did not.<p>RabbitMQ&#x27;s behavior with publisher confirms disabled is similar but not identical. Even with pub confirms off, most RabbitMQ clients &quot;publish&quot; operations are synchronous (as in they return after data has been written out over a socket). Publisher confirms are an added layer of resilience that enables clients to listen to RabbitMQ saying &quot;I have successfully routed (and, depending on settings, persisted to disk) these messages&quot;. That acknowledgement may never come (rabbit may be overloaded, crashing, or may reject a publish), but on most happy-path systems data loss is not severe even without publisher confirms--turning them on is there to fill the non-happy-path case, and also to provide a very primitive system of backpressure to publishers, forcing them to wait on an overloaded broker rather than sending it even more data when e.g. its persister is slowing down.<p>Because of that behavior, I&#x27;d argue that even without publisher confirms, RabbitMQ&#x27;s publish behavior is still less data-loss prone than Kafka&#x27;s. Regardless, the only way to run either system in truly reliable &quot;publish means my data is on the broker&quot; mode is to set the bufsize&#x2F;flush interval to the minimums (in Kafka&#x27;s case) or to wait for a confirmation after every single publish (in Rabbit&#x27;s).<p>As with all strategies to maximize reliability, those both come with a performance cost, and shouldn&#x27;t be blindly adopted unless you have a good understanding of how much data and performance loss is acceptable.')