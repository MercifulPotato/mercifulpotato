Item(by='pedrocr', descendants=None, kids=[25978633], score=None, time=1612098115, title=None, item_type='comment', url=None, parent=25978426, text='&gt; There is no &quot;non-broken&quot; way that an app can consistently render at a real number scale into an integer sized buffer. Everything is going to have rounding or blurring in some way. This is unavoidable.<p>This is just not true. There&#x27;s nothing special about 1x and 2x when you&#x27;re rendering a font or a vector. 1.5x may very well be the scale at which everything snaps into the pixel grid because that&#x27;s how the pt-&gt;px conversion happened to play out at that given DPI. Thinking of 1x and 2x rendering as being special cases is exactly the problem here. And even for things where you want to pixel align if you leave that to the client some can make more intelligent decisions. There&#x27;s nothing stopping a UI library snapping everything to the pixel grid at arbitrary scales by varying the spacing between elements slightly between steps. That&#x27;s the whole point, the client can do better things in a lot of cases and while the discussion was about Weston the Wayland protocol has ended up with this limitation, which makes sense since Weston is the reference implementation. There&#x27;s currently no way for a client to request the scale factor of the screen and render directly at that factor.<p>&gt; But of course that has the same issues with mixed DPI and you would only really want to use it for performance-sensitive applications that are not particularly sensitive to scale.<p>There&#x27;s no problem with mixed-DPI. If you have a 1x screen and a 1.5x screen render at 1.5x and scale down in the lower screen when spanning two screens. When in one screen exclusively render directly at 1x or 1.5x.')