Item(by='danieldk', descendants=None, kids=None, score=None, time=1612194060, title=None, item_type='comment', url=None, parent=25988898, text='<i>Huggingface fills the need for task based prediction when you have a GPU.</i><p>With model distillation, you can make models that annotate hundreds of sentences per second on a single CPU with a library like Huggingface Transformers.<p>For instance, one of my distilled Dutch multi-task syntax models (UD POS, language-specific POS, lemmatization, morphology, dependency parsing) annotates 316 sentences per second with 4 threads on a Ryzen 3700X. This distilled model has virtually no loss in accuracy compared to the finetuned XLM-RoBERTa base model.<p>I don&#x27;t use Huggingface Transformers, but ported some of their implementations to Rust [1], but that should not make a big difference since all the heavy lifting happens in C++ in libtorch anyway.<p>tl;dr: it is not true that tranformers are only useful for GPU prediction. You can get high CPU prediction speeds with some tricks (distillation, length-based bucketing in batches, using MKL, etc.).<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;tensordot&#x2F;syntaxdot&#x2F;tree&#x2F;main&#x2F;syntaxdot-transformers&#x2F;src&#x2F;models" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;tensordot&#x2F;syntaxdot&#x2F;tree&#x2F;main&#x2F;syntaxdot-t...</a>')