Item(by='chronolitus', descendants=None, kids=None, score=None, time=1608549820, title=None, item_type='comment', url=None, parent=25487747, text='An interesting biologically-inspired alternative to backpropagation is STDP (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Spike-timing-dependent_plasticity" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Spike-timing-dependent_plastic...</a>)<p>However, it&#x27;s still quite hard to get useful results from it in practice.<p>---------------<p>I personally believe that one component of intelligence is the ability to apply cognitive patterns created for a particular input to other inputs. (Very simplified example: A block of &quot;neurons&quot; which have learned to recognize the pattern &quot;is hurt by&quot; when given a subject (group of pixels in image) and object (other group of pixels in image), could be applied to another subject&#x2F;object pair, for example coming from processed audio. But if the audio processing takes 10 layers, and the image processing 5, the connection has to run backwards)<p>To do this in a state of the art deep network, you need the ability to create backward connections. Backward connections imply loops, and loops break backprop (unlike loops in RNNs, which can be easily unrolled AFAIK). So with the current backprop-trained feedforward model, you have to create patterns multiple time instead of reusing them.<p>This is why I will pay attention to backprop alternatives which allow loops, despite their (currently many) disadvantages. This and modular training are the two aspects of learning I would personally focus on.')