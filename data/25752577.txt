Item(by='LinuxBender', descendants=None, kids=None, score=None, time=1610481507, title=None, item_type='comment', url=None, parent=25751802, text='230 does protect platforms from liability of what their user base posts.  Having run forums and chat servers for a long time, I can attest to the experience of having to moderate content and having received legal complaints.   There are two major factors that people are conflating in these discussions.  There is the direct legal aspect of having illicit content.  The platform is covered if they make an effort to remove illicit content AND they themselves are not encouraging the illegal behavior.  So for example, if they have users that also have admin roles and make sub-forums that promote illegal behavior and they do not warn&#x2F;ban the admins, they may eventually be outside the protection of section 230.<p>Then there is the acceptable use policy of the hosting provider(s). <i>dns, server, cdn, app store</i>  This is entirely outside of 230.  If the provider gets enough complaints, they may eventually see your site as a risk and may choose to terminate your account in order to protect the image of their business.  They do not want their reputation tarnished as it will affect their profits.  I think that is totally fair.  If you want to run a site that may likely provoke emotional response from the public, then in my opinion it would be best to find a hosting provider that accepts the risk in a contract.  The contract should state what is expected of you and what you expect of them and what happens if the contract is to be terminated, such as off-boarding timelines. Smaller startups are at higher risk as they provider has less to lose by booting them off their infrastructure.<p>Where I believe this issue has gone sideways is what the industry believes to be considered an appropriate method of moderation.  The big platforms like Facebook, Twitter, Apple are using automated systems to block or shadow-ban things they consider a risk to their company or their hosting providers.  This leads to people fleeing those systems and going to the smaller startups that do not yet have these automated moderation and shadow-banning systems and that is what happened with Parler and a handful of other newer platforms that wanted to capture all the refuges of the big platforms.  A similar thing is happening with that alternate to Youtube, but I can not remember what it is called.  Bitchute?<p>Another potential problem that may confuse the 230 discussion could be that many powerful politicians and corporate leaders use the big platforms like Twitter and Facebook.  They and big lobbyists and investors may have some influence over the behavior of these platforms and may be able to tell them to squash the sites that do not follow the automated version of banning and shadow-banning.  Does that create echo chambers? Is that what is happening here?  Not sure.  If so, I predict it will push many people under ground and that is probably not great for agents that would like to keep an eye on certain people.')