Item(by='aftbit', descendants=None, kids=[25451900, 25451608], score=None, time=1608157351, title=None, item_type='comment', url=None, parent=25447805, text='I&#x27;ve had basically this exact setup running on a cloud server for the past few months. It&#x27;s pretty liberating to be able to open ports to 10.44.0.0&#x2F;24 and have all of my different machines easily access them, regardless of what networks they&#x27;re connected to. I&#x27;ve added in an instance of nsd that manages a zonefile so I can do:<p>ssh laptop.wg.mydomain.net<p>which resolves into 10.44.0.3 for example.<p>There are two major downsides with this approach:<p>(1) all of your traffic must go by way of the bounce server, even if your machines are on the same LAN.<p>(2) the bounce host can observe all of your traffic.<p>I&#x27;ve been considering adding a second layer of wireguard on top of this for end-to-end encryption between nodes. Each machine would have one interface configured like this article suggests simply to assign a stable NAT-traversing IP address. Then they&#x27;d each have a second interface using the first-level IP addresses as endpoints. This would result in 2x the encryption overhead, both in CPU and more importantly in MTU, but it would mean the bounce host was no longer able to intercept even unencrypted data. If you run a daemon on all of the hosts, they can play with the endpoints used on this second level network to avoid using the bounce host when they find they can directly connect to each other, but the default path would work (if somewhat slowly) by the double tunnel process even without this daemon.<p>For bonus points, run multiple bounce hosts, connect to all of them from each peer, then select the endpoint to use for each paired peer connection based on the total path latency.')