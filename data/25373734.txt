Item(by='calebkaiser', descendants=None, kids=None, score=None, time=1607608769, title=None, item_type='comment', url=None, parent=25370325, text='Thanks! The inference is currently run on GPU (a g4dn.xlarge instance), but can be swapped for CPUs by simply changing the compute request in the configuration YAML.<p>In theory, a nano has more than enough memory and storage to run it, but with how tied into AWS the current implementation is, I don&#x27;t know how well it would run on a Jetson Nano without some hackery. Admittedly, however, I&#x27;ve not done much with the Nano, so I don&#x27;t want to come off as overly confident here.')