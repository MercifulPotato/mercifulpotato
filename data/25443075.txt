Item(by='PoignardAzur', descendants=None, kids=None, score=None, time=1608129913, title=None, item_type='comment', url=None, parent=25435028, text='Can someone ELI5 what are the differences between the different libraries are? The article uses a lot of jargon, an something that frustrates me about getting into machine learning is that teaching material will either abstract away what the internals do or assume that you already know how the internals work.<p>Some specific questions:<p>&gt; They provide ways of specifying and building computational graphs<p>Is the article talking about neural networks? As in, arrays of arrays of weights, where input values go through successive layers, and for each layer the same instruction is applied to some values with the respective weight?<p>Or is it talking about a graph as in, a functional graph, where manually written functions call other manually written functions? (hence why a later paragraph talks about if-else statements and for loops)<p>&gt; Almost all tensor computation libraries support autodifferentiation in some capacity (either forward-mode, backward-mode, or both).<p>What are those?<p>From the wikipedia article, it sounds like autodifferentiation basically means running f(x+dx)-f(x), but if there are entire frameworks handling it, then there&#x27;s probably something fancier going on.<p>&gt; According to the JAX quickstart, JAX bills itself as “NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research”. Hence, its focus is heavily on autodifferentiation.<p>The earlier description makes it sound like JAX does some cutting-edge compilation stuff to transform semi-arbitrary functions (with ifs and else and loops and stuff) into a function that returns it derivative.<p>So how can that stuff run on the GPU? It sounds like there would be a lot of branching code.<p>And how is that related to machine learning &#x2F; neural networks?')