Item(by='dcolkitt', descendants=None, kids=[25656804], score=None, time=1609908695, title=None, item_type='comment', url=None, parent=25651385, text='The root-case of skepticism has always been that while Transformers do exceptionally well on finite-sized tasks, they lack any fully recursive understanding of the concepts.[0]<p>A human can learn basic arithmetic, then generalize those principles to bigger number arithmetic, then go from there to algebra, then calculus, then so. Successively building on previously learned concepts in a fully recursive manner. Transformers are limited by the exponential size of their network. So GPT-3 does very well with 2-digit addition and okay with 2-digit multiplication, but can&#x27;t abstract to 6-digit arithmetic.<p>DALL-E is an incredible achievement, but doesn&#x27;t really do anything to change this fact. GPT-3 can have an excellent understanding of a finite sized concept space, yet it&#x27;s still architecturally limited at building recursive abstractions. So maybe it can understand &quot;green block on a red block&quot;. But try to give it something like &quot;a 32x16 checkerboard of green and red blocks surrounded by a gold border frame studded with blue triangles&quot;. I guarantee the architecture can&#x27;t get that exactly correct.<p>The point is that, in some sense, GPT-3 is a technical dead-end. We&#x27;ve had to exponentially scale up the size of the network (12B parameters) to make the same complexity gains that humans make with linear training. The fact that we&#x27;ve managed to push it this far is an incredible technical achievement, but it&#x27;s pretty clear that we&#x27;re still missing something fundamental.<p>[0] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1906.06755.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1906.06755.pdf</a>')