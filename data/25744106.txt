Item(by='simonh', descendants=None, kids=[25744166], score=None, time=1610441872, title=None, item_type='comment', url=None, parent=25744003, text='More intelligent systems will find more efficient ways to achieve their goals than dumber ones (us). They will also select more effective intermediate goals and &#x27;stepping stone&#x27; states towards their goals than we can anticipate. The consequence of this is that we cannot anticipate what means and intermediate objectives they will use. That&#x27;s exactly what you are doing in your example, but attempting to speculate on that is futile.<p>That&#x27;s what the paperclip maximiser example tries to show. In that example one of the intermediate goals to creating maximum paperclips is destroying human civilisation. It&#x27;s not the ultimate objective, it&#x27;s just an intermediate state necessary to efficient completion of the underlying goal. It&#x27;s possible that in fact a superintelligent paperclip maximiser might work out a better strategy that uses human civilisation as a level to maximise paperclips, but maybe not. There&#x27;s simply no way for us to be sure, because by definition it&#x27;s smarter than us.')