Item(by='waste_monk', descendants=None, kids=None, score=None, time=1607558757, title=None, item_type='comment', url=None, parent=25364368, text='&gt;PornHub would delete video uploaded by userA. userB would reupload with different video name<p>Allowing (re-)upload of prohibited or previously removed content is a fatal flaw, and I find it hard to believe they&#x27;ve been allowing it for so long without either being staggeringly incompetent as an organisation, or wilfully turning a blind eye in the name of profits.<p>There are various lists of hashes for Child Sexual Abuse Materials which I&#x27;m sure they&#x27;d have access to, and they could license something like Microsoft PhotoDNA [1] (or implement something similar themselves, it&#x27;s not like they&#x27;re lacking the tech talent) which is able to detect image alterations that break simple checksum comparisons and operate on video content.<p>They don&#x27;t need to play whack-a-mole, for the most part this should be a solved problem. Obviously if it&#x27;s new content that hasn&#x27;t been fingerprinted before you still need manual reporting and moderation, but they could and should be scanning against known CSAM on upload and quarantining it &#x2F; shadow banning the user until it can be evaluated by a human and passed off to law enforcement.<p>It&#x27;s hard to do this at scale given how much content they ingest every day, but they need to bite the bullet and invest some cash and engineering time - what they are doing now simply isn&#x27;t good enough. Hopefully having the spotlight put on them will force them to do the right thing.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;PhotoDNA" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;PhotoDNA</a>')