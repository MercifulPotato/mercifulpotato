Item(by='fallous', descendants=None, kids=None, score=None, time=1603831195, title=None, item_type='comment', url=None, parent=24910006, text='Why does it make human-like predictions as if it knows about reality?  Because it&#x27;s essentially pattern-matching the consensus of the literature it was trained on.  Literature as a whole will tend to settle on a consensus sentiment, albeit one that is probably significantly behind the current consensus sentiment (it takes time to accumulate enough mass to move the weights).  If your interactions with GPT-3 fall into the rather sizeable consensus that most people either subscribe to or are familiar with then it will certainly prove a decent mimic of understanding.  If, however, you attempt to teach it a novel concept or if you dig into its interactions long enough to test for depth of understanding you run into the gaps and GPT-3 either begins to mimic that bullshit artist everyone knows that claims to know things but is only regurgitating platitudes and buzzwords or it begins to mimic behavior that would make you question whether it was sober and&#x2F;or sane.<p>There&#x27;s little doubt that GPT-3 could hold its own quite well in a bout of polite conversation and&#x2F;or small talk that features in many social situations but that&#x27;s more a commentary on the limited area of knowledge and behavior that etiquette expects for interactions in such settings.<p>It could also be trained on the canon of Shakespeare and behave as a prior work of Shakespeare, but if you left one play out of that canon and then attempted to have GPT-3 generate that missing work it wouldn&#x27;t... at all.  It doesn&#x27;t approximate how Shakespeare thought, it approximates the literature he produced that you used for training.<p>None of this is to denigrate the achievement that GPT-3 represents, it is merely to point out that it is unfair to GPT-3 to attempt to hold it to the standards of AGI.')