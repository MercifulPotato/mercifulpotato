Item(by='alquemist', descendants=None, kids=[25257785], score=None, time=1606763329, title=None, item_type='comment', url=None, parent=25255018, text='FWIW, transformers is to sequences what convnets is to grids, modulo important considerations like kernel size and normalization. Think of transformers as really wide (N) and really short (1) convolutions. Both are instances of graphnets with a suitable neighbor function. Once normalization was cracked by transformers, all sort of interesting graphnets became possible, though it&#x27;s possible that stacked k-dimensional convolutions are sufficient in practice.')