Item(by='mywittyname', descendants=None, kids=None, score=None, time=1605719681, title=None, item_type='comment', url=None, parent=25139010, text='&gt; Wouldnâ€™t it make more sense to let the CPU do it, or have specialized cards for the job. So you could have an AMD graphics card and an Nvidia ML card.<p>Graphics cards <i>are</i> the specialized cards for ML.  They are several orders of magnitude faster than GP CPUs at ML because they are specifically designed for the mathematical calculations done in most ML.<p>We call them graphics cards because that was the first real general use-case for doing lots of linear algebra on large data sets.  But long before CUDA came along, people were abusing graphics API to perform more general purpose high-performance calculations for things like video decompression or physics.<p>nVidia has good leadership and recognized that there would soon be a sizeable market for video cards as a cheap replacement for specialized hardware used in super computers.<p>Interesting tidbit, when I was in college, one of our CS professors invested in a cluster of PS3s as a cheap &quot;super computer&quot; as they were substantially faster at ML than anything else you could get for $600.')