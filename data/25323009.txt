Item(by='danieldk', descendants=None, kids=[25323096, 25324193, 25323121], score=None, time=1607256790, title=None, item_type='comment', url=None, parent=25322962, text='I wonder how much it is even possible to train cutting-edge models models. I am sure that there are still tasks where a simple feed forward network or RNN.<p>However, you can just barely finetune any for the base pretrained transformer models (e.g. BERT base or XLM-R base) with 8GB VRAM and need 12GB or 16GB VRAM to finetune larger models. Given that M1 Macs are currently limited to 16GB of shared RAM, I think training competitive models is currently very limited with the memory limitations.<p>I guess the real fun only starts when Apple releases higher-end machines with 32 or 64GB of RAM.')