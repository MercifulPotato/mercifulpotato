Item(by='Imnimo', descendants=None, kids=[25828101], score=None, time=1611011507, title=None, item_type='comment', url=None, parent=25827698, text='I guess the way I look at it is that if you can automatically label your training set, you either have solved the problem you set out to learn (just use your labeler as your classifier&#x2F;detector&#x2F;whatever), or you&#x27;re exploiting some limitation of the training set.  Given a human-annotated test set, I&#x27;d want to see a comparison between three outputs:<p>-The outputs of the auto-labeler.  If this is strong, you&#x27;ve learned that you didn&#x27;t need the training set after all - you managed to solve the problem without it!<p>-The outputs of a model trained on auto-labeled data.  If this is strong but the above test was not, then this pipeline makes sense.<p>-The outputs of a model trained on human-labeled data.  If this is strong but the above tests were not, we&#x27;re in trouble.<p>If none of the three are strong, then the training data was lacking (assuming we&#x27;ve done our best on tuning the model we&#x27;re trying to train), and so no real value was gained by annotating it.')