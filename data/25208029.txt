Item(by='js8', descendants=None, kids=[25208177, 25208446], score=None, time=1606302750, title=None, item_type='comment', url=None, parent=25207771, text='&gt; But I think that this is because in many cases humans speak based on reflex and without thought.<p>Maybe. Then the question is, why do we train a system that is supposed to be intelligent on such data?<p>The problem I see is this. Let&#x27;s assume for a minute that GPT-3 really has the capability to be superintelligent (relative to humans). And you give it as an example to follow, human conversations or musings that do not always make logical sense. What is it supposed to do? The black box inside it can operate in one of two ways, either (1) it will play along and decrease its intelligence to our level and perfectly imitate the imperfections, or (2) it will just say that what we say is wrong and will not be able to really elaborate for the sake of conciseness (and perhaps it will only slightly hint at the arguments). Now, will we really think it is more intelligent if it does (2) instead of (1)? I am not sure about that.<p>Imagine yourself in a similar situation, by social circumstances you&#x27;re forced to talk to somebody obviously stupid, perhaps in the position of authority over you. (Like, for example, a stupid policeman in a repressive regime.) And what he says is clearly wrong and illogical and so on. Will you just (1) nod in an agreement, or (2) try to invoke philosophers to actually argue with the idiot, and try to convince him? I think most intelligent people will actually choose (1) in that circumstance.<p>I think unless we really understand what is happening inside GPT-3, we cannot really tell whether it is really dumb or just playing dumb for the above reason. I think both are a possibility, because we know that systems with measurable outcome (for some problems) better than humans already exist. It might be true that GPT-3 fails on those measures, but we cannot exclude that it fails simply because we didn&#x27;t train it for these tasks, or even for a task of &quot;show its intelligence off&quot;. Maybe it &quot;misunderstood&quot; what we asked it to do.<p>Perhaps you could play a &quot;reverse text game&quot; with GPT-3, where the human would write the adventure and the GPT-3 would input player commands. Then we could perhaps better evaluate how good it is at strategizing, by comparing it to human players. But this is not very scalable.')