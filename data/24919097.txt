Item(by='yowlingcat', descendants=None, kids=[24919378], score=None, time=1603895376, title=None, item_type='comment', url=None, parent=24918739, text='I&#x27;m kinda curious about what some of the recent advances in differential privacy in ML could imply in the field here. Taking a look at FB&#x27;s Opacus, they basically use added noise to ensure that the data &quot;in transit&quot; is not really the data &quot;at rest&quot; and I&#x27;ve toyed with the idea of storing working data and data at rest differently for sensitive information, with the latter being encrypted by a user&#x27;s OTP so an operator breach cannot effectively do much.<p>The problem with this approach is the question &quot;how much noise?&quot; Too little and an adversarial attacker can do some kind of a regression based attack. Too much and the &quot;anonymized&quot; data fails to be a useful &quot;substitute&quot; for previous analysis&#x2F;business logic previously operating on the raw data. There&#x27;s also the whole field of homomorphic encryption to get into as well. It looks like there have been some interesting breakthroughs there from the crypto nut side, specifically with zk-snarks. I can&#x27;t say I understand the underlying tech well enough to comment, though. Fascinating subject.<p>[1] <a href="https:&#x2F;&#x2F;ai.facebook.com&#x2F;blog&#x2F;introducing-opacus-a-high-speed-library-for-training-pytorch-models-with-differential-privacy&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ai.facebook.com&#x2F;blog&#x2F;introducing-opacus-a-high-speed...</a>')