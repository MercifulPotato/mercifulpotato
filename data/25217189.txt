Item(by='fatbird', descendants=None, kids=[25221137], score=None, time=1606369113, title=None, item_type='comment', url=None, parent=25216530, text='<i>You can do joins with queries that we call aggregation pipelines. They&#x27;re super-powerful,</i><p>And hot garbage for performance.  Unlike relational indices on foreign keys, mongo simply... does a lookup for each doc in the pipeline step.  Indexing the looked up collection does nothing extra in aggregation, you&#x27;re just doing a repetitive manual join.  A simple aggregation query that I wrote that added a value from a second collection based on its timestamp compared to the original document&#x27;s timestamp, took at least an order of magnitude more time than without the lookup.<p>Aggregations on a single collection are very performant.  But never try to lookup or join anything.<p><i>Scaling data is mostly about RAM, so if you can, buy more RAM. If CPU is your bottleneck, upgrade your CPU, or buy a bigger disk, if that&#x27;s your issue.</i><p>Never listen to this person about anything.<p>The mongo I&#x27;m dealing with now scales by database.  Each entity has between 20-100GB of data in its own database; we&#x27;re adding entities continually.  If I try to replicate for performance, I&#x27;ll be replicating everything--there&#x27;s no selective replication.  If I shard, I&#x27;ll be sharding <i>within a collection</i>, which is the equivalent of striped RAID--great if that&#x27;s what you need.  I don&#x27;t.  I need to shard at the database layer.  I need my queries routed according to the database at which they&#x27;re aimed, not by the sharding key.  Can I?  Not a chance in hell with any of the existing scaling mechanisms from Mongo.  My current mongo VM is already the largest Azure offers.  How do I add more RAM to that?')