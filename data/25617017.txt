Item(by='dahart', descendants=None, kids=[25617080, 25617265, 25619363], score=None, time=1609625259, title=None, item_type='comment', url=None, parent=25616527, text='&gt; Because the expense is not really worth it<p>I disagree with this takeaway. But full disclosure I’m biased: I work on OptiX. There is a reason Pixar and Arnold and Vray and most other major industry renderers are moving to the GPU, because the trends are clear and because it has recently become ‘worth it’. Many renderers are reporting factors of 2-10 for production scale scene rendering. (Here’s a good example: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ZlmRuR5MKmU" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ZlmRuR5MKmU</a>) There definitely are tradeoffs, and you’ve accurately pointed out several of them - memory constraints, paging, micropolygons, etc. Yes, it does take a lot of engineering to make the best use of the GPU, but the scale of scenes in production with GPUs today is already firmly well past being limited to turntables, and the writing is on the wall - the trend is clearly moving toward GPU farms.')