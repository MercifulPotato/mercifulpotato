Item(by='thunderbird120', descendants=None, kids=None, score=None, time=1608491579, title=None, item_type='comment', url=None, parent=25488662, text='Gradient decent optimizes performance of a model on a given dataset. If you stop training on one dataset and start training on another one your model will become more optimized for the second dataset and less optimized for the first. This will usually result in degraded performance on classes of data found more commonly in the first dataset but not the second. This is what people mean by &quot;forgetting&quot;. It doesn&#x27;t matter how much of the model you fine-tune, the effect is still present though the effect size varies.')