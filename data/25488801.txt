Item(by='unishark', descendants=None, kids=[25489144], score=None, time=1608492132, title=None, item_type='comment', url=None, parent=25488268, text='Its not a complicated concept, just a stretch of the concept of memory. Training in deep learning is done in batches. So &quot;learning&quot; (i.e. the gradient updates to weights) that happens due to your early batches of data can be undone by the gradient updates for later batches.<p>The gradient in machine learning is based on the loss. Specifically it&#x27;s the direction that reduces the loss the fastest. So, not only the most recent batches, but specifically by the recent data that is predicted incorrectly. It doesn&#x27;t have any &quot;confidence&quot; from the memory of what was predicted right previously, for example, it just currently only cares about changing to suit the most recent batches.')