Item(by='edwardjhu', descendants=None, kids=None, score=None, time=1607295235, title=None, item_type='comment', url=None, parent=25314830, text='The claim here is a bit misleading, as already pointed out by other comments, since the kernel is an evolving one that is essentially learned after seeing the data.<p>Contrary to many related works that compare wide neural networks to kernel methods, our recent work shows that one can study a feature learning infinite-width limit with realistic learning rate.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2011.14522" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2011.14522</a><p>We identified what separates the kernel regime (e.g., NTK) and the feature learning regime. In the infinite-width limit, OP&#x27;s work could belong to either regime depending on the parametrization, i.e. the path kernel is either equal to the NTK or performing feature learning.<p>It&#x27;s an incredibly interesting research topic. Please feel free to comment with thoughts on our work :)')