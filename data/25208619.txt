Item(by='ChefboyOG', descendants=None, kids=[25208983], score=None, time=1606308301, title=None, item_type='comment', url=None, parent=25207126, text='It&#x27;s important to keep in mind when reading anything about GPT-3 that the stated objective in training it wasn&#x27;t to produce any kind of AGI, but specifically to see if a model could perform context-specific language tasks without first being fine-tuned to the specific context. For example, GPT-2 (GPT-3&#x27;s predecessor) was originally used in AI Dungeon, but it first had to be fine-tuned on a large corpus of choose-your-own-adventure texts. GPT-3 doesn&#x27;t need to be fine-tunedâ€”it just works.<p>From the original paper:<p>&quot;Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.&quot;<p>Of course, it makes sense that people critique GPT-3 in terms of its actual progress towards human-like intelligence, since every news publication writes about it like it&#x27;s Skynet and OpenAI&#x27;s stated goal is AGI. I do think, however, that we miss all the parts of GPT-3 that are exciting and innovative when we view it through the binary of &quot;Is it really understanding as a human does?&quot; (Not that you were reducing it to that, just speaking about the conversation around GPT-3 more broadly.)')