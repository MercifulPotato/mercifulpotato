Item(by='fpgaminer', descendants=None, kids=[25308900, 25308351], score=None, time=1607116782, title=None, item_type='comment', url=None, parent=25307566, text='&gt; as long as you have a sane set of regularization techniques (or have we even ditched those too?).<p>Maybe.  In the context of natural language at least, Transformers require less and less data to reach the same result as you increase the number of parameters.  No regularization needed.  See Figure 2 in the paper Scaling Laws for Neural Language Models (2001.08361).<p>It&#x27;s quite odd.  Who knows if that will hold for other domains, like protein folding.  It may very well be the case though, since AFAIK DeepMind&#x27;s folding model used attention to reach these landmark results.')