Item(by='knuthsat', descendants=None, kids=[25761107], score=None, time=1610529218, title=None, item_type='comment', url=None, parent=25759430, text='This is very nice. Reminds me a lot of tricks used for simple linear models. And it seems to work, given that Leela Chess Zero is losing with quite a gap.<p>Most of the times one would learn a model by just changing a single feature and then doing the whole sum made no sense.<p>A good example is learning a sequence of decisions, where each decision might have a cost associated to it, you can then say that the current decision depends on a previous one and vary the previous one to learn to recover from errors. If previous decision was bad, then you&#x27;d still like to make the best decision for current state.).<p>So even if your training data does not have this error-recovery example, you can just iterate through all previous decisions and make the model learn to recover.<p>An optimization in that case would be to just not redo the whole sum (for computing the decision function of a linear model).')