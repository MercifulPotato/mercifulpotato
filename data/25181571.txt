Item(by='Der_Einzige', descendants=None, kids=None, score=None, time=1606082842, title=None, item_type='comment', url=None, parent=25180778, text='This paper is neat!<p>More broadly, I am very concerned by how difficult it is to actually meaningfully run model explainability techniques on black-box models in the context of text&#x2F;images in the real world. Most people are not AI researchers and do not want to deal with the hassle of trying random research code off of github (which usually is chalk full of bugs)<p>the ELI5 library implements LIME really well for text, and I have not found an equivalent with SHAP that has all of the same functionality (highlighting most important words and exporting in pretty HTML being the big one).<p>It&#x27;s even worse when you start moving towards the neural network gradient explanations. There are libraries like Captum except in practice it fails on all the big-azz transformer language models I try to train. I&#x27;m still not sure as to how to run any of these gradient explanation methods on pytorch transformer models. Even if they do run, they don&#x27;t display saliency maps with as much detail as the ELI5 library does...<p>There&#x27;s so much good research in explainable AI, and so much engineering work which remains to be done! FFS I had to hack the hell out of ELI5&#x2F;LIME to get it to work on clustering (and I still don&#x27;t know why explainable clustering is not really done, and why I couldn&#x27;t find an off-the-shelf solution for this.)')