Item(by='mncharity', descendants=None, kids=None, score=None, time=1603291140, title=None, item_type='comment', url=None, parent=24846887, text='Exploratory building of rich non-traditional (not necessarily handless) user interfaces is becoming increasingly accessible.  For instance, here&#x27;s a web demo of Google&#x27;s MediaPipe&#x27;s face and iris tracking[1].  And hand tracking[2] with a downward-facing camera, permits enriching keyboard and touchpad&#x2F;tablets:  annotating events with which finger was used, and where on a keycap it was pressed; hand and finger gestures in 2D and 3D; and positional touchless events.  And speech to text... sigh.<p>But doing sensor fusion is hard.  And strongly impacts system architecture.  &quot;Launch the missiles&quot;... 1000 ms later... oh, nope, that was &quot;<i>Lunch is mussels</i> in butter&quot;.  &quot;Spacebar keypress event&quot;... 50 ms later... &quot;btw, that was a thumb, at the 20% position&quot;.  &quot;Ok, so 2000 ms ago, just before the foo, there was a 3D gesture bar&quot;.  So app state needs to easily roll backward and forward, because you won&#x27;t fully know what happened now until seconds from now.  Upside is traditional &quot;have to wait a bit until we know whether mumble&quot; latencies can be optimistically speculated away.<p>[1] <a href="https:&#x2F;&#x2F;viz.mediapipe.dev&#x2F;demo&#x2F;iris_tracking" rel="nofollow">https:&#x2F;&#x2F;viz.mediapipe.dev&#x2F;demo&#x2F;iris_tracking</a> (&quot;run&quot; button is top right)\n[2] <a href="https:&#x2F;&#x2F;viz.mediapipe.dev&#x2F;demo&#x2F;hand_tracking" rel="nofollow">https:&#x2F;&#x2F;viz.mediapipe.dev&#x2F;demo&#x2F;hand_tracking</a>')