Item(by='Veedrac', descendants=None, kids=[25678790], score=None, time=1610036003, title=None, item_type='comment', url=None, parent=25662936, text='&gt; The accuracy largely comes from the fact that addition rarely requires carrying more than a single digit. So it&#x27;s easy to pattern match from single digit problems that it was previously trained on.<p>Again, not at all true due to BPEs.<p><pre><code>      [12] [65] [25] [42] [185]\n    + [580] [22] [75] [80] [93]\n    ---------------------------\n      [706] [75] [300] [278]\n</code></pre>\n(note that GPT-3 is never told that the BPE [580] is composed of the digits [5], [8], and [0]. It has to guess this from the contexts [580] occurs in.)<p>&gt; With multiplication, which requires much more extensive cross-column interaction, accuracy falls off a cliff with anything more than a few digits.<p>You couldn&#x27;t learn long multiplication if you had to use BPEs, were never told how BPEs worked or corresponded to sane encodings, were basically never shown how to do multiplication, and were forced to do it without any working out.<p>Quick, what&#x27;s 542983 * 39486? No writing anything down, you have to output the numbers in order, and a single wrong digit is a fail. (That&#x27;s easy mode, I won&#x27;t even bother asking you to do BPEs.)<p>ML models can learn multiplication, <i>obviously</i> they can learn multiplication, they just can&#x27;t do it in this absurd adversarial context. GPT-f[1] was doing Metamath proofs on 9-digit division (again, a vastly harder context, they involve ~10k proof steps) with 50-50 accuracy, and we have a toy proof of concept[2] for straight multiplication.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2009.03393" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2009.03393</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;Thopliterce&#x2F;transformer-arithmetic" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;Thopliterce&#x2F;transformer-arithmetic</a>')