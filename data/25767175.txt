Item(by='akiselev', descendants=None, kids=None, score=None, time=1610567814, title=None, item_type='comment', url=None, parent=25766065, text='The limits on die size for the competitive consumer chip market are nothing like that of the B2B market. Largest chip ever made was over 40,000 mm^2 [1] compared to Intel&#x27;s 10900K at ~205 mm^2. In production mainframe chips like IBM&#x27;s Z15s are on the order of 700mm^2. The fab process has a lot of levers so very low defect rates are possible but not at the scale of a consumer CPU.<p>[1] <a href="https:&#x2F;&#x2F;techcrunch.com&#x2F;2019&#x2F;08&#x2F;19&#x2F;the-five-technical-challenges-cerebras-overcame-in-building-the-first-trillion-transistor-chip&#x2F;?utm_campaign=fullarticle&amp;utm_medium=referral&amp;utm_source=inshorts" rel="nofollow">https:&#x2F;&#x2F;techcrunch.com&#x2F;2019&#x2F;08&#x2F;19&#x2F;the-five-technical-challen...</a><p>Edit: I assume a supercoducting microprocessor would use a strategy similar to the AI monolith in [1]. Just fuse off and route around errors on a contiguous wafer and distribute the computation to exploit the dark zones for heat dissipation.')