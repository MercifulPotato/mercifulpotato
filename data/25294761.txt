Item(by='rossnordby', descendants=None, kids=None, score=None, time=1607028919, title=None, item_type='comment', url=None, parent=25291348, text='Games that are aiming more for a &quot;cinematic narrative experience&quot; might be perfectly fine with a few 33ms frames of latency, and a total input latency far exceeding 100ms. Competitive twitchy games will tend to be more aggressive. And VR games too, of course.<p>In principle, you can push GPU pipelines to <i>very</i> low latencies. Continually uploading input and other state asynchronously and rendering from the most recent snapshot (with some interpolation or extrapolation as needed for smoothing out temporal jitter) can get you down to total application-induced latencies below 10ms. Even less with architectures that decouple shading and projection.<p>Doing this requires leaving the traditional &#x27;CPU figures out what needs to be drawn and submits a bunch of draw calls&#x27; model, though. The GPU needs to have everything it needs to determine what to draw on its own. If using the usual graphics pipeline, that would mean all frustum&#x2F;occlusion culling and draw command generation happens on the GPU, and the CPU simply submits indirect calls that tell the GPU &quot;go draw whatever is in this other buffer that you put together&quot;.<p>This is something I&#x27;m working on at the moment, and the one downside is that other games that don&#x27;t try to clamp down on latency now cause a subtle but continuous mild frustration.')