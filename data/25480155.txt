Item(by='MereInterest', descendants=None, kids=[25480522, 25480597, 25480608], score=None, time=1608404786, title=None, item_type='comment', url=None, parent=25479836, text='To add to the other answers, machine learning based on biased datasets results in biased models.  For example, Word2Vec can be used to make analogies.  Having it determine &quot;Man is to woman as king is to ___.&quot; results in &quot;queen&quot;, which is reasonable.  On the other hand, determining &quot;Man is to computer programmer as woman is to ___.&quot; results in &quot;homemaker&quot;.  The algorithm wasn&#x27;t deliberately designed to have sexist bias in it, but there was implicit bias in the dataset from which it learned.<p><a href="https:&#x2F;&#x2F;www.technologyreview.com&#x2F;2016&#x2F;07&#x2F;27&#x2F;158634&#x2F;how-vector-space-mathematics-reveals-the-hidden-sexism-in-language&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.technologyreview.com&#x2F;2016&#x2F;07&#x2F;27&#x2F;158634&#x2F;how-vecto...</a><p>As another example, predictive policing tries to place police in places with higher crime rates.  Those crime rates are determined by looking at past history of police reports and arrests.  That past history has human bias already in it, with disproportionately higher arrest rates in places with racial minorities.  The effect of the predictive policing is to justify overpolicing of minorities.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Predictive_policing#Criticisms" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Predictive_policing#Criticisms</a>')