Item(by='rayiner', descendants=None, kids=[25382520], score=None, time=1607661196, title=None, item_type='comment', url=None, parent=25382159, text='ARM64 can&#x27;t be that easy to decode, since ARM&#x27;s recent high-performance cores (A78, X1) decode ARM64 instructions into MOPS and feature a MOP cache: <a href="https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;15813&#x2F;arm-cortex-a78-cortex-x1-cpu-ip-diverging" rel="nofollow">https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;15813&#x2F;arm-cortex-a78-cortex-x...</a>. And we don&#x27;t know that the M1 <i>doesn&#x27;t</i> do that. Also, even on Zen 2, the entire decode section is still a fraction of the size of say the vector units: <a href="https:&#x2F;&#x2F;forums.anandtech.com&#x2F;threads&#x2F;annotated-hi-res-core-die-shots-of-zen-1-and-2.2577382" rel="nofollow">https:&#x2F;&#x2F;forums.anandtech.com&#x2F;threads&#x2F;annotated-hi-res-core-d...</a>. And the cores themselves take up a small amount of the die space on a modern CPU: <a href="https:&#x2F;&#x2F;cdn.mos.cms.futurecdn.net&#x2F;m22pkncJXbqSMVisfrWcZ5-1024-80.jpg.webp" rel="nofollow">https:&#x2F;&#x2F;cdn.mos.cms.futurecdn.net&#x2F;m22pkncJXbqSMVisfrWcZ5-102...</a>.<p>A bet doing 8-wide x86 decoding would be tough, but once you&#x27;ve got a micro-up cache, it&#x27;s doable so long as you have a cache hit. Zen 3 is 8-wide the 95% of the time you hit the micro-up cache.<p>The real question is how does Apple keep that thing fed? An 8-wide decoder is pointless if most of the time you&#x27;ve got 6 empty pipelines: <a href="https:&#x2F;&#x2F;open.hpi.de&#x2F;courses&#x2F;parprog2014&#x2F;items&#x2F;aybclrPgY4nPyYUB24z0x" rel="nofollow">https:&#x2F;&#x2F;open.hpi.de&#x2F;courses&#x2F;parprog2014&#x2F;items&#x2F;aybclrPgY4nPyY...</a> (discussing ILP wall). M1 outperforming Zen 3 by 20% on the SPEC GCC benchmark, at 1&#x2F;3 lower clocks-speed. That&#x27;s 80% more ILP than an Zen 3, which is itself a large advance in ILP.')