Item(by='KhoomeiK', descendants=None, kids=[25353550], score=None, time=1607462643, title=None, item_type='comment', url=None, parent=25346456, text='Discovering Symbolic Models from Deep Learning with Inductive Biases [1] trains graph neural nets on astrophysical phenomena and then performs symbolic regression to generate algebraic formulae to elegantly model the phenomena in a classical physics framework. It&#x27;s largely gone under the radar but has pretty interesting implications for NLP and language theory in my opinion.<p>Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures [2] applies DFA, an approach to training neural nets without backprop,  to modern architectures like the Transformer. It does surprisingly well and is a step in the right direction for biologically plausible neural nets as well as potentially significant efficiency gains.<p>Hopfield Networks is All You Need [3] analyzes the Transformer architecture as the classical Hopfield Network. This one got a lot of buzz on HN so I won&#x27;t talk about it too much, but it&#x27;s part of a slew of other analyses of the Transformer that basically show how generalizable the attention mechanism is. It also sorta confirms many researchers&#x27; inkling that Transformers are likely just memorizing patterns in their training corpus.<p>Edit: Adding a few interesting older NLP papers that I came across this year.<p>StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding [4]<p>Do Syntax Trees Help Pre-trained Transformers Extract Information? [5]<p>Learning to Compose Neural Networks for Question Answering [6]<p>Parsing with Compositional Vector Grammars [7]<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.11287" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.11287</a><p>[2] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.12878" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.12878</a><p>[3] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2008.02217" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2008.02217</a><p>[4] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1908.04577" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1908.04577</a><p>[5] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2008.09084" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2008.09084</a><p>[6] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1601.01705" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1601.01705</a><p>[7] <a href="https:&#x2F;&#x2F;www.aclweb.org&#x2F;anthology&#x2F;P13-1045&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.aclweb.org&#x2F;anthology&#x2F;P13-1045&#x2F;</a>')