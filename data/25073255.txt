Item(by='RobinL', descendants=None, kids=[25078873, 25078300, 25073375], score=None, time=1605206808, title=None, item_type='comment', url=None, parent=25072373, text='We use Glue extensively, but we have a rule of thumb not to use any of the &#x27;special sauce&#x27;.    That means is using it purely for &#x27;Spark as a service&#x27;, so we&#x27;re pretty much always reading&#x2F;writing data from&#x2F;to S3 using spark using a script that would work on any Spark cluster (i.e. not using the GlueContext type stuff, i don&#x27;t even know what it does tbh).<p>For this purpose I think it&#x27;s fantastic.  Write a PySpark script and press go  (we have a package on pypi called etl_manager to facilitate this).   It &#x27;just works&#x27; for this use case, and there&#x27;s a huge amount of value for us in not having to think at all about managing or configuring a Spark cluster.<p>Our biggest bugbear was slow job startup times and a lack of pip installs, but both of those are fixed with glue 2.0 which was released recently.<p>We don&#x27;t use any of the visual&#x2F;GUI based tools for our jobs, we just write our own Spark code and version control in Github.  That&#x27;s unlikely to change any time soon with products like Databrew.  That said, the data profiling tool in Databrew does look like it could be useful as something to refer to when writing code.<p>(I realise this doesn&#x27;t help with your specific issue, but i thought it was helpful to offer an example of a good experience)')