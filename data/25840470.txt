Item(by='dmtomas', descendants=None, kids=None, score=None, time=1611099304, title=None, item_type='comment', url=None, parent=25840469, text=' Computers are seen normally as perfect machines that follow fixed rules to get the best &#x27;&#x27;secure&#x27;&#x27; return, but what happens if it is impossible to get a perfect strategy with the information they have? Will machines learn to lie to compensate the lack of information?<p>I trained two different AI&#x27;s as observers with a Q-learning algorithm and a neural network to play a simple &quot;min-max&quot; game, in this game there is the possibility to lie but it has a risk factor associated.<p>This two different AIs got different results making the Q-learning algorithm lie about 30% of the time and the neural network lying less than 3% of the matches.')