Item(by='contravariant', descendants=None, kids=None, score=None, time=1603444170, title=None, item_type='comment', url=None, parent=24864829, text='Most of those things are stuff you&#x27;ll encounter if you read up on Hamiltonian MC and information field theory, but it might take some additional reading to get all the required background knowledge.<p>The first two are just tricks really. For the weighted estimates you just go from:<p><pre><code>    log(P) = \\sum log(P_i)\n</code></pre>\nto<p><pre><code>    log(P) = \\sum w_i log(P_i)\n</code></pre>\nand for the sufficient statistics you get derivations like:<p><pre><code>    log(P) = \\sum_i (1&#x2F;2) (x_i - m)^2\n           = \\sum_i (1&#x2F;2) x_i^2 - x_i m - (1&#x2F;2)m^2\n           = (\\sum_i (1&#x2F;2) x_i^2) - (\\sum_i x_i) m - (1&#x2F;2)m^2\n</code></pre>\nto show that (\\sum_i (1&#x2F;2) x_i^2) and (\\sum_i x_i) fully determine the posterior distribution.')