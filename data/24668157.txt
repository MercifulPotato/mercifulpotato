Item(by='whimsicalism', descendants=None, kids=[24668824], score=None, time=1601682026, title=None, item_type='comment', url=None, parent=24667586, text='Sure, we are not using any advanced parallelism beyond just batching requests for translation and spinning up more servers if we have more requests. No distillation.<p>For translation, there&#x27;s really not that much to say - we run the transformers on CPU and they seem to be pretty quick. We have a little more tolerance for latency here than with speech.<p>Real-time deep speech recognition on CPU is a little trickier. wav2letter++ has the best performance we&#x27;ve found. it&#x27;s implemented entirely in C++ and streaming inference is quick on CPU. Without a GPU (and even with tbh), it is not feasible to do real-time decoding with a transformer LM, so we use n-grams.')