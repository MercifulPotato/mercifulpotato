Item(by='eivarv', descendants=None, kids=None, score=None, time=1608333324, title=None, item_type='comment', url=None, parent=25472866, text='<p><pre><code>    More seriously, while program overheads have undoubtedly gotten higher, that&#x27;s always been in exchange for something else, even if non-technical, like a better developer experience and platform support.\n</code></pre>\nSoftware architecture usually entails trade-offs in many directions, between NFRs, etc. – but it feels to me like the intepretation of the performance-scale (as well as developer experience, really) is warped by two things:<p>- Many devs aren&#x27;t familiar with the real breadth of possibilities and discount some approaches without understanding them (using &quot;developer experience&quot;, &quot;platform support&quot;, and &quot;too low level&quot; as excuses), thereby skewing towards familiarity and preference. As most devs are familiar with web tech, this tends to win out. Other legitimate issues might not be even be understood (HCI, attack surface, accessibility, etc.)<p>- The performance hit might not be significant enough to notice on a dev-grade machine (e.g. M1-processor, 32GB of RAM, etc.) and&#x2F;or in isolation, but will become noticable when regular users run several apps built using the same heavy stack.<p>The latter is particularly apparent if you try to run multiple Electron-apps simultaneously on a low-to-medium specced laptop – which I find pretty egregious as a user, as multitasking computer systems has been a thing for a while now.<p>My interpretation is that many people (or companies) will usually write software that&#x27;s no more performant than what they can get away with – which can be summed up, if cynically, as &quot;users won&#x27;t see any major performance gain or increase in capability in their day-to-day usage because of shitty software.&quot;')