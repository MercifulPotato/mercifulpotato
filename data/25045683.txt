Item(by='alquemist', descendants=None, kids=None, score=None, time=1605015047, title=None, item_type='comment', url=None, parent=25044989, text='Intelligent trial and error.<p>1. Transformers are an extension of the attention mechanism, a well known LSTM extension that worked well for machine translation (2014, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473</a>). A transformer model is essentially building a multi headed attention module, analogue to CNNs, then stacking several layers of them, analogue to CNNs &#x2F; stacked LSTMs. (1998, <a href="http:&#x2F;&#x2F;yann.lecun.com&#x2F;exdb&#x2F;publis&#x2F;pdf&#x2F;lecun-01a.pdf" rel="nofollow">http:&#x2F;&#x2F;yann.lecun.com&#x2F;exdb&#x2F;publis&#x2F;pdf&#x2F;lecun-01a.pdf</a>)<p>2. Transfomers use residual blocks, which were introduced by the ResNet CNN arhitecture (2015, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1512.03385" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1512.03385</a>). At the time, ResNet was topping the ImageNet benchmarks. This technique helps preventing the vanishing gradient problem during training.<p>3. Transformers use normalization extensively. Layer normalization and attention normalization. This helps keeping internal vectors in the neighborhood of 1 and prevents vanishing gradient training collapse.<p>4. Correct initialization of the network vectors also helps preventing the vanishing gradients problem.<p>5. Unsupervised pretraining was one of the first tricks to make deep networks work (2009, <a href="https:&#x2F;&#x2F;www.jmlr.org&#x2F;papers&#x2F;volume11&#x2F;erhan10a&#x2F;erhan10a.pdf" rel="nofollow">https:&#x2F;&#x2F;www.jmlr.org&#x2F;papers&#x2F;volume11&#x2F;erhan10a&#x2F;erhan10a.pdf</a>)<p>6. Pretraining was used extensively in the vision community for transfer learning, i.e. reusing the weights of a network trained on ImageNet and replace the top layers &#x2F; loss function to tackle a different problem.<p>7. Finally, language modelling, that is predicting the next word in a sentence, was a well known technique to make machine translation work better. Researchers were looking for better language modelling techniques using large corpora (2013, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1312.3005" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1312.3005</a>, <a href="https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;billion-word-imputation" rel="nofollow">https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;billion-word-imputation</a>)')