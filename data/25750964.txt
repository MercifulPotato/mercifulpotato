Item(by='jimrandomh', descendants=None, kids=None, score=None, time=1610475395, title=None, item_type='comment', url=None, parent=25735086, text='I&#x27;m the author of a previous paper about the AI containment problem. This new paper, Alfonseca et al, is kind of crap.<p>This paper defines the term &quot;containment problem&quot; in a nonstandard way. This is different from how I use it in <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1604.00545.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1604.00545.pdf</a> (which is largely about preventing side-channels and security vulnerabilities in realistic simulator environments) or how Bostrom&#x27;s uses it.<p>In this paper, the containment problem is defined as executing a Turing machine for unbounded steps, iff it does not cause harm, given a function which defines whether an output counts as harm. They use a Godelian argument to show that this is incomputable, because a program could simulate the simulator, and condition its behavior in a way that creates a contradiction.<p>This is a toy problem which sort of vaguely rhymes with AI containment and AI safety, but contributes nothing to either of them. A real-world simulation environment wouldn&#x27;t have infinite computation; there would be a finite bound, and &quot;the AI reached the end of the simulation period without making up its mind about what it would do&quot; is just an expected result which means the AI hasn&#x27;t been proven safe.')