Item(by='macksd', descendants=None, kids=None, score=None, time=1603226555, title=None, item_type='comment', url=None, parent=24841868, text='Picture linear regression. If you have a bunch of data points you want to fit a line to, for any given line you can add up the vertical distance between all your data points and the line and come up with a measure of how inaccurate the line is. This is called your &quot;loss&quot;. If you keep trying different values for the slope and intercept, you would find that this function is shaped like a big bowl, or valley. Regression is the process of repeatedly taking a step downhill until you can&#x27;t go anywhere but up, and that must be the optimal line.<p>Neural networks train in a similar way. You have a &quot;loss&quot; function that adds up how wrong your predictions are compared to the training data. You try different values for the weights in your neural network to see which ones send you downhill the fastest, step them in that direction, and repeat. Since the loss function in this case is more complex, it&#x27;s not a single valley, but potentially many valleys. You can end up at a decent local minimum.')