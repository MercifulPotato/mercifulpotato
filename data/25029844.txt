Item(by='013a', descendants=None, kids=None, score=None, time=1604875869, title=None, item_type='comment', url=None, parent=25023504, text='I think any timebox-based batching strategy would effectively just trade frontend performance for backend performance. Your backend would have to deal with fewer requests, and there&#x27;s always a number of &quot;things&quot; the backend needs to do with every request regardless of the content (e.g. check a token against a database), so fewer requests is nice. But, some components would have to wait up-to X milliseconds to get their data, and if we&#x27;re talking about a network request that takes 100ms, balancing the value of X to be big enough to actually have an impact, while being small enough to not double the time it takes for some components to get data, would prove very difficult.<p>The backend performance you could gain is kinda spurious anyway. We&#x27;re talking about N requests being coalesced into 1 mega-request; I would rather clients send me N small requests, not 1 mega-request. Horizontal scaling is easy to set up and quick to do in real-time; vertical scaling is harder.<p>And I think, while a solution like this is interesting in the domain of &quot;you&#x27;ve got two sibling components rendered at the same time who&#x27;s queries could be combined&quot;, that&#x27;s not exactly the issue I described a few comments up. Caching really doesn&#x27;t enter into play here; it would not be measurably faster to just issue one mega-request for one component and let the other wait for the cache (which is, in the end, what we&#x27;re talking about). I mean, it saves one network request, but 90% of a network request is just waiting on IO, and if there&#x27;s one thing Javascript is really good at, its sitting around and waiting. It can wait for dozens of things at a time.<p>The issue is more specifically surrounding two components being rendered at different times. Imagine a User Preview card which displays a user&#x27;s first name; the user clicks on that card, which takes them to a new page, the User Profile, which displays both their first and last name. Short of using a God Query for the Preview which covers a broad set of common user fields, like firing a shotgun and hoping you&#x27;ve got the cache saturated for unknown upcoming components, this situation will take two requests. The shotgun approach is what we do, and what many companies do; it works, but its imprecise. It can hurt the performance of the leading components which pulled the short stick, if that God Query gets too big, and as an application evolves you&#x27;re gonna forget to keep that God Query updated with new things you may need.<p>This problem is enticing because it feels like there should be a solution, and we just haven&#x27;t, as a community, found it. I think GraphQL, at its core, has the capability to solve this; its not like REST which is so undefined and hazy that any solution to something super-complex like this would only work in the domain of one company&#x27;s &quot;way of doing things&quot;. I hope one day we can figure it out, because I suspect that a general, easy to use way of preemptively saturating a cache like this would be a MASSIVE performance boost to every GraphQL client that&#x27;s ever been written.')