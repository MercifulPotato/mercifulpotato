Item(by='sillysaurusx', descendants=None, kids=[25969012, 25979652, 25968766], score=None, time=1612005680, title=None, item_type='comment', url=None, parent=25967641, text='I thought of a change to gradient accumulation, which I call Adam accumulation:<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;theshawwn&#x2F;status&#x2F;1355343951033602057" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;theshawwn&#x2F;status&#x2F;1355343951033602057</a><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25964420" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25964420</a><p>Unfortunately, no one seems to understand it, which isn&#x27;t a great sign. I&#x27;m either not explaining it very well, or the idea doesn&#x27;t make sense.<p>In short:<p><pre><code>  for example in batch:\n    accum += adam(gradients(example))\n\n  param += accum\n  accum = 0\n</code></pre>\nThat way, adam statistics are updated for every training example.<p>Traditional gradient accumulation looks like this:<p><pre><code>  for example in batch:\n    accum += gradients(example)\n\n  param += adam(accum)\n  accum = 0\n</code></pre>\n... which only updates Adam once.<p>(It&#x27;s equivalent to a bigger batch size.)<p>Probably best to just implement Adam accumulation and see if it works, I suppose.<p>(Sorry for rambling about this here. I was just hoping to find some prior work along these lines, if anyone knew of something.)')