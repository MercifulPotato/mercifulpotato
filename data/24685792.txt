Item(by='yorwba', descendants=None, kids=[24688921], score=None, time=1601887613, title=None, item_type='comment', url=None, parent=24684586, text='&gt; Given how scarce entropy sources tend to be, it always torqued me a bit that if I want to simulate a dice roll I had to grab a whole float worth of data from the entropy source instead of something smaller.<p>Grabbing a float worth of data and reducing it to a dice roll is no different from getting the bits for the dice roll directly. Entropy doesn&#x27;t decrease just by getting bits from an RNG, it decreases by an attacker <i>learning what those bits are</i>.<p>If you throw away bits without looking at them, an attacker doesn&#x27;t get to look at them either, so entropy doesn&#x27;t decrease. Otherwise, running an RNG in a loop would decrease entropy all the way to zero (meaning its values become fully predictable) no matter what you do with the output. But RNGs that are actually used have a finite state space they cycle through. Completing a full (long) cycle returns to the original state and how could entropy decrease between two identical states?<p>Entropy measures how hard it is for an external observer to predict the next random value. If you run an RNG for a full cycle and tell someone all the values, then they can perfectly predict the next cycle just by looking at their recording (entropy is zero). But if you don&#x27;t let them know anything, they&#x27;re none the wiser (entropy remains unchanged).<p>&gt; we donâ€™t generate 1-6 via division, we generate it by multiplying a random number between 0.0 and 1.0 times 6 and add a 1<p>You can do that, but you don&#x27;t get each value with the same probability. E.g. if the floating point type can represent eight values between 0.0 and 1.0, you might think that&#x27;s enough, since you only need six, but actually<p><pre><code>  &gt;&gt;&gt; [1+int((i&#x2F;8.)*6) for i in range(8)]\n  [1, 1, 2, 3, 4, 4, 5, 6]\n</code></pre>\n1 and 4 end up twice as likely as other numbers.<p>Double precision floating point numbers have enough different possibilities for this effect not to matter much, but if you want to eliminate it completely, you need to detect it, reject the current value and generate a new one. That extra check may be implemented using a division, but can be optimized (thus &quot;nearly divisionless&quot;).')