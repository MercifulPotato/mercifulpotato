Item(by='xamuel', descendants=None, kids=[24876753], score=None, time=1603484641, title=None, item_type='comment', url=None, parent=24872873, text='It&#x27;s not about whether the agent will or will not find an optimal solution (no agent can possibly find good solutions in <i>every</i> environment). It&#x27;s about whether the agent could even <i>understand</i> the true environment based on your real-value-reward description of it.<p>Suppose the true environment has various tasks the agent can do which give big-O-complexity-valued rewards, one task giving a reward of O(2^n), and others giving rewards of O(n^i) for various i. For concreteness, say Task A rewards O(2^n), Task B rewards O(n^10000), and Task C rewards O(n^20000).<p>Now suppose you present this to the agent using real-valued rewards, say, where O(n^i) is replaced by arctan(i) and O(2^n) is replaced by arctan(2)+pi, as you suggest, then the agent will be deluded into thinking, e.g., that Task B and Task C give almost identical rewards (Task B gives reward 1.57069633 and Task C gives reward 1.57074633, which barely differ from each other at all). This is misleading because in the true environment, Task C gives much more reward than Task B. Yes, the agent understands Task C gives a bigger reward, but the agent totally mis-understands how much bigger :)')