Item(by='duvenaud', descendants=None, kids=[25490144, 25491323, 25489287], score=None, time=1608494838, title=None, item_type='comment', url=None, parent=25488912, text='Sure.  First of all, I want to say that backprop, by which I mean reverse-mode differentiation for computing gradients, combined with gradient descent for updating parameters, is pretty great.  In a sense it&#x27;s the last thing we should be trying to replace, since pretty much the whole deep learning revolution was about replacing hand-designed functions with ones that can be optimized in this way.<p>Reverse-mode differentiation has about the same time cost as whatever function you&#x27;re optimizing, no matter how many parameters you need gradients for.  This which is about as good as one could hope for, and is what lets it scale to billions of parameters.<p>The main downside of reverse-mode differentiation (and one of the reasons it&#x27;s biologically implausible) is that it requires storing all the intermediate numbers that were computed when evaluating the function on the forward pass.  So its memory cost grows with the complexity of the function being optimized.<p>So the main practical problem with reverse-mode differentiation + gradient descent is the memory requirement, and much of the research presented in the workshop is about ways to get around this.  A few of the major approaches are:<p>1) Only storing a subset of the forward activations, to get noisier gradients at less memory cost.  This is what the &quot;Randomized Automatic Differentiation&quot; paper does.  You can also save memory and get exact gradients if you re-construct the activations as you need them (called checkpointing), but this is slower.<p>2) Only training one layer at a time.  This is what the &quot;Layer-wise Learning&quot; papers are doing.  I suppose you could also say that this is what the &quot;feedback alignment&quot; papers are doing.<p>3) If the function being optimized is a fixed-point computation (such as an optimization), you can compute its gradient without needing to store any activations by using the implicit function theorem.  This is what my talk was about.<p>4) Some other forms of sensitivity analysis (not exactly the same as computing gradients) can be done by just letting a dynamical system run for a little while.  Barak Pearlmutter has some work on how he thinks this is what happens in slow-wave sleep to make our brains less prone to seizures when we&#x27;re awake.<p>I&#x27;m missing a lot of relevant work, and again I don&#x27;t even know all the work that was presented at this one workshop.  But I hope this helps.')