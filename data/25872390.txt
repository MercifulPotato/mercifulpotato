Item(by='diarrhea', descendants=None, kids=None, score=None, time=1611329081, title=None, item_type='comment', url=None, parent=25871494, text='&gt; So I don&#x27;t know how that&#x27;s a bad idea, considering sync of a large number of files is the single purpose of file sync.<p>Yeah it is, but things aren&#x27;t all rosy and still very flaky here and there (again, I&#x27;d wager it&#x27;s similar for most other cloud sync clients). So we have to still work around these limitations.<p>I&#x27;m not very familiar with git internals, but git writes <i>many</i> files, most of them tiny. When I physically copy a directory with a git repo, the working directory (the currently checked out files) always copies over fast, but the potentially tens of thousands of tiny files take forever (they&#x27;re not continuous and each one has to be seeked on disk, and each file has some overhead. The overhead isn&#x27;t noticeable for large files, but adds up for numerous tiny ones).<p>Next to selfhosting Nextcloud, I also selfhost GitLab for that purpose. That is way overkill though. There&#x27;s lightweight git servers like Gitea that I&#x27;ve heard people selfhost. If you don&#x27;t rely on dumb file-based sync but only sync to your remote in git protocol, things won&#x27;t go wrong anymore.')