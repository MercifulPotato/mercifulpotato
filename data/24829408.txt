Item(by='dragontamer', descendants=None, kids=[24830038], score=None, time=1603132600, title=None, item_type='comment', url=None, parent=24829255, text='I&#x27;m not an expert on Xilinx&#x27;s stuff, but Xilinx&#x27;s 18-bit x 27-bit multiplier DSP48 architecture still seems quite awkward to me.<p><a href="https:&#x2F;&#x2F;www.xilinx.com&#x2F;support&#x2F;documentation&#x2F;white_papers&#x2F;wp486-deep-learning-int8.pdf" rel="nofollow">https:&#x2F;&#x2F;www.xilinx.com&#x2F;support&#x2F;documentation&#x2F;white_papers&#x2F;wp...</a><p>True, a systolic processor can be built out of these components, but I doubt it&#x27;d be anything as fast as a proper matrix-multiplication unit that NVidia is putting out. And that&#x27;s after the complications associated with FPGA programming.<p>----------<p>Xilinx&#x27;s FPGAs are a combination of LUTs (4-bit or 6-bit look-up tables), Routing, and &quot;DSP Slices&quot; (prefabricated multipliers). The bulk of your math is still going to be performed in the DSP Slice.<p>Where FPGAs win is that their LUTs and Routing tables allow you to create custom high-speed glue-logic between these DSP slices. But when it comes to something like a regular Convolutional neural net, the routing is very straightforward.<p>------<p>I&#x27;m not really a deep learning expert. I know that FP16 seems to be pushed by NVidia. This INT8 stuff seems odd to me (there&#x27;s no way INT8 gets the same dynamic range as FP16), but I&#x27;m also not really sure if the dynamic range is needed or useful in deep learning applications.')