Item(by='mannykannot', descendants=None, kids=None, score=None, time=1602636723, title=None, item_type='comment', url=None, parent=24770071, text='&gt; You could restate my point as &quot;there is no evidence it is in fact useful&quot;<p>And if you had originally stated your point that way, I would probably pointed out that there is equally no evidence that it will not be useful, if it turns out to be the case.<p>&gt; ...but it&#x27;s certainly not obvious that a better model implies an ontological line between itself and reality.<p>Clearly, I failed to get my point across, to the point where I cannot guess where this question is coming from. Let&#x27;s see if I can be clearer...<p>My position on the definitions of words is that they are contingent on our knowledge and that new knowledge can change our definitions (I gave &#x27;energy&#x27; as an example, and you appear to have accepted this point a couple of posts back.)<p>I take the same view of ontologies; the categories we see are contingent on what we know and may change as our knowledge increases. This should not be surprising, given that ontologies are specific cases of words with meanings that nominally pertain to how the world is. There is no implication here that a better model implies an ontological line between itself and reality; rather, the point here is effectively a &quot;so what&quot; reply to your statement, &quot;it&#x27;s a tenable position to claim [Newtonian mechanics] is just a model, and we accept that model because it&#x27;s useful.&quot; Mutatis mutandis, as they say, and consequently, there is no justification for holding on to old ontologies if new facts suggest a better alternative, any more than there is for models or theories.<p>&gt; To put it bluntly, is a better model really &quot;more true&quot;, i.e. qualitatively different from a worse one?<p>Did you mean to write that, especially given that, in your previous post, you offered an argument for the proposition that &quot;Newtonian mechanics is <i>false</i>, because, for example, it fails to accurately predict Mercury&#x27;s orbit.&quot; If there is a relevant point here, I think it is that &quot;more true&quot; models are qualitatively better (and quantitatively better, also.)<p>&gt; Ironically, if it were possible to [show an algorithm that can effectively emulate your brain within an acceptable margin of error], it would be proof there is no &quot;intelligence&quot;, only &quot;computability&quot;, making the first entirely redundant.<p>How so? If &quot;intelligence&quot; is a useful concept now (and your objection to &quot;artificial intelligence&quot; seems to be predicated on it being so), when we do not know if the mind is computationally modelable, why would this usefulness necessarily vanish if this turns out to be the case?<p>&gt; I&#x27;ll take the seventh.<p>? - I&#x27;m not familiar with this expression.')