Item(by='quicklime', descendants=None, kids=[24840371, 24840654, 24840774, 24840329, 24840729, 24840479, 24840593], score=None, time=1603215841, title=None, item_type='comment', url=None, parent=24835336, text='&gt; High dimensional spaces are unlikely to have local optima, and probably donâ€™t have any optima at all.<p>Can someone who knows more about DL than I do help me understand this a little better?<p>The article uses the analogy of walls:<p>&gt; Just recall what is necessary for a set of parameters to be at a optimum. All the gradients need to be zero, and the hessian needs to be positive semidefinite. In other words, you need to be surrounded by walls. In 4 dimensions, you can walk through walls. GPT3 has 175 billion parameters. In 175 billion dimensions, walls are so far beneath your notice that if you observe them at all it is like God looking down upon individual protons.<p>I&#x27;m struggling to understand what this really means in 4+ dimensions. But when I try to envision it going from 1 or 2 to 3 dimensions, it doesn&#x27;t seem obvious at all that a 3D space should have fewer local optima than a 2D space.<p>In fact, having a &quot;universal function&quot; like a deep network seems like it should have more local optima. What am I missing?')