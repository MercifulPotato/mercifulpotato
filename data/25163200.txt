Item(by='wnoise', descendants=None, kids=[25165233], score=None, time=1605896290, title=None, item_type='comment', url=None, parent=25159573, text='What I don&#x27;t understand is _what_ these applications are.\nWhat does this surprise value tell me, and how can it be calibrated to anything quantitative?  What&#x27;s the equivalent of statistics that I can use to alter my original model&#x27;s rankings to conform to what I observe?<p>I can understand ranking as coarse-graining of probability if there&#x27;s some way to approximately map it back to the standard notion of probability.<p>On the other hand, using rank to direct collections of traces in a well-defined order is admittedly a neat trick; I can see using it to find something like the &quot;most reasonable&quot; or &quot;easiest to explain&quot; solution to a given logic problem. (I have actually seen a hand-coded version for Sudoku that attempts to minimize the backtracking depth to make it as easy to explain as possible.)  But when would I want to do that to a program where the ranks represent instead how common a given event is?  Verifying the no-exception case first?  Is that useful?')