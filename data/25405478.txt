Item(by='fock', descendants=None, kids=None, score=None, time=1607853796, title=None, item_type='comment', url=None, parent=25404648, text='well, your neural network is just a very complicated - ideally continously differentiable - function. In principle you could just fit this function to data with nonlinear least squares (e.g. Levenberg-Marquardt), we use backpropagation and SGD, because it&#x27;s faster.\nConcerning simulated annealing: have a look at the old Alphafold and check how they optimize their neural networks...<p>So, could you explain again how the concepts and foundations of deep learning differs from plain old regression techniques?')