Item(by='jacques_chester', descendants=None, kids=None, score=None, time=1610908106, title=None, item_type='comment', url=None, parent=25812179, text='They&#x27;re definitely thin on explaining the sample sizes. They say 54 configurations over &quot;nearly 1,000&quot;, which suggest 17 tests (918 runs) or 18 tests(972 runs) per configuration.<p>They run 4 different benchmarks (CPU, network, I&#x2F;O, TPC-C), suggesting an average of around 4.25 or 4.5 per bechmark per configuration. If instead they ran 16 per configuration, that would be a nice round 4 per benchmark per configuration, but total runs would drop to 864, somewhat less than &quot;nearly 1000&quot;.<p>Assuming my figures are sound, we&#x27;re looking at 4 to 5 samples per combination. Without some information about the within-group variation, though, it&#x27;s difficult to distinguish what variation was due to &quot;weather&quot; and what was due to the platform.<p>I do however think that the effect size of some results is enough to make them useful (eg, network throughput). But all of the close results (eg single-core difference between AWS and Azure) are not very reliable, in my view.')