Item(by='gamozolabs', descendants=None, kids=None, score=None, time=1607424741, title=None, item_type='comment', url=None, parent=25341673, text='Determinism is already fairly important in fuzzing for 2 major reasons.<p>One, is that having determinism makes it easier to triage bugs. If I find a crash while fuzzing, but there&#x27;s no way to reproduce it, it&#x27;s going to pretty much never get fixed (unless the call stack from the first crash is obvious enough to prepare a fix). For fuzzing systems, this is effectively mandatory. Imagine a memory allocation failure leading to a NULL-deref, the odds of hitting that same bug are so low because it requires going OOM at the same point in a subsequent execution. Snapshot fuzzing (each fuzz case starts from a snapshotted state of memory&#x2F;registers&#x2F;whatever) mitigates some of this, but there&#x27;s still noise from context switches that will affect RNGs, which will affect heap layouts, which will affect everything on the system. That being said, most fuzzing out there publicly is not system&#x27;s fuzzing, and thus this is typically not something people think about.<p>But two, is what most people use determinism for. In modern fuzzing, coverage guidance is pretty much mandatory. This means when new code is hit, to save off the input such that it can be built upon. At a very simple level, this means a problem which is 256^4, turns into a 256*4, as all requirements do not need to be satisfied simultaneously, as long as the previous requirements cause new code to get hit they can be built upon. Of course, if the program does not behave the same way every time, the noise can start to erode the value which is provided here, since you&#x27;re not actually building upon the same thing.<p>I&#x27;m not sure if this answers the question.')