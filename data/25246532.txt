Item(by='fractionalhare', descendants=None, kids=[25246716, 25246710, 25246713, 25246610], score=None, time=1606671131, title=None, item_type='comment', url=None, parent=25236675, text='I can&#x27;t evaluate the entire book in one sitting, so I read through the section on linear regression. EDIT: Also read replies to this comment.<p>In section 4.1, Linear Regression, subsection &quot;Normality&quot;:<p><i>&gt; It is assumed that the target outcome given the features follows a normal distribution. If this assumption is violated, the estimated confidence intervals of the feature weights are invalid.</i><p>This is incorrect. Linear regression does not require the outcomes follow a normal distribution. Rather, it requires that the residual <i>errors</i> are normally distributed.<p>The next subsection, &quot;Homoscedasticity&quot;, is technically correct but incomplete. A more general assumption of linear regression is stationarity. Heteroskedasticity is one, but not the only, way to violate stationarity. In my opinion this section should focus on stationarity more generally than homoskedasticity, because constant variance is a necessary but insufficient assumption.<p>In the following subsection, &quot;Independence&quot;, I don&#x27;t see any error. I think this would be a good opportunity to talk about autocorrelation of the residuals and outcomes, because this is why linear regression cannot generally be used for time series data. Autocorrelation (or correlation via an underlying temporal trend) both violate independence.<p>I generally like section 4.1.6. I think reasonable people could disagree on whether linear models are relatively explainable, but overall the technicals in this section are correct. Linear models are simpler to understand and very powerful, but they do break down when most of the correlative relationship is nonlinear (e.g. monotonic or exponential) or when there are too many features to capture.')