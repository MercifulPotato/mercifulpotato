Item(by='dcolkitt', descendants=None, kids=None, score=None, time=1603727334, title=None, item_type='comment', url=None, parent=24896730, text='The importance of FPGAs is generally oversold. They definitely have their place, but the problem is due to the complexity you can only evaluate very simple triggers and&#x2F;or must sacrifice a ton of agility where strategy development is measured in years instead of weeks.<p>The biggest manifestation of this is that FPGAs really struggle with sizing. Say there&#x27;s a given market event that produces a very obvious opportunity, but it&#x27;s not so obvious how big that opportunity is. Archetypical case is when the price changes, and there&#x27;s a new level formed on the book.<p>It&#x27;s almost certainly the case that adding an epsilon of liquidity at the first position of the queue is profitable. What&#x27;s harder to determine is where the breakeven point is. Should you add 1 lot? 100 lots? 10,000 lots? That&#x27;s a lot more complex because you generally have to be sensitive to the typical sizes in that particular market and that particular time, as well as how strong the level formation event was. E.g. was it in response to something that looks like a very big order, or just some random liquidity depletion.<p>What happens is that the FPGAs know they should quote an epsilon, but can&#x27;t really confidently quote anything bigger than that. So if the right answer is a thousand lots, the FPGA will quickly capture first queue position with something like a hundred lots, then the software-based players with take the rest of the 900 shares for themselves.<p>What you see with FPGAs is a segment that makes very consistent, very good money relative to the volume that it executes. But fails to capture the sizable majority of the market share.')