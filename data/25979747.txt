Item(by='joefourier', descendants=None, kids=[25979829], score=None, time=1612109033, title=None, item_type='comment', url=None, parent=25979471, text='Evolution strategies (essentially the same as genetic algorithms) have been successfully applied to GPU-training of neural networks and parallelise very well. Look at various papers on neuro-evolution (e.g. the ones from the now defunct Uber AI Labs).<p>A simple, very inefficient implementation would be: do inference on <i>n</i> randomly initialised neural networks, calculate loss, select the <i>m</i> best performing and copy them to fill all <i>n</i> spots, add gaussian noise to each weight and bias of every individual, repeat. More efficient mutation and selection strategies exist, but the principle is similar and parallelisation on GPUs is trivial and actually easier than current approaches (atomic operations e.g. compare-and-swap can be used to avoid going back to the CPU for selection).<p>I believe the real reason for neuro-evolution&#x27;s  unpopularity is that for most problems, gradient descent is just faster and more efficient.<p>What would be interesting might be to combine both approaches, using evolution strategies on hyperparameter search, although I haven&#x27;t read the literature on that front.')