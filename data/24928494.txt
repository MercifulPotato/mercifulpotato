Item(by='exged', descendants=None, kids=None, score=None, time=1603961880, title=None, item_type='comment', url=None, parent=24928076, text='To be fair, it&#x27;s not uncommon for a ML researcher &#x2F; engineer to use tens (~$10&#x2F;hr on cloud, $100k from Nvidia) or even hundreds (~$100&#x2F;hr on cloud, $1M from Nvidia) of GPUs to speed up their iteration time. If there was a way to spend half as much on hundreds of AMD GPUs instead that would be a huge win, well worth even months of the researcher&#x27;s time.<p>The catch is that ML software stacks have had hundreds if not thousands of man-years of effort put into things like cuDNN, CUDA operator implementations, and Nvidia-specific system code (eg. for distributed training). Many formidable competitors like Google TPU have emerged, but Nvidia is currently holding onto its leadership position for now because the wide support and polish is just not there for any of the competitors yet.')