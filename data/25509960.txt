Item(by='MSM', descendants=None, kids=[25513590], score=None, time=1608664465, title=None, item_type='comment', url=None, parent=25509767, text='EDIT: After looking into it, it seems like Spark calls both things predicate pushdowns (eliminating unnecessary row group reads via the statistics AND pushing the predicates down to the lowest possible level). You&#x27;re right, I&#x27;m wrong!<p>&gt;Parquet files contain min&#x2F;max metadata for all columns. When possible, entire files are skipped, but this is relatively rare. This is called predicate pushdown filtering.<p>A nitpick, but I wouldn&#x27;t call this predicate pushdown, it&#x27;s partition (or segment) elimination. A predicate being pushed down potentially allows files to be skipped through this process though')