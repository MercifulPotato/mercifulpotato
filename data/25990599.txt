Item(by='thu2111', descendants=None, kids=None, score=None, time=1612199539, title=None, item_type='comment', url=None, parent=25990022, text='This feels like AWS apologism.<p>Slack knew how to set up their infrastructure. Nothing in the postmortem implies AWS was misconfigured. AWS spotted the problem and fixed it entirely on their side.<p>Nothing in this report suggests that Slack has unique usage patterns. Users returning to work after Christmas is not a phenomenon unique to Slack.<p>Their problems were:<p>1. The AWS infrastructure broke due to an event as predictable as the start of the year. That&#x27;s on Amazon.<p>2. Their infrastructure is too complicated. Their auto-scaling created chaos by shutting down machines whilst engineers were logged into them due to bad heuristics, although it&#x27;s not like this was a good way to save money, and their separation of Slack into many different AWS accounts created weird bottlenecks they had no way to understand or fix.<p>3. They were unable to diagnose the root cause and the outage ended when AWS noticed the problem and fixed their gateway system themselves.<p><i>The cloud isn&#x27;t some magic thing that solves all scaling problems</i><p>In this case it actually created scaling problems where none needed to exist. AWS is expensive compared to dedicated machines in a colo. Part of the justification for that high cost is seamless scalability and ability to &#x27;flex&#x27;.<p>But Slack doesn&#x27;t need the ability to flex here. Scaling down over the holidays and then back up once people returned to work just isn&#x27;t that important for them - it&#x27;s unlikely there were a large number of jobs queued up waiting to run on their spare hardware for a few days anyway. It just wasn&#x27;t a good way to save money: a massive outage certainly cost them far more than they&#x27;ll ever save.')