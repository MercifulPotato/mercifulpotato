Item(by='ChuckMcM', descendants=None, kids=[24973519], score=None, time=1604277115, title=None, item_type='comment', url=None, parent=24958423, text='I saw this when it was a twitter thread[1] and noted then, some good points and some not so good points.<p>One of the things that comes up with Risc-V a lot is code density, and it is a sore spot for many CPU designers because it is shamelessly abused by marketing departments as a measure of &#x27;goodness&#x27;. This has sort of trained these engineers to flinch when something doesn&#x27;t exhibit good code density, and the author is no exception.<p>However, FLASH&#x2F;RAM volume has gone up hugely. This is in part because once you run out of logic to lay down in a chip you flood fill the rest with RAM and&#x2F;or FLASH because hey the chip has to be big enough to hold pad landings for all of its pins. This has taken a lot of pressure off the code density thing and now it seems appropriate to look at algorithmic capacity.<p>I agree with the author that it is a much better use of resources to put a hard multiplier on a chip than it is to have more space in flash so that you can do that with instructions. But from a algorithmic capacity question? It is all Turing complete so really what is the practical difference?<p>Now I started life programming on PDP-11&#x27;s that had a &quot;native&quot; instruction set that was not unlike RISC-V in being maximally simple. We called it &quot;microcode&quot; :-) And the &quot;real&quot; instructions were actually sequences of microcode in a microcode ROM. That was pretty cool because you could swap out floating point instructions for vector instructions or string handling instructions if you wanted.<p>I will not be surprised in the least if people design &quot;custom instruction sets&quot; that layer on top of a RISC-V core, just like layering a front end stack on web assembly. And the available resources to do that are pretty plentiful.<p>One of the coolest architectures I got to play with was the Xerox &quot;D&quot; machines. They took this to an extreme and some amazing software was written for them. You could get really close to maximizing utilization. It was very interesting to load a new instruction set, recompile your Mesa code, and have it run faster with no changes to the hardware at all.<p>Of course DEC and Xerox didn&#x27;t invent this, the IBM 360 had a big button on the front &quot;IMPL&quot; which was &quot;Initial Micro Program Load&quot; which prepped the instruction set for what ever OS you were about to start up.<p>And now you can build systems like this with open source tools and off the shelf FPGA dev boards. Such a great time to be interested in systems architecture.<p>[1] <a href="https:&#x2F;&#x2F;twitter.com&#x2F;erincandescent&#x2F;status&#x2F;1154535799423229952" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;erincandescent&#x2F;status&#x2F;115453579942322995...</a>')