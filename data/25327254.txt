Item(by='6gvONxR4sf7o', descendants=None, kids=None, score=None, time=1607293819, title=None, item_type='comment', url=None, parent=25314830, text='This is a really cool new bit of intuition:<p>&gt; A key property of path kernels is that they combat the curse of dimensionality by incorporating derivatives into the kernel: two data points are similar if the candidate function’s derivatives at them are similar, rather than if they are close in the input space. This can greatly improve kernel machines’ ability to approximate highly variable functions (Bengio et al., 2005). It also means that points that are far in Euclidean space can be close in gradient space, potentially improving the ability to model complex functions. (For example, the maxima of a sine wave are all close in gradient space, even though they can be arbitrarily far apart in the input space.)')